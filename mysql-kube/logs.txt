* 
* ==> Audit <==
* |-----------|--------------------------------|----------|----------|---------|---------------------|---------------------|
|  Command  |              Args              | Profile  |   User   | Version |     Start Time      |      End Time       |
|-----------|--------------------------------|----------|----------|---------|---------------------|---------------------|
| start     |                                | minikube | yaswitha | v1.32.0 | 04 Feb 24 14:53 +08 | 04 Feb 24 14:56 +08 |
| start     | --vm-driver=docker             | minikube | yaswitha | v1.32.0 | 04 Feb 24 15:01 +08 | 04 Feb 24 15:02 +08 |
| stop      |                                | minikube | yaswitha | v1.32.0 | 05 Feb 24 09:43 +08 | 05 Feb 24 09:43 +08 |
| start     |                                | minikube | yaswitha | v1.32.0 | 05 Feb 24 09:43 +08 | 05 Feb 24 09:44 +08 |
| service   | mongo-express-service          | minikube | yaswitha | v1.32.0 | 05 Feb 24 10:30 +08 | 05 Feb 24 10:35 +08 |
| service   | mongo-express-service          | minikube | yaswitha | v1.32.0 | 05 Feb 24 10:35 +08 | 05 Feb 24 10:50 +08 |
| service   | mongo-express-service          | minikube | yaswitha | v1.32.0 | 05 Feb 24 10:51 +08 | 05 Feb 24 10:56 +08 |
| service   | mongo-express-service          | minikube | yaswitha | v1.32.0 | 05 Feb 24 10:57 +08 | 05 Feb 24 11:02 +08 |
| delete    |                                | minikube | yaswitha | v1.32.0 | 05 Feb 24 11:07 +08 | 05 Feb 24 11:07 +08 |
| start     | --memory=4096                  | minikube | yaswitha | v1.32.0 | 05 Feb 24 11:08 +08 | 05 Feb 24 11:11 +08 |
|           | --driver=virtualbox            |          |          |         |                     |                     |
| service   | mongo-express-service          | minikube | yaswitha | v1.32.0 | 05 Feb 24 11:16 +08 | 05 Feb 24 11:16 +08 |
| stop      |                                | minikube | yaswitha | v1.32.0 | 05 Feb 24 18:46 +08 | 05 Feb 24 18:47 +08 |
| start     |                                | minikube | yaswitha | v1.32.0 | 05 Feb 24 18:47 +08 | 05 Feb 24 18:50 +08 |
| service   | mongo-express-service          | minikube | yaswitha | v1.32.0 | 05 Feb 24 19:03 +08 | 05 Feb 24 19:03 +08 |
| service   | mongo-express-service          | minikube | yaswitha | v1.32.0 | 05 Feb 24 21:10 +08 | 05 Feb 24 21:10 +08 |
| start     |                                | minikube | yaswitha | v1.32.0 | 06 Feb 24 09:19 +08 | 06 Feb 24 09:21 +08 |
| addons    | enable ingress                 | minikube | yaswitha | v1.32.0 | 06 Feb 24 10:09 +08 | 06 Feb 24 10:10 +08 |
| dashboard |                                | minikube | yaswitha | v1.32.0 | 06 Feb 24 10:23 +08 |                     |
| delete    | mysql                          | minikube | yaswitha | v1.32.0 | 06 Feb 24 16:52 +08 |                     |
| stop      |                                | minikube | yaswitha | v1.32.0 | 06 Feb 24 21:08 +08 | 06 Feb 24 21:08 +08 |
| start     |                                | minikube | yaswitha | v1.32.0 | 07 Feb 24 11:30 +08 |                     |
|-----------|--------------------------------|----------|----------|---------|---------------------|---------------------|

* 
* ==> Last Start <==
* Log file created at: 2024/02/07 11:30:43
Running on machine: Yaswithas-MacBook-Air
Binary: Built with gc go1.21.3 for darwin/amd64
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I0207 11:30:43.575465    3511 out.go:296] Setting OutFile to fd 1 ...
I0207 11:30:43.576292    3511 out.go:348] isatty.IsTerminal(1) = true
I0207 11:30:43.576299    3511 out.go:309] Setting ErrFile to fd 2...
I0207 11:30:43.576307    3511 out.go:348] isatty.IsTerminal(2) = true
I0207 11:30:43.576671    3511 root.go:338] Updating PATH: /Users/yaswitha/.minikube/bin
I0207 11:30:43.695747    3511 out.go:303] Setting JSON to false
I0207 11:30:43.978748    3511 start.go:128] hostinfo: {"hostname":"Yaswithas-MacBook-Air.local","uptime":3240,"bootTime":1707273403,"procs":459,"os":"darwin","platform":"darwin","platformFamily":"Standalone Workstation","platformVersion":"12.7.3","kernelVersion":"21.6.0","kernelArch":"x86_64","virtualizationSystem":"","virtualizationRole":"","hostId":"e71a6db4-2cc6-5b2a-a817-722231a3ef9f"}
W0207 11:30:43.979790    3511 start.go:136] gopshost.Virtualization returned error: not implemented yet
I0207 11:30:43.998582    3511 out.go:177] üòÑ  minikube v1.32.0 on Darwin 12.7.3
I0207 11:30:44.025257    3511 notify.go:220] Checking for updates...
I0207 11:30:44.043019    3511 config.go:182] Loaded profile config "minikube": Driver=virtualbox, ContainerRuntime=docker, KubernetesVersion=v1.28.3
I0207 11:30:44.043187    3511 driver.go:378] Setting default libvirt URI to qemu:///system
I0207 11:30:45.400405    3511 virtualbox.go:136] virtual box version: 6.1.30r148432
I0207 11:30:45.416302    3511 out.go:177] ‚ú®  Using the virtualbox driver based on existing profile
I0207 11:30:45.429760    3511 start.go:298] selected driver: virtualbox
I0207 11:30:45.429781    3511 start.go:902] validating driver "virtualbox" against &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO:https://storage.googleapis.com/minikube/iso/minikube-v1.32.1-amd64.iso KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 Memory:4096 CPUs:2 DiskSize:20000 VMDriver: Driver:virtualbox HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.28.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.59.100 Port:8443 KubernetesVersion:v1.28.3 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[dashboard:true default-storageclass:true ingress:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/Users:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 AutoPauseInterval:1m0s GPUs:}
I0207 11:30:45.429958    3511 start.go:913] status for virtualbox: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:6.1.30r148432
}
I0207 11:30:45.508504    3511 cni.go:84] Creating CNI manager for ""
I0207 11:30:45.508916    3511 cni.go:158] "virtualbox" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0207 11:30:45.508941    3511 start_flags.go:323] config:
{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO:https://storage.googleapis.com/minikube/iso/minikube-v1.32.1-amd64.iso KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 Memory:4096 CPUs:2 DiskSize:20000 VMDriver: Driver:virtualbox HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.28.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.59.100 Port:8443 KubernetesVersion:v1.28.3 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[dashboard:true default-storageclass:true ingress:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/Users:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 AutoPauseInterval:1m0s GPUs:}
I0207 11:30:45.509425    3511 iso.go:125] acquiring lock: {Name:mkd73ab2cbf30a1d6ac19d439e77e2d94274fdd1 Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I0207 11:30:45.536961    3511 out.go:177] üëç  Starting control plane node minikube in cluster minikube
I0207 11:30:45.554152    3511 preload.go:132] Checking if preload exists for k8s version v1.28.3 and runtime docker
I0207 11:30:45.554301    3511 preload.go:148] Found local preload: /Users/yaswitha/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.28.3-docker-overlay2-amd64.tar.lz4
I0207 11:30:45.554330    3511 cache.go:56] Caching tarball of preloaded images
I0207 11:30:45.554786    3511 preload.go:174] Found /Users/yaswitha/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.28.3-docker-overlay2-amd64.tar.lz4 in cache, skipping download
I0207 11:30:45.554854    3511 cache.go:59] Finished verifying existence of preloaded tar for  v1.28.3 on docker
I0207 11:30:45.555044    3511 profile.go:148] Saving config to /Users/yaswitha/.minikube/profiles/minikube/config.json ...
I0207 11:30:45.557504    3511 start.go:365] acquiring machines lock for minikube: {Name:mk0d35d94b59cbe39d4a96fd823e35ca690ad233 Clock:{} Delay:500ms Timeout:13m0s Cancel:<nil>}
I0207 11:30:45.562523    3511 start.go:369] acquired machines lock for "minikube" in 4.980288ms
I0207 11:30:45.562566    3511 start.go:96] Skipping create...Using existing machine configuration
I0207 11:30:45.562581    3511 fix.go:54] fixHost starting: 
I0207 11:30:45.563105    3511 main.go:141] libmachine: COMMAND: /usr/local/bin/VBoxManage showvminfo minikube --machinereadable
I0207 11:30:48.114504    3511 main.go:141] libmachine: STDOUT:
{
name="minikube"
groups="/"
ostype="Linux 2.6 / 3.x / 4.x (64-bit)"
UUID="7785180f-7acb-4aa1-be56-47fec5875cea"
CfgFile="/Users/yaswitha/.minikube/machines/minikube/minikube/minikube.vbox"
SnapFldr="/Users/yaswitha/.minikube/machines/minikube/minikube/Snapshots"
LogFldr="/Users/yaswitha/.minikube/machines/minikube/minikube/Logs"
hardwareuuid="7785180f-7acb-4aa1-be56-47fec5875cea"
memory=4096
pagefusion="off"
vram=8
cpuexecutioncap=100
hpet="on"
cpu-profile="host"
chipset="piix3"
firmware="BIOS"
cpus=2
pae="on"
longmode="on"
triplefaultreset="off"
apic="on"
x2apic="off"
nested-hw-virt="off"
cpuid-portability-level=0
bootmenu="disabled"
boot1="dvd"
boot2="dvd"
boot3="disk"
boot4="none"
acpi="on"
ioapic="on"
biosapic="apic"
biossystemtimeoffset=0
rtcuseutc="on"
hwvirtex="on"
nestedpaging="on"
largepages="on"
vtxvpid="on"
vtxux="on"
paravirtprovider="default"
effparavirtprovider="kvm"
VMState="poweroff"
VMStateChangeTime="2024-02-06T13:08:55.000000000"
graphicscontroller="vboxvga"
monitorcount=1
accelerate3d="off"
accelerate2dvideo="off"
teleporterenabled="off"
teleporterport=0
teleporteraddress=""
teleporterpassword=""
tracing-enabled="off"
tracing-allow-vm-access="off"
tracing-config=""
autostart-enabled="off"
autostart-delay=0
defaultfrontend=""
vmprocpriority="default"
storagecontrollername0="SATA"
storagecontrollertype0="IntelAhci"
storagecontrollerinstance0="0"
storagecontrollermaxportcount0="30"
storagecontrollerportcount0="30"
storagecontrollerbootable0="on"
"SATA-0-0"="/Users/yaswitha/.minikube/machines/minikube/boot2docker.iso"
"SATA-ImageUUID-0-0"="e2e6d007-9e65-4ada-ac79-de64d81e96bf"
"SATA-tempeject"="off"
"SATA-IsEjected"="off"
"SATA-1-0"="/Users/yaswitha/.minikube/machines/minikube/disk.vmdk"
"SATA-ImageUUID-1-0"="b71bfcd5-e591-40ba-89ef-042783c5fb47"
"SATA-2-0"="none"
"SATA-3-0"="none"
"SATA-4-0"="none"
"SATA-5-0"="none"
"SATA-6-0"="none"
"SATA-7-0"="none"
"SATA-8-0"="none"
"SATA-9-0"="none"
"SATA-10-0"="none"
"SATA-11-0"="none"
"SATA-12-0"="none"
"SATA-13-0"="none"
"SATA-14-0"="none"
"SATA-15-0"="none"
"SATA-16-0"="none"
"SATA-17-0"="none"
"SATA-18-0"="none"
"SATA-19-0"="none"
"SATA-20-0"="none"
"SATA-21-0"="none"
"SATA-22-0"="none"
"SATA-23-0"="none"
"SATA-24-0"="none"
"SATA-25-0"="none"
"SATA-26-0"="none"
"SATA-27-0"="none"
"SATA-28-0"="none"
"SATA-29-0"="none"
natnet1="nat"
macaddress1="080027C1DD90"
cableconnected1="on"
nic1="nat"
nictype1="virtio"
nicspeed1="0"
mtu="0"
sockSnd="64"
sockRcv="64"
tcpWndSnd="64"
tcpWndRcv="64"
Forwarding(0)="ssh,tcp,127.0.0.1,59633,,22"
hostonlyadapter2="vboxnet0"
macaddress2="080027594EE2"
cableconnected2="on"
nic2="hostonly"
nictype2="virtio"
nicspeed2="0"
nic3="none"
nic4="none"
nic5="none"
nic6="none"
nic7="none"
nic8="none"
hidpointing="ps2mouse"
hidkeyboard="ps2kbd"
uart1="off"
uart2="off"
uart3="off"
uart4="off"
lpt1="off"
lpt2="off"
audio="coreaudio"
audio_out="off"
audio_in="off"
clipboard="disabled"
draganddrop="disabled"
vrde="off"
usb="off"
ehci="off"
xhci="off"
SharedFolderNameMachineMapping1="Users"
SharedFolderPathMachineMapping1="/Users"
videocap="off"
videocapaudio="off"
capturescreens="0"
capturefilename="/Users/yaswitha/.minikube/machines/minikube/minikube/minikube.webm"
captureres="1024x768"
capturevideorate=512
capturevideofps=25
captureopts=""
GuestMemoryBalloon=0
}
I0207 11:30:48.115592    3511 main.go:141] libmachine: STDERR:
{
}
I0207 11:30:48.115747    3511 fix.go:102] recreateIfNeeded on minikube: state=Stopped err=<nil>
W0207 11:30:48.115768    3511 fix.go:128] unexpected machine state, will restart: <nil>
I0207 11:30:48.132501    3511 out.go:177] üîÑ  Restarting existing virtualbox VM for "minikube" ...
I0207 11:30:48.144199    3511 main.go:141] libmachine: COMMAND: /usr/local/bin/VBoxManage showvminfo minikube --machinereadable
I0207 11:30:48.272258    3511 main.go:141] libmachine: STDOUT:
{
name="minikube"
groups="/"
ostype="Linux 2.6 / 3.x / 4.x (64-bit)"
UUID="7785180f-7acb-4aa1-be56-47fec5875cea"
CfgFile="/Users/yaswitha/.minikube/machines/minikube/minikube/minikube.vbox"
SnapFldr="/Users/yaswitha/.minikube/machines/minikube/minikube/Snapshots"
LogFldr="/Users/yaswitha/.minikube/machines/minikube/minikube/Logs"
hardwareuuid="7785180f-7acb-4aa1-be56-47fec5875cea"
memory=4096
pagefusion="off"
vram=8
cpuexecutioncap=100
hpet="on"
cpu-profile="host"
chipset="piix3"
firmware="BIOS"
cpus=2
pae="on"
longmode="on"
triplefaultreset="off"
apic="on"
x2apic="off"
nested-hw-virt="off"
cpuid-portability-level=0
bootmenu="disabled"
boot1="dvd"
boot2="dvd"
boot3="disk"
boot4="none"
acpi="on"
ioapic="on"
biosapic="apic"
biossystemtimeoffset=0
rtcuseutc="on"
hwvirtex="on"
nestedpaging="on"
largepages="on"
vtxvpid="on"
vtxux="on"
paravirtprovider="default"
effparavirtprovider="kvm"
VMState="poweroff"
VMStateChangeTime="2024-02-06T13:08:55.000000000"
graphicscontroller="vboxvga"
monitorcount=1
accelerate3d="off"
accelerate2dvideo="off"
teleporterenabled="off"
teleporterport=0
teleporteraddress=""
teleporterpassword=""
tracing-enabled="off"
tracing-allow-vm-access="off"
tracing-config=""
autostart-enabled="off"
autostart-delay=0
defaultfrontend=""
vmprocpriority="default"
storagecontrollername0="SATA"
storagecontrollertype0="IntelAhci"
storagecontrollerinstance0="0"
storagecontrollermaxportcount0="30"
storagecontrollerportcount0="30"
storagecontrollerbootable0="on"
"SATA-0-0"="/Users/yaswitha/.minikube/machines/minikube/boot2docker.iso"
"SATA-ImageUUID-0-0"="e2e6d007-9e65-4ada-ac79-de64d81e96bf"
"SATA-tempeject"="off"
"SATA-IsEjected"="off"
"SATA-1-0"="/Users/yaswitha/.minikube/machines/minikube/disk.vmdk"
"SATA-ImageUUID-1-0"="b71bfcd5-e591-40ba-89ef-042783c5fb47"
"SATA-2-0"="none"
"SATA-3-0"="none"
"SATA-4-0"="none"
"SATA-5-0"="none"
"SATA-6-0"="none"
"SATA-7-0"="none"
"SATA-8-0"="none"
"SATA-9-0"="none"
"SATA-10-0"="none"
"SATA-11-0"="none"
"SATA-12-0"="none"
"SATA-13-0"="none"
"SATA-14-0"="none"
"SATA-15-0"="none"
"SATA-16-0"="none"
"SATA-17-0"="none"
"SATA-18-0"="none"
"SATA-19-0"="none"
"SATA-20-0"="none"
"SATA-21-0"="none"
"SATA-22-0"="none"
"SATA-23-0"="none"
"SATA-24-0"="none"
"SATA-25-0"="none"
"SATA-26-0"="none"
"SATA-27-0"="none"
"SATA-28-0"="none"
"SATA-29-0"="none"
natnet1="nat"
macaddress1="080027C1DD90"
cableconnected1="on"
nic1="nat"
nictype1="virtio"
nicspeed1="0"
mtu="0"
sockSnd="64"
sockRcv="64"
tcpWndSnd="64"
tcpWndRcv="64"
Forwarding(0)="ssh,tcp,127.0.0.1,59633,,22"
hostonlyadapter2="vboxnet0"
macaddress2="080027594EE2"
cableconnected2="on"
nic2="hostonly"
nictype2="virtio"
nicspeed2="0"
nic3="none"
nic4="none"
nic5="none"
nic6="none"
nic7="none"
nic8="none"
hidpointing="ps2mouse"
hidkeyboard="ps2kbd"
uart1="off"
uart2="off"
uart3="off"
uart4="off"
lpt1="off"
lpt2="off"
audio="coreaudio"
audio_out="off"
audio_in="off"
clipboard="disabled"
draganddrop="disabled"
vrde="off"
usb="off"
ehci="off"
xhci="off"
SharedFolderNameMachineMapping1="Users"
SharedFolderPathMachineMapping1="/Users"
videocap="off"
videocapaudio="off"
capturescreens="0"
capturefilename="/Users/yaswitha/.minikube/machines/minikube/minikube/minikube.webm"
captureres="1024x768"
capturevideorate=512
capturevideofps=25
captureopts=""
GuestMemoryBalloon=0
}
I0207 11:30:48.272850    3511 main.go:141] libmachine: STDERR:
{
}
I0207 11:30:48.273132    3511 main.go:141] libmachine: Check network to re-create if needed...
I0207 11:30:48.273174    3511 main.go:141] libmachine: COMMAND: /usr/local/bin/VBoxManage list hostonlyifs
I0207 11:30:48.349862    3511 main.go:141] libmachine: STDOUT:
{
Name:            vboxnet0
GUID:            786f6276-656e-4074-8000-0a0027000000
DHCP:            Disabled
IPAddress:       192.168.59.1
NetworkMask:     255.255.255.0
IPV6Address:     
IPV6NetworkMaskPrefixLength: 0
HardwareAddress: 0a:00:27:00:00:00
MediumType:      Ethernet
Wireless:        No
Status:          Down
VBoxNetworkName: HostInterfaceNetworking-vboxnet0

}
I0207 11:30:48.350468    3511 main.go:141] libmachine: STDERR:
{
}
I0207 11:30:48.350950    3511 main.go:141] libmachine: Searching for hostonly interface for IPv4: 192.168.59.1 and Mask: ffffff00
I0207 11:30:48.350990    3511 main.go:141] libmachine: Found: vboxnet0
I0207 11:30:48.351011    3511 main.go:141] libmachine: COMMAND: /usr/local/bin/VBoxManage list dhcpservers
I0207 11:30:48.413406    3511 main.go:141] libmachine: STDOUT:
{
NetworkName:    HostInterfaceNetworking-vboxnet0
Dhcpd IP:       192.168.59.2
LowerIPAddress: 192.168.59.100
UpperIPAddress: 192.168.59.254
NetworkMask:    255.255.255.0
Enabled:        Yes
Global Configuration:
    minLeaseTime:     default
    defaultLeaseTime: default
    maxLeaseTime:     default
    Forced options:   None
    Suppressed opts.: None
        1/legacy: 255.255.255.0
Groups:               None
Individual Configs:   None
}
I0207 11:30:48.413854    3511 main.go:141] libmachine: STDERR:
{
}
I0207 11:30:48.413981    3511 main.go:141] libmachine: Removing orphan DHCP servers...
I0207 11:30:48.413997    3511 main.go:141] libmachine: COMMAND: /usr/local/bin/VBoxManage list hostonlyifs
I0207 11:30:48.491361    3511 main.go:141] libmachine: STDOUT:
{
Name:            vboxnet0
GUID:            786f6276-656e-4074-8000-0a0027000000
DHCP:            Disabled
IPAddress:       192.168.59.1
NetworkMask:     255.255.255.0
IPV6Address:     
IPV6NetworkMaskPrefixLength: 0
HardwareAddress: 0a:00:27:00:00:00
MediumType:      Ethernet
Wireless:        No
Status:          Down
VBoxNetworkName: HostInterfaceNetworking-vboxnet0

}
I0207 11:30:48.491910    3511 main.go:141] libmachine: STDERR:
{
}
I0207 11:30:48.492064    3511 main.go:141] libmachine: Adding/Modifying DHCP server "192.168.59.15" with address range "192.168.59.100" - "192.168.59.254"...
I0207 11:30:48.492085    3511 main.go:141] libmachine: COMMAND: /usr/local/bin/VBoxManage list dhcpservers
I0207 11:30:48.548792    3511 main.go:141] libmachine: STDOUT:
{
NetworkName:    HostInterfaceNetworking-vboxnet0
Dhcpd IP:       192.168.59.2
LowerIPAddress: 192.168.59.100
UpperIPAddress: 192.168.59.254
NetworkMask:    255.255.255.0
Enabled:        Yes
Global Configuration:
    minLeaseTime:     default
    defaultLeaseTime: default
    maxLeaseTime:     default
    Forced options:   None
    Suppressed opts.: None
        1/legacy: 255.255.255.0
Groups:               None
Individual Configs:   None
}
I0207 11:30:48.549246    3511 main.go:141] libmachine: STDERR:
{
}
I0207 11:30:48.549545    3511 main.go:141] libmachine: COMMAND: /usr/local/bin/VBoxManage dhcpserver modify --netname HostInterfaceNetworking-vboxnet0 --ip 192.168.59.15 --netmask 255.255.255.0 --lowerip 192.168.59.100 --upperip 192.168.59.254 --enable
I0207 11:30:48.616000    3511 main.go:141] libmachine: STDOUT:
{
}
I0207 11:30:48.616698    3511 main.go:141] libmachine: STDERR:
{
}
I0207 11:30:48.616794    3511 main.go:141] libmachine: COMMAND: /usr/local/bin/VBoxManage modifyvm minikube --nic2 hostonly --nictype2 virtio --nicpromisc2 deny --hostonlyadapter2 vboxnet0 --cableconnected2 on
I0207 11:30:48.681517    3511 main.go:141] libmachine: STDOUT:
{
}
I0207 11:30:48.681996    3511 main.go:141] libmachine: STDERR:
{
}
I0207 11:30:48.682192    3511 main.go:141] libmachine: COMMAND: /usr/local/bin/VBoxManage modifyvm minikube --natpf1 delete ssh
I0207 11:30:48.742190    3511 main.go:141] libmachine: STDOUT:
{
}
I0207 11:30:48.743120    3511 main.go:141] libmachine: STDERR:
{
}
I0207 11:30:48.743216    3511 main.go:141] libmachine: COMMAND: /usr/local/bin/VBoxManage modifyvm minikube --natpf1 ssh,tcp,127.0.0.1,59633,,22
I0207 11:30:48.807271    3511 main.go:141] libmachine: STDOUT:
{
}
I0207 11:30:48.808260    3511 main.go:141] libmachine: STDERR:
{
}
I0207 11:30:48.808306    3511 main.go:141] libmachine: COMMAND: /usr/local/bin/VBoxManage startvm minikube --type headless
I0207 11:30:50.719526    3511 main.go:141] libmachine: STDOUT:
{
Waiting for VM "minikube" to power on...
VM "minikube" has been successfully started.
}
I0207 11:30:50.719565    3511 main.go:141] libmachine: STDERR:
{
}
I0207 11:30:50.719627    3511 main.go:141] libmachine: Checking vm logs: /Users/yaswitha/.minikube/machines/minikube/minikube/Logs/VBox.log
I0207 11:30:50.720807    3511 main.go:141] libmachine: Waiting for an IP...
I0207 11:30:50.720823    3511 main.go:141] libmachine: Getting to WaitForSSH function...
I0207 11:30:50.722115    3511 main.go:141] libmachine: Using SSH client type: native
I0207 11:30:50.754703    3511 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x1405ca0] 0x1408980 <nil>  [] 0s} 127.0.0.1 59633 <nil> <nil>}
I0207 11:30:50.754715    3511 main.go:141] libmachine: About to run SSH command:
exit 0
I0207 11:31:21.085897    3511 main.go:141] libmachine: Error dialing TCP: ssh: handshake failed: read tcp 127.0.0.1:50400->127.0.0.1:59633: read: connection reset by peer
I0207 11:31:24.183935    3511 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0207 11:31:24.184041    3511 main.go:141] libmachine: COMMAND: /usr/local/bin/VBoxManage showvminfo minikube --machinereadable
I0207 11:31:24.530152    3511 main.go:141] libmachine: STDOUT:
{
name="minikube"
groups="/"
ostype="Linux 2.6 / 3.x / 4.x (64-bit)"
UUID="7785180f-7acb-4aa1-be56-47fec5875cea"
CfgFile="/Users/yaswitha/.minikube/machines/minikube/minikube/minikube.vbox"
SnapFldr="/Users/yaswitha/.minikube/machines/minikube/minikube/Snapshots"
LogFldr="/Users/yaswitha/.minikube/machines/minikube/minikube/Logs"
hardwareuuid="7785180f-7acb-4aa1-be56-47fec5875cea"
memory=4096
pagefusion="off"
vram=8
cpuexecutioncap=100
hpet="on"
cpu-profile="host"
chipset="piix3"
firmware="BIOS"
cpus=2
pae="on"
longmode="on"
triplefaultreset="off"
apic="on"
x2apic="off"
nested-hw-virt="off"
cpuid-portability-level=0
bootmenu="disabled"
boot1="dvd"
boot2="dvd"
boot3="disk"
boot4="none"
acpi="on"
ioapic="on"
biosapic="apic"
biossystemtimeoffset=0
rtcuseutc="on"
hwvirtex="on"
nestedpaging="on"
largepages="on"
vtxvpid="on"
vtxux="on"
paravirtprovider="default"
effparavirtprovider="kvm"
VMState="running"
VMStateChangeTime="2024-02-07T03:30:50.709000000"
graphicscontroller="vboxvga"
monitorcount=1
accelerate3d="off"
accelerate2dvideo="off"
teleporterenabled="off"
teleporterport=0
teleporteraddress=""
teleporterpassword=""
tracing-enabled="off"
tracing-allow-vm-access="off"
tracing-config=""
autostart-enabled="off"
autostart-delay=0
defaultfrontend=""
vmprocpriority="default"
storagecontrollername0="SATA"
storagecontrollertype0="IntelAhci"
storagecontrollerinstance0="0"
storagecontrollermaxportcount0="30"
storagecontrollerportcount0="30"
storagecontrollerbootable0="on"
"SATA-0-0"="/Users/yaswitha/.minikube/machines/minikube/boot2docker.iso"
"SATA-ImageUUID-0-0"="e2e6d007-9e65-4ada-ac79-de64d81e96bf"
"SATA-tempeject"="off"
"SATA-IsEjected"="off"
"SATA-1-0"="/Users/yaswitha/.minikube/machines/minikube/disk.vmdk"
"SATA-ImageUUID-1-0"="b71bfcd5-e591-40ba-89ef-042783c5fb47"
"SATA-2-0"="none"
"SATA-3-0"="none"
"SATA-4-0"="none"
"SATA-5-0"="none"
"SATA-6-0"="none"
"SATA-7-0"="none"
"SATA-8-0"="none"
"SATA-9-0"="none"
"SATA-10-0"="none"
"SATA-11-0"="none"
"SATA-12-0"="none"
"SATA-13-0"="none"
"SATA-14-0"="none"
"SATA-15-0"="none"
"SATA-16-0"="none"
"SATA-17-0"="none"
"SATA-18-0"="none"
"SATA-19-0"="none"
"SATA-20-0"="none"
"SATA-21-0"="none"
"SATA-22-0"="none"
"SATA-23-0"="none"
"SATA-24-0"="none"
"SATA-25-0"="none"
"SATA-26-0"="none"
"SATA-27-0"="none"
"SATA-28-0"="none"
"SATA-29-0"="none"
natnet1="nat"
macaddress1="080027C1DD90"
cableconnected1="on"
nic1="nat"
nictype1="virtio"
nicspeed1="0"
mtu="0"
sockSnd="64"
sockRcv="64"
tcpWndSnd="64"
tcpWndRcv="64"
Forwarding(0)="ssh,tcp,127.0.0.1,59633,,22"
hostonlyadapter2="vboxnet0"
macaddress2="080027594EE2"
cableconnected2="on"
nic2="hostonly"
nictype2="virtio"
nicspeed2="0"
nic3="none"
nic4="none"
nic5="none"
nic6="none"
nic7="none"
nic8="none"
hidpointing="ps2mouse"
hidkeyboard="ps2kbd"
uart1="off"
uart2="off"
uart3="off"
uart4="off"
lpt1="off"
lpt2="off"
audio="coreaudio"
audio_out="off"
audio_in="off"
clipboard="disabled"
draganddrop="disabled"
SessionName="headless"
VideoMode="720,400,0"@0,0 1
vrde="off"
usb="off"
ehci="off"
xhci="off"
SharedFolderNameMachineMapping1="Users"
SharedFolderPathMachineMapping1="/Users"
VRDEActiveConnection="off"
VRDEClients==0
videocap="off"
videocapaudio="off"
capturescreens="0"
capturefilename="/Users/yaswitha/.minikube/machines/minikube/minikube/minikube.webm"
captureres="1024x768"
capturevideorate=512
capturevideofps=25
captureopts=""
GuestMemoryBalloon=0
GuestOSType="Linux26_64"
GuestAdditionsRunLevel=2
GuestAdditionsVersion="6.0.0 r127566"
GuestAdditionsFacility_VirtualBox Base Driver=50,1707276680237
GuestAdditionsFacility_VirtualBox System Service=50,1707276680982
GuestAdditionsFacility_Seamless Mode=0,1707276680089
GuestAdditionsFacility_Graphics Mode=0,1707276680088
}
I0207 11:31:24.530243    3511 main.go:141] libmachine: STDERR:
{
}
I0207 11:31:24.530506    3511 main.go:141] libmachine: COMMAND: /usr/local/bin/VBoxManage showvminfo minikube --machinereadable
I0207 11:31:24.687630    3511 main.go:141] libmachine: STDOUT:
{
name="minikube"
groups="/"
ostype="Linux 2.6 / 3.x / 4.x (64-bit)"
UUID="7785180f-7acb-4aa1-be56-47fec5875cea"
CfgFile="/Users/yaswitha/.minikube/machines/minikube/minikube/minikube.vbox"
SnapFldr="/Users/yaswitha/.minikube/machines/minikube/minikube/Snapshots"
LogFldr="/Users/yaswitha/.minikube/machines/minikube/minikube/Logs"
hardwareuuid="7785180f-7acb-4aa1-be56-47fec5875cea"
memory=4096
pagefusion="off"
vram=8
cpuexecutioncap=100
hpet="on"
cpu-profile="host"
chipset="piix3"
firmware="BIOS"
cpus=2
pae="on"
longmode="on"
triplefaultreset="off"
apic="on"
x2apic="off"
nested-hw-virt="off"
cpuid-portability-level=0
bootmenu="disabled"
boot1="dvd"
boot2="dvd"
boot3="disk"
boot4="none"
acpi="on"
ioapic="on"
biosapic="apic"
biossystemtimeoffset=0
rtcuseutc="on"
hwvirtex="on"
nestedpaging="on"
largepages="on"
vtxvpid="on"
vtxux="on"
paravirtprovider="default"
effparavirtprovider="kvm"
VMState="running"
VMStateChangeTime="2024-02-07T03:30:50.709000000"
graphicscontroller="vboxvga"
monitorcount=1
accelerate3d="off"
accelerate2dvideo="off"
teleporterenabled="off"
teleporterport=0
teleporteraddress=""
teleporterpassword=""
tracing-enabled="off"
tracing-allow-vm-access="off"
tracing-config=""
autostart-enabled="off"
autostart-delay=0
defaultfrontend=""
vmprocpriority="default"
storagecontrollername0="SATA"
storagecontrollertype0="IntelAhci"
storagecontrollerinstance0="0"
storagecontrollermaxportcount0="30"
storagecontrollerportcount0="30"
storagecontrollerbootable0="on"
"SATA-0-0"="/Users/yaswitha/.minikube/machines/minikube/boot2docker.iso"
"SATA-ImageUUID-0-0"="e2e6d007-9e65-4ada-ac79-de64d81e96bf"
"SATA-tempeject"="off"
"SATA-IsEjected"="off"
"SATA-1-0"="/Users/yaswitha/.minikube/machines/minikube/disk.vmdk"
"SATA-ImageUUID-1-0"="b71bfcd5-e591-40ba-89ef-042783c5fb47"
"SATA-2-0"="none"
"SATA-3-0"="none"
"SATA-4-0"="none"
"SATA-5-0"="none"
"SATA-6-0"="none"
"SATA-7-0"="none"
"SATA-8-0"="none"
"SATA-9-0"="none"
"SATA-10-0"="none"
"SATA-11-0"="none"
"SATA-12-0"="none"
"SATA-13-0"="none"
"SATA-14-0"="none"
"SATA-15-0"="none"
"SATA-16-0"="none"
"SATA-17-0"="none"
"SATA-18-0"="none"
"SATA-19-0"="none"
"SATA-20-0"="none"
"SATA-21-0"="none"
"SATA-22-0"="none"
"SATA-23-0"="none"
"SATA-24-0"="none"
"SATA-25-0"="none"
"SATA-26-0"="none"
"SATA-27-0"="none"
"SATA-28-0"="none"
"SATA-29-0"="none"
natnet1="nat"
macaddress1="080027C1DD90"
cableconnected1="on"
nic1="nat"
nictype1="virtio"
nicspeed1="0"
mtu="0"
sockSnd="64"
sockRcv="64"
tcpWndSnd="64"
tcpWndRcv="64"
Forwarding(0)="ssh,tcp,127.0.0.1,59633,,22"
hostonlyadapter2="vboxnet0"
macaddress2="080027594EE2"
cableconnected2="on"
nic2="hostonly"
nictype2="virtio"
nicspeed2="0"
nic3="none"
nic4="none"
nic5="none"
nic6="none"
nic7="none"
nic8="none"
hidpointing="ps2mouse"
hidkeyboard="ps2kbd"
uart1="off"
uart2="off"
uart3="off"
uart4="off"
lpt1="off"
lpt2="off"
audio="coreaudio"
audio_out="off"
audio_in="off"
clipboard="disabled"
draganddrop="disabled"
SessionName="headless"
VideoMode="720,400,0"@0,0 1
vrde="off"
usb="off"
ehci="off"
xhci="off"
SharedFolderNameMachineMapping1="Users"
SharedFolderPathMachineMapping1="/Users"
VRDEActiveConnection="off"
VRDEClients==0
videocap="off"
videocapaudio="off"
capturescreens="0"
capturefilename="/Users/yaswitha/.minikube/machines/minikube/minikube/minikube.webm"
captureres="1024x768"
capturevideorate=512
capturevideofps=25
captureopts=""
GuestMemoryBalloon=0
GuestOSType="Linux26_64"
GuestAdditionsRunLevel=2
GuestAdditionsVersion="6.0.0 r127566"
GuestAdditionsFacility_VirtualBox Base Driver=50,1707276680237
GuestAdditionsFacility_VirtualBox System Service=50,1707276680982
GuestAdditionsFacility_Seamless Mode=0,1707276680089
GuestAdditionsFacility_Graphics Mode=0,1707276680088
}
I0207 11:31:24.687730    3511 main.go:141] libmachine: STDERR:
{
}
I0207 11:31:24.688151    3511 main.go:141] libmachine: Host-only MAC: 080027594ee2

I0207 11:31:24.688281    3511 main.go:141] libmachine: Using SSH client type: native
I0207 11:31:24.690175    3511 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x1405ca0] 0x1408980 <nil>  [] 0s} 127.0.0.1 59633 <nil> <nil>}
I0207 11:31:24.690190    3511 main.go:141] libmachine: About to run SSH command:
ip addr show
I0207 11:31:24.789568    3511 main.go:141] libmachine: SSH cmd err, output: <nil>: 1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
2: eth0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP group default qlen 1000
    link/ether 08:00:27:c1:dd:90 brd ff:ff:ff:ff:ff:ff
    inet 10.0.2.15/24 brd 10.0.2.255 scope global dynamic eth0
       valid_lft 86396sec preferred_lft 86396sec
3: eth1: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP group default qlen 1000
    link/ether 08:00:27:59:4e:e2 brd ff:ff:ff:ff:ff:ff
    inet 192.168.59.100/24 brd 192.168.59.255 scope global dynamic eth1
       valid_lft 596sec preferred_lft 596sec
4: sit0@NONE: <NOARP> mtu 1480 qdisc noop state DOWN group default qlen 1000
    link/sit 0.0.0.0 brd 0.0.0.0

I0207 11:31:24.789667    3511 main.go:141] libmachine: SSH returned: 1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
2: eth0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP group default qlen 1000
    link/ether 08:00:27:c1:dd:90 brd ff:ff:ff:ff:ff:ff
    inet 10.0.2.15/24 brd 10.0.2.255 scope global dynamic eth0
       valid_lft 86396sec preferred_lft 86396sec
3: eth1: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP group default qlen 1000
    link/ether 08:00:27:59:4e:e2 brd ff:ff:ff:ff:ff:ff
    inet 192.168.59.100/24 brd 192.168.59.255 scope global dynamic eth1
       valid_lft 596sec preferred_lft 596sec
4: sit0@NONE: <NOARP> mtu 1480 qdisc noop state DOWN group default qlen 1000
    link/sit 0.0.0.0 brd 0.0.0.0

END SSH

I0207 11:31:24.789694    3511 main.go:141] libmachine: IP is 192.168.59.100
I0207 11:31:24.789714    3511 main.go:141] libmachine: COMMAND: /usr/local/bin/VBoxManage showvminfo minikube --machinereadable
I0207 11:31:25.030382    3511 main.go:141] libmachine: STDOUT:
{
name="minikube"
groups="/"
ostype="Linux 2.6 / 3.x / 4.x (64-bit)"
UUID="7785180f-7acb-4aa1-be56-47fec5875cea"
CfgFile="/Users/yaswitha/.minikube/machines/minikube/minikube/minikube.vbox"
SnapFldr="/Users/yaswitha/.minikube/machines/minikube/minikube/Snapshots"
LogFldr="/Users/yaswitha/.minikube/machines/minikube/minikube/Logs"
hardwareuuid="7785180f-7acb-4aa1-be56-47fec5875cea"
memory=4096
pagefusion="off"
vram=8
cpuexecutioncap=100
hpet="on"
cpu-profile="host"
chipset="piix3"
firmware="BIOS"
cpus=2
pae="on"
longmode="on"
triplefaultreset="off"
apic="on"
x2apic="off"
nested-hw-virt="off"
cpuid-portability-level=0
bootmenu="disabled"
boot1="dvd"
boot2="dvd"
boot3="disk"
boot4="none"
acpi="on"
ioapic="on"
biosapic="apic"
biossystemtimeoffset=0
rtcuseutc="on"
hwvirtex="on"
nestedpaging="on"
largepages="on"
vtxvpid="on"
vtxux="on"
paravirtprovider="default"
effparavirtprovider="kvm"
VMState="running"
VMStateChangeTime="2024-02-07T03:30:50.709000000"
graphicscontroller="vboxvga"
monitorcount=1
accelerate3d="off"
accelerate2dvideo="off"
teleporterenabled="off"
teleporterport=0
teleporteraddress=""
teleporterpassword=""
tracing-enabled="off"
tracing-allow-vm-access="off"
tracing-config=""
autostart-enabled="off"
autostart-delay=0
defaultfrontend=""
vmprocpriority="default"
storagecontrollername0="SATA"
storagecontrollertype0="IntelAhci"
storagecontrollerinstance0="0"
storagecontrollermaxportcount0="30"
storagecontrollerportcount0="30"
storagecontrollerbootable0="on"
"SATA-0-0"="/Users/yaswitha/.minikube/machines/minikube/boot2docker.iso"
"SATA-ImageUUID-0-0"="e2e6d007-9e65-4ada-ac79-de64d81e96bf"
"SATA-tempeject"="off"
"SATA-IsEjected"="off"
"SATA-1-0"="/Users/yaswitha/.minikube/machines/minikube/disk.vmdk"
"SATA-ImageUUID-1-0"="b71bfcd5-e591-40ba-89ef-042783c5fb47"
"SATA-2-0"="none"
"SATA-3-0"="none"
"SATA-4-0"="none"
"SATA-5-0"="none"
"SATA-6-0"="none"
"SATA-7-0"="none"
"SATA-8-0"="none"
"SATA-9-0"="none"
"SATA-10-0"="none"
"SATA-11-0"="none"
"SATA-12-0"="none"
"SATA-13-0"="none"
"SATA-14-0"="none"
"SATA-15-0"="none"
"SATA-16-0"="none"
"SATA-17-0"="none"
"SATA-18-0"="none"
"SATA-19-0"="none"
"SATA-20-0"="none"
"SATA-21-0"="none"
"SATA-22-0"="none"
"SATA-23-0"="none"
"SATA-24-0"="none"
"SATA-25-0"="none"
"SATA-26-0"="none"
"SATA-27-0"="none"
"SATA-28-0"="none"
"SATA-29-0"="none"
natnet1="nat"
macaddress1="080027C1DD90"
cableconnected1="on"
nic1="nat"
nictype1="virtio"
nicspeed1="0"
mtu="0"
sockSnd="64"
sockRcv="64"
tcpWndSnd="64"
tcpWndRcv="64"
Forwarding(0)="ssh,tcp,127.0.0.1,59633,,22"
hostonlyadapter2="vboxnet0"
macaddress2="080027594EE2"
cableconnected2="on"
nic2="hostonly"
nictype2="virtio"
nicspeed2="0"
nic3="none"
nic4="none"
nic5="none"
nic6="none"
nic7="none"
nic8="none"
hidpointing="ps2mouse"
hidkeyboard="ps2kbd"
uart1="off"
uart2="off"
uart3="off"
uart4="off"
lpt1="off"
lpt2="off"
audio="coreaudio"
audio_out="off"
audio_in="off"
clipboard="disabled"
draganddrop="disabled"
SessionName="headless"
VideoMode="720,400,0"@0,0 1
vrde="off"
usb="off"
ehci="off"
xhci="off"
SharedFolderNameMachineMapping1="Users"
SharedFolderPathMachineMapping1="/Users"
VRDEActiveConnection="off"
VRDEClients==0
videocap="off"
videocapaudio="off"
capturescreens="0"
capturefilename="/Users/yaswitha/.minikube/machines/minikube/minikube/minikube.webm"
captureres="1024x768"
capturevideorate=512
capturevideofps=25
captureopts=""
GuestMemoryBalloon=0
GuestOSType="Linux26_64"
GuestAdditionsRunLevel=2
GuestAdditionsVersion="6.0.0 r127566"
GuestAdditionsFacility_VirtualBox Base Driver=50,1707276680237
GuestAdditionsFacility_VirtualBox System Service=50,1707276680982
GuestAdditionsFacility_Seamless Mode=0,1707276680089
GuestAdditionsFacility_Graphics Mode=0,1707276680088
}
I0207 11:31:25.030439    3511 main.go:141] libmachine: STDERR:
{
}
I0207 11:31:25.030571    3511 main.go:141] libmachine: COMMAND: /usr/local/bin/VBoxManage showvminfo minikube --machinereadable
I0207 11:31:25.209770    3511 main.go:141] libmachine: STDOUT:
{
name="minikube"
groups="/"
ostype="Linux 2.6 / 3.x / 4.x (64-bit)"
UUID="7785180f-7acb-4aa1-be56-47fec5875cea"
CfgFile="/Users/yaswitha/.minikube/machines/minikube/minikube/minikube.vbox"
SnapFldr="/Users/yaswitha/.minikube/machines/minikube/minikube/Snapshots"
LogFldr="/Users/yaswitha/.minikube/machines/minikube/minikube/Logs"
hardwareuuid="7785180f-7acb-4aa1-be56-47fec5875cea"
memory=4096
pagefusion="off"
vram=8
cpuexecutioncap=100
hpet="on"
cpu-profile="host"
chipset="piix3"
firmware="BIOS"
cpus=2
pae="on"
longmode="on"
triplefaultreset="off"
apic="on"
x2apic="off"
nested-hw-virt="off"
cpuid-portability-level=0
bootmenu="disabled"
boot1="dvd"
boot2="dvd"
boot3="disk"
boot4="none"
acpi="on"
ioapic="on"
biosapic="apic"
biossystemtimeoffset=0
rtcuseutc="on"
hwvirtex="on"
nestedpaging="on"
largepages="on"
vtxvpid="on"
vtxux="on"
paravirtprovider="default"
effparavirtprovider="kvm"
VMState="running"
VMStateChangeTime="2024-02-07T03:30:50.709000000"
graphicscontroller="vboxvga"
monitorcount=1
accelerate3d="off"
accelerate2dvideo="off"
teleporterenabled="off"
teleporterport=0
teleporteraddress=""
teleporterpassword=""
tracing-enabled="off"
tracing-allow-vm-access="off"
tracing-config=""
autostart-enabled="off"
autostart-delay=0
defaultfrontend=""
vmprocpriority="default"
storagecontrollername0="SATA"
storagecontrollertype0="IntelAhci"
storagecontrollerinstance0="0"
storagecontrollermaxportcount0="30"
storagecontrollerportcount0="30"
storagecontrollerbootable0="on"
"SATA-0-0"="/Users/yaswitha/.minikube/machines/minikube/boot2docker.iso"
"SATA-ImageUUID-0-0"="e2e6d007-9e65-4ada-ac79-de64d81e96bf"
"SATA-tempeject"="off"
"SATA-IsEjected"="off"
"SATA-1-0"="/Users/yaswitha/.minikube/machines/minikube/disk.vmdk"
"SATA-ImageUUID-1-0"="b71bfcd5-e591-40ba-89ef-042783c5fb47"
"SATA-2-0"="none"
"SATA-3-0"="none"
"SATA-4-0"="none"
"SATA-5-0"="none"
"SATA-6-0"="none"
"SATA-7-0"="none"
"SATA-8-0"="none"
"SATA-9-0"="none"
"SATA-10-0"="none"
"SATA-11-0"="none"
"SATA-12-0"="none"
"SATA-13-0"="none"
"SATA-14-0"="none"
"SATA-15-0"="none"
"SATA-16-0"="none"
"SATA-17-0"="none"
"SATA-18-0"="none"
"SATA-19-0"="none"
"SATA-20-0"="none"
"SATA-21-0"="none"
"SATA-22-0"="none"
"SATA-23-0"="none"
"SATA-24-0"="none"
"SATA-25-0"="none"
"SATA-26-0"="none"
"SATA-27-0"="none"
"SATA-28-0"="none"
"SATA-29-0"="none"
natnet1="nat"
macaddress1="080027C1DD90"
cableconnected1="on"
nic1="nat"
nictype1="virtio"
nicspeed1="0"
mtu="0"
sockSnd="64"
sockRcv="64"
tcpWndSnd="64"
tcpWndRcv="64"
Forwarding(0)="ssh,tcp,127.0.0.1,59633,,22"
hostonlyadapter2="vboxnet0"
macaddress2="080027594EE2"
cableconnected2="on"
nic2="hostonly"
nictype2="virtio"
nicspeed2="0"
nic3="none"
nic4="none"
nic5="none"
nic6="none"
nic7="none"
nic8="none"
hidpointing="ps2mouse"
hidkeyboard="ps2kbd"
uart1="off"
uart2="off"
uart3="off"
uart4="off"
lpt1="off"
lpt2="off"
audio="coreaudio"
audio_out="off"
audio_in="off"
clipboard="disabled"
draganddrop="disabled"
SessionName="headless"
VideoMode="720,400,0"@0,0 1
vrde="off"
usb="off"
ehci="off"
xhci="off"
SharedFolderNameMachineMapping1="Users"
SharedFolderPathMachineMapping1="/Users"
VRDEActiveConnection="off"
VRDEClients==0
videocap="off"
videocapaudio="off"
capturescreens="0"
capturefilename="/Users/yaswitha/.minikube/machines/minikube/minikube/minikube.webm"
captureres="1024x768"
capturevideorate=512
capturevideofps=25
captureopts=""
GuestMemoryBalloon=0
GuestOSType="Linux26_64"
GuestAdditionsRunLevel=2
GuestAdditionsVersion="6.0.0 r127566"
GuestAdditionsFacility_VirtualBox Base Driver=50,1707276680237
GuestAdditionsFacility_VirtualBox System Service=50,1707276680982
GuestAdditionsFacility_Seamless Mode=0,1707276680089
GuestAdditionsFacility_Graphics Mode=0,1707276680088
}
I0207 11:31:25.209863    3511 main.go:141] libmachine: STDERR:
{
}
I0207 11:31:25.210243    3511 main.go:141] libmachine: Host-only MAC: 080027594ee2

I0207 11:31:25.211654    3511 main.go:141] libmachine: Using SSH client type: native
I0207 11:31:25.212423    3511 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x1405ca0] 0x1408980 <nil>  [] 0s} 127.0.0.1 59633 <nil> <nil>}
I0207 11:31:25.212439    3511 main.go:141] libmachine: About to run SSH command:
ip addr show
I0207 11:31:25.306367    3511 main.go:141] libmachine: SSH cmd err, output: <nil>: 1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
2: eth0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP group default qlen 1000
    link/ether 08:00:27:c1:dd:90 brd ff:ff:ff:ff:ff:ff
    inet 10.0.2.15/24 brd 10.0.2.255 scope global dynamic eth0
       valid_lft 86396sec preferred_lft 86396sec
3: eth1: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP group default qlen 1000
    link/ether 08:00:27:59:4e:e2 brd ff:ff:ff:ff:ff:ff
    inet 192.168.59.100/24 brd 192.168.59.255 scope global dynamic eth1
       valid_lft 596sec preferred_lft 596sec
4: sit0@NONE: <NOARP> mtu 1480 qdisc noop state DOWN group default qlen 1000
    link/sit 0.0.0.0 brd 0.0.0.0

I0207 11:31:25.306443    3511 main.go:141] libmachine: SSH returned: 1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
2: eth0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP group default qlen 1000
    link/ether 08:00:27:c1:dd:90 brd ff:ff:ff:ff:ff:ff
    inet 10.0.2.15/24 brd 10.0.2.255 scope global dynamic eth0
       valid_lft 86396sec preferred_lft 86396sec
3: eth1: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP group default qlen 1000
    link/ether 08:00:27:59:4e:e2 brd ff:ff:ff:ff:ff:ff
    inet 192.168.59.100/24 brd 192.168.59.255 scope global dynamic eth1
       valid_lft 596sec preferred_lft 596sec
4: sit0@NONE: <NOARP> mtu 1480 qdisc noop state DOWN group default qlen 1000
    link/sit 0.0.0.0 brd 0.0.0.0

END SSH

I0207 11:31:25.306493    3511 main.go:141] libmachine: COMMAND: /usr/local/bin/VBoxManage list hostonlyifs
I0207 11:31:25.406910    3511 main.go:141] libmachine: STDOUT:
{
Name:            vboxnet0
GUID:            786f6276-656e-4074-8000-0a0027000000
DHCP:            Disabled
IPAddress:       192.168.59.1
NetworkMask:     255.255.255.0
IPV6Address:     
IPV6NetworkMaskPrefixLength: 0
HardwareAddress: 0a:00:27:00:00:00
MediumType:      Ethernet
Wireless:        No
Status:          Up
VBoxNetworkName: HostInterfaceNetworking-vboxnet0

}
I0207 11:31:25.406981    3511 main.go:141] libmachine: STDERR:
{
}
I0207 11:31:25.407696    3511 main.go:141] libmachine: Found: vboxnet0
I0207 11:31:25.413128    3511 main.go:141] libmachine: COMMAND: /usr/local/bin/VBoxManage showvminfo minikube --machinereadable
I0207 11:31:25.565510    3511 main.go:141] libmachine: STDOUT:
{
name="minikube"
groups="/"
ostype="Linux 2.6 / 3.x / 4.x (64-bit)"
UUID="7785180f-7acb-4aa1-be56-47fec5875cea"
CfgFile="/Users/yaswitha/.minikube/machines/minikube/minikube/minikube.vbox"
SnapFldr="/Users/yaswitha/.minikube/machines/minikube/minikube/Snapshots"
LogFldr="/Users/yaswitha/.minikube/machines/minikube/minikube/Logs"
hardwareuuid="7785180f-7acb-4aa1-be56-47fec5875cea"
memory=4096
pagefusion="off"
vram=8
cpuexecutioncap=100
hpet="on"
cpu-profile="host"
chipset="piix3"
firmware="BIOS"
cpus=2
pae="on"
longmode="on"
triplefaultreset="off"
apic="on"
x2apic="off"
nested-hw-virt="off"
cpuid-portability-level=0
bootmenu="disabled"
boot1="dvd"
boot2="dvd"
boot3="disk"
boot4="none"
acpi="on"
ioapic="on"
biosapic="apic"
biossystemtimeoffset=0
rtcuseutc="on"
hwvirtex="on"
nestedpaging="on"
largepages="on"
vtxvpid="on"
vtxux="on"
paravirtprovider="default"
effparavirtprovider="kvm"
VMState="running"
VMStateChangeTime="2024-02-07T03:30:50.709000000"
graphicscontroller="vboxvga"
monitorcount=1
accelerate3d="off"
accelerate2dvideo="off"
teleporterenabled="off"
teleporterport=0
teleporteraddress=""
teleporterpassword=""
tracing-enabled="off"
tracing-allow-vm-access="off"
tracing-config=""
autostart-enabled="off"
autostart-delay=0
defaultfrontend=""
vmprocpriority="default"
storagecontrollername0="SATA"
storagecontrollertype0="IntelAhci"
storagecontrollerinstance0="0"
storagecontrollermaxportcount0="30"
storagecontrollerportcount0="30"
storagecontrollerbootable0="on"
"SATA-0-0"="/Users/yaswitha/.minikube/machines/minikube/boot2docker.iso"
"SATA-ImageUUID-0-0"="e2e6d007-9e65-4ada-ac79-de64d81e96bf"
"SATA-tempeject"="off"
"SATA-IsEjected"="off"
"SATA-1-0"="/Users/yaswitha/.minikube/machines/minikube/disk.vmdk"
"SATA-ImageUUID-1-0"="b71bfcd5-e591-40ba-89ef-042783c5fb47"
"SATA-2-0"="none"
"SATA-3-0"="none"
"SATA-4-0"="none"
"SATA-5-0"="none"
"SATA-6-0"="none"
"SATA-7-0"="none"
"SATA-8-0"="none"
"SATA-9-0"="none"
"SATA-10-0"="none"
"SATA-11-0"="none"
"SATA-12-0"="none"
"SATA-13-0"="none"
"SATA-14-0"="none"
"SATA-15-0"="none"
"SATA-16-0"="none"
"SATA-17-0"="none"
"SATA-18-0"="none"
"SATA-19-0"="none"
"SATA-20-0"="none"
"SATA-21-0"="none"
"SATA-22-0"="none"
"SATA-23-0"="none"
"SATA-24-0"="none"
"SATA-25-0"="none"
"SATA-26-0"="none"
"SATA-27-0"="none"
"SATA-28-0"="none"
"SATA-29-0"="none"
natnet1="nat"
macaddress1="080027C1DD90"
cableconnected1="on"
nic1="nat"
nictype1="virtio"
nicspeed1="0"
mtu="0"
sockSnd="64"
sockRcv="64"
tcpWndSnd="64"
tcpWndRcv="64"
Forwarding(0)="ssh,tcp,127.0.0.1,59633,,22"
hostonlyadapter2="vboxnet0"
macaddress2="080027594EE2"
cableconnected2="on"
nic2="hostonly"
nictype2="virtio"
nicspeed2="0"
nic3="none"
nic4="none"
nic5="none"
nic6="none"
nic7="none"
nic8="none"
hidpointing="ps2mouse"
hidkeyboard="ps2kbd"
uart1="off"
uart2="off"
uart3="off"
uart4="off"
lpt1="off"
lpt2="off"
audio="coreaudio"
audio_out="off"
audio_in="off"
clipboard="disabled"
draganddrop="disabled"
SessionName="headless"
VideoMode="720,400,0"@0,0 1
vrde="off"
usb="off"
ehci="off"
xhci="off"
SharedFolderNameMachineMapping1="Users"
SharedFolderPathMachineMapping1="/Users"
VRDEActiveConnection="off"
VRDEClients==0
videocap="off"
videocapaudio="off"
capturescreens="0"
capturefilename="/Users/yaswitha/.minikube/machines/minikube/minikube/minikube.webm"
captureres="1024x768"
capturevideorate=512
capturevideofps=25
captureopts=""
GuestMemoryBalloon=0
GuestOSType="Linux26_64"
GuestAdditionsRunLevel=2
GuestAdditionsVersion="6.0.0 r127566"
GuestAdditionsFacility_VirtualBox Base Driver=50,1707276680237
GuestAdditionsFacility_VirtualBox System Service=50,1707276680982
GuestAdditionsFacility_Seamless Mode=0,1707276680089
GuestAdditionsFacility_Graphics Mode=0,1707276680088
}
I0207 11:31:25.565572    3511 main.go:141] libmachine: STDERR:
{
}
I0207 11:31:25.565826    3511 main.go:141] libmachine: COMMAND: /usr/local/bin/VBoxManage showvminfo minikube --machinereadable
I0207 11:31:25.729783    3511 main.go:141] libmachine: STDOUT:
{
name="minikube"
groups="/"
ostype="Linux 2.6 / 3.x / 4.x (64-bit)"
UUID="7785180f-7acb-4aa1-be56-47fec5875cea"
CfgFile="/Users/yaswitha/.minikube/machines/minikube/minikube/minikube.vbox"
SnapFldr="/Users/yaswitha/.minikube/machines/minikube/minikube/Snapshots"
LogFldr="/Users/yaswitha/.minikube/machines/minikube/minikube/Logs"
hardwareuuid="7785180f-7acb-4aa1-be56-47fec5875cea"
memory=4096
pagefusion="off"
vram=8
cpuexecutioncap=100
hpet="on"
cpu-profile="host"
chipset="piix3"
firmware="BIOS"
cpus=2
pae="on"
longmode="on"
triplefaultreset="off"
apic="on"
x2apic="off"
nested-hw-virt="off"
cpuid-portability-level=0
bootmenu="disabled"
boot1="dvd"
boot2="dvd"
boot3="disk"
boot4="none"
acpi="on"
ioapic="on"
biosapic="apic"
biossystemtimeoffset=0
rtcuseutc="on"
hwvirtex="on"
nestedpaging="on"
largepages="on"
vtxvpid="on"
vtxux="on"
paravirtprovider="default"
effparavirtprovider="kvm"
VMState="running"
VMStateChangeTime="2024-02-07T03:30:50.709000000"
graphicscontroller="vboxvga"
monitorcount=1
accelerate3d="off"
accelerate2dvideo="off"
teleporterenabled="off"
teleporterport=0
teleporteraddress=""
teleporterpassword=""
tracing-enabled="off"
tracing-allow-vm-access="off"
tracing-config=""
autostart-enabled="off"
autostart-delay=0
defaultfrontend=""
vmprocpriority="default"
storagecontrollername0="SATA"
storagecontrollertype0="IntelAhci"
storagecontrollerinstance0="0"
storagecontrollermaxportcount0="30"
storagecontrollerportcount0="30"
storagecontrollerbootable0="on"
"SATA-0-0"="/Users/yaswitha/.minikube/machines/minikube/boot2docker.iso"
"SATA-ImageUUID-0-0"="e2e6d007-9e65-4ada-ac79-de64d81e96bf"
"SATA-tempeject"="off"
"SATA-IsEjected"="off"
"SATA-1-0"="/Users/yaswitha/.minikube/machines/minikube/disk.vmdk"
"SATA-ImageUUID-1-0"="b71bfcd5-e591-40ba-89ef-042783c5fb47"
"SATA-2-0"="none"
"SATA-3-0"="none"
"SATA-4-0"="none"
"SATA-5-0"="none"
"SATA-6-0"="none"
"SATA-7-0"="none"
"SATA-8-0"="none"
"SATA-9-0"="none"
"SATA-10-0"="none"
"SATA-11-0"="none"
"SATA-12-0"="none"
"SATA-13-0"="none"
"SATA-14-0"="none"
"SATA-15-0"="none"
"SATA-16-0"="none"
"SATA-17-0"="none"
"SATA-18-0"="none"
"SATA-19-0"="none"
"SATA-20-0"="none"
"SATA-21-0"="none"
"SATA-22-0"="none"
"SATA-23-0"="none"
"SATA-24-0"="none"
"SATA-25-0"="none"
"SATA-26-0"="none"
"SATA-27-0"="none"
"SATA-28-0"="none"
"SATA-29-0"="none"
natnet1="nat"
macaddress1="080027C1DD90"
cableconnected1="on"
nic1="nat"
nictype1="virtio"
nicspeed1="0"
mtu="0"
sockSnd="64"
sockRcv="64"
tcpWndSnd="64"
tcpWndRcv="64"
Forwarding(0)="ssh,tcp,127.0.0.1,59633,,22"
hostonlyadapter2="vboxnet0"
macaddress2="080027594EE2"
cableconnected2="on"
nic2="hostonly"
nictype2="virtio"
nicspeed2="0"
nic3="none"
nic4="none"
nic5="none"
nic6="none"
nic7="none"
nic8="none"
hidpointing="ps2mouse"
hidkeyboard="ps2kbd"
uart1="off"
uart2="off"
uart3="off"
uart4="off"
lpt1="off"
lpt2="off"
audio="coreaudio"
audio_out="off"
audio_in="off"
clipboard="disabled"
draganddrop="disabled"
SessionName="headless"
VideoMode="720,400,0"@0,0 1
vrde="off"
usb="off"
ehci="off"
xhci="off"
SharedFolderNameMachineMapping1="Users"
SharedFolderPathMachineMapping1="/Users"
VRDEActiveConnection="off"
VRDEClients==0
videocap="off"
videocapaudio="off"
capturescreens="0"
capturefilename="/Users/yaswitha/.minikube/machines/minikube/minikube/minikube.webm"
captureres="1024x768"
capturevideorate=512
capturevideofps=25
captureopts=""
GuestMemoryBalloon=0
GuestOSType="Linux26_64"
GuestAdditionsRunLevel=2
GuestAdditionsVersion="6.0.0 r127566"
GuestAdditionsFacility_VirtualBox Base Driver=50,1707276680237
GuestAdditionsFacility_VirtualBox System Service=50,1707276680982
GuestAdditionsFacility_Seamless Mode=0,1707276680089
GuestAdditionsFacility_Graphics Mode=0,1707276680088
}
I0207 11:31:25.729861    3511 main.go:141] libmachine: STDERR:
{
}
I0207 11:31:25.730213    3511 main.go:141] libmachine: Host-only MAC: 080027594ee2

I0207 11:31:25.730283    3511 main.go:141] libmachine: Using SSH client type: native
I0207 11:31:25.730930    3511 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x1405ca0] 0x1408980 <nil>  [] 0s} 127.0.0.1 59633 <nil> <nil>}
I0207 11:31:25.730941    3511 main.go:141] libmachine: About to run SSH command:
ip addr show
I0207 11:31:25.828504    3511 main.go:141] libmachine: SSH cmd err, output: <nil>: 1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
2: eth0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP group default qlen 1000
    link/ether 08:00:27:c1:dd:90 brd ff:ff:ff:ff:ff:ff
    inet 10.0.2.15/24 brd 10.0.2.255 scope global dynamic eth0
       valid_lft 86395sec preferred_lft 86395sec
3: eth1: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP group default qlen 1000
    link/ether 08:00:27:59:4e:e2 brd ff:ff:ff:ff:ff:ff
    inet 192.168.59.100/24 brd 192.168.59.255 scope global dynamic eth1
       valid_lft 595sec preferred_lft 595sec
4: sit0@NONE: <NOARP> mtu 1480 qdisc noop state DOWN group default qlen 1000
    link/sit 0.0.0.0 brd 0.0.0.0

I0207 11:31:25.828564    3511 main.go:141] libmachine: SSH returned: 1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
2: eth0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP group default qlen 1000
    link/ether 08:00:27:c1:dd:90 brd ff:ff:ff:ff:ff:ff
    inet 10.0.2.15/24 brd 10.0.2.255 scope global dynamic eth0
       valid_lft 86395sec preferred_lft 86395sec
3: eth1: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP group default qlen 1000
    link/ether 08:00:27:59:4e:e2 brd ff:ff:ff:ff:ff:ff
    inet 192.168.59.100/24 brd 192.168.59.255 scope global dynamic eth1
       valid_lft 595sec preferred_lft 595sec
4: sit0@NONE: <NOARP> mtu 1480 qdisc noop state DOWN group default qlen 1000
    link/sit 0.0.0.0 brd 0.0.0.0

END SSH

I0207 11:31:25.828860    3511 profile.go:148] Saving config to /Users/yaswitha/.minikube/profiles/minikube/config.json ...
I0207 11:31:25.870505    3511 machine.go:88] provisioning docker machine ...
I0207 11:31:25.870567    3511 buildroot.go:166] provisioning hostname "minikube"
I0207 11:31:25.870650    3511 main.go:141] libmachine: Using SSH client type: native
I0207 11:31:25.871622    3511 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x1405ca0] 0x1408980 <nil>  [] 0s} 127.0.0.1 59633 <nil> <nil>}
I0207 11:31:25.871635    3511 main.go:141] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I0207 11:31:25.990499    3511 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0207 11:31:25.990586    3511 main.go:141] libmachine: Using SSH client type: native
I0207 11:31:25.991254    3511 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x1405ca0] 0x1408980 <nil>  [] 0s} 127.0.0.1 59633 <nil> <nil>}
I0207 11:31:25.991276    3511 main.go:141] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I0207 11:31:26.078929    3511 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0207 11:31:26.078970    3511 buildroot.go:172] set auth options {CertDir:/Users/yaswitha/.minikube CaCertPath:/Users/yaswitha/.minikube/certs/ca.pem CaPrivateKeyPath:/Users/yaswitha/.minikube/certs/ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:/Users/yaswitha/.minikube/machines/server.pem ServerKeyPath:/Users/yaswitha/.minikube/machines/server-key.pem ClientKeyPath:/Users/yaswitha/.minikube/certs/key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:/Users/yaswitha/.minikube/certs/cert.pem ServerCertSANs:[] StorePath:/Users/yaswitha/.minikube}
I0207 11:31:26.079019    3511 buildroot.go:174] setting up certificates
I0207 11:31:26.079091    3511 provision.go:83] configureAuth start
I0207 11:31:26.079125    3511 main.go:141] libmachine: COMMAND: /usr/local/bin/VBoxManage showvminfo minikube --machinereadable
I0207 11:31:26.306458    3511 main.go:141] libmachine: STDOUT:
{
name="minikube"
groups="/"
ostype="Linux 2.6 / 3.x / 4.x (64-bit)"
UUID="7785180f-7acb-4aa1-be56-47fec5875cea"
CfgFile="/Users/yaswitha/.minikube/machines/minikube/minikube/minikube.vbox"
SnapFldr="/Users/yaswitha/.minikube/machines/minikube/minikube/Snapshots"
LogFldr="/Users/yaswitha/.minikube/machines/minikube/minikube/Logs"
hardwareuuid="7785180f-7acb-4aa1-be56-47fec5875cea"
memory=4096
pagefusion="off"
vram=8
cpuexecutioncap=100
hpet="on"
cpu-profile="host"
chipset="piix3"
firmware="BIOS"
cpus=2
pae="on"
longmode="on"
triplefaultreset="off"
apic="on"
x2apic="off"
nested-hw-virt="off"
cpuid-portability-level=0
bootmenu="disabled"
boot1="dvd"
boot2="dvd"
boot3="disk"
boot4="none"
acpi="on"
ioapic="on"
biosapic="apic"
biossystemtimeoffset=0
rtcuseutc="on"
hwvirtex="on"
nestedpaging="on"
largepages="on"
vtxvpid="on"
vtxux="on"
paravirtprovider="default"
effparavirtprovider="kvm"
VMState="running"
VMStateChangeTime="2024-02-07T03:30:50.709000000"
graphicscontroller="vboxvga"
monitorcount=1
accelerate3d="off"
accelerate2dvideo="off"
teleporterenabled="off"
teleporterport=0
teleporteraddress=""
teleporterpassword=""
tracing-enabled="off"
tracing-allow-vm-access="off"
tracing-config=""
autostart-enabled="off"
autostart-delay=0
defaultfrontend=""
vmprocpriority="default"
storagecontrollername0="SATA"
storagecontrollertype0="IntelAhci"
storagecontrollerinstance0="0"
storagecontrollermaxportcount0="30"
storagecontrollerportcount0="30"
storagecontrollerbootable0="on"
"SATA-0-0"="/Users/yaswitha/.minikube/machines/minikube/boot2docker.iso"
"SATA-ImageUUID-0-0"="e2e6d007-9e65-4ada-ac79-de64d81e96bf"
"SATA-tempeject"="off"
"SATA-IsEjected"="off"
"SATA-1-0"="/Users/yaswitha/.minikube/machines/minikube/disk.vmdk"
"SATA-ImageUUID-1-0"="b71bfcd5-e591-40ba-89ef-042783c5fb47"
"SATA-2-0"="none"
"SATA-3-0"="none"
"SATA-4-0"="none"
"SATA-5-0"="none"
"SATA-6-0"="none"
"SATA-7-0"="none"
"SATA-8-0"="none"
"SATA-9-0"="none"
"SATA-10-0"="none"
"SATA-11-0"="none"
"SATA-12-0"="none"
"SATA-13-0"="none"
"SATA-14-0"="none"
"SATA-15-0"="none"
"SATA-16-0"="none"
"SATA-17-0"="none"
"SATA-18-0"="none"
"SATA-19-0"="none"
"SATA-20-0"="none"
"SATA-21-0"="none"
"SATA-22-0"="none"
"SATA-23-0"="none"
"SATA-24-0"="none"
"SATA-25-0"="none"
"SATA-26-0"="none"
"SATA-27-0"="none"
"SATA-28-0"="none"
"SATA-29-0"="none"
natnet1="nat"
macaddress1="080027C1DD90"
cableconnected1="on"
nic1="nat"
nictype1="virtio"
nicspeed1="0"
mtu="0"
sockSnd="64"
sockRcv="64"
tcpWndSnd="64"
tcpWndRcv="64"
Forwarding(0)="ssh,tcp,127.0.0.1,59633,,22"
hostonlyadapter2="vboxnet0"
macaddress2="080027594EE2"
cableconnected2="on"
nic2="hostonly"
nictype2="virtio"
nicspeed2="0"
nic3="none"
nic4="none"
nic5="none"
nic6="none"
nic7="none"
nic8="none"
hidpointing="ps2mouse"
hidkeyboard="ps2kbd"
uart1="off"
uart2="off"
uart3="off"
uart4="off"
lpt1="off"
lpt2="off"
audio="coreaudio"
audio_out="off"
audio_in="off"
clipboard="disabled"
draganddrop="disabled"
SessionName="headless"
VideoMode="720,400,0"@0,0 1
vrde="off"
usb="off"
ehci="off"
xhci="off"
SharedFolderNameMachineMapping1="Users"
SharedFolderPathMachineMapping1="/Users"
VRDEActiveConnection="off"
VRDEClients==0
videocap="off"
videocapaudio="off"
capturescreens="0"
capturefilename="/Users/yaswitha/.minikube/machines/minikube/minikube/minikube.webm"
captureres="1024x768"
capturevideorate=512
capturevideofps=25
captureopts=""
GuestMemoryBalloon=0
GuestOSType="Linux26_64"
GuestAdditionsRunLevel=2
GuestAdditionsVersion="6.0.0 r127566"
GuestAdditionsFacility_VirtualBox Base Driver=50,1707276680237
GuestAdditionsFacility_VirtualBox System Service=50,1707276680982
GuestAdditionsFacility_Seamless Mode=0,1707276680089
GuestAdditionsFacility_Graphics Mode=0,1707276680088
}
I0207 11:31:26.306558    3511 main.go:141] libmachine: STDERR:
{
}
I0207 11:31:26.306805    3511 main.go:141] libmachine: COMMAND: /usr/local/bin/VBoxManage showvminfo minikube --machinereadable
I0207 11:31:26.519462    3511 main.go:141] libmachine: STDOUT:
{
name="minikube"
groups="/"
ostype="Linux 2.6 / 3.x / 4.x (64-bit)"
UUID="7785180f-7acb-4aa1-be56-47fec5875cea"
CfgFile="/Users/yaswitha/.minikube/machines/minikube/minikube/minikube.vbox"
SnapFldr="/Users/yaswitha/.minikube/machines/minikube/minikube/Snapshots"
LogFldr="/Users/yaswitha/.minikube/machines/minikube/minikube/Logs"
hardwareuuid="7785180f-7acb-4aa1-be56-47fec5875cea"
memory=4096
pagefusion="off"
vram=8
cpuexecutioncap=100
hpet="on"
cpu-profile="host"
chipset="piix3"
firmware="BIOS"
cpus=2
pae="on"
longmode="on"
triplefaultreset="off"
apic="on"
x2apic="off"
nested-hw-virt="off"
cpuid-portability-level=0
bootmenu="disabled"
boot1="dvd"
boot2="dvd"
boot3="disk"
boot4="none"
acpi="on"
ioapic="on"
biosapic="apic"
biossystemtimeoffset=0
rtcuseutc="on"
hwvirtex="on"
nestedpaging="on"
largepages="on"
vtxvpid="on"
vtxux="on"
paravirtprovider="default"
effparavirtprovider="kvm"
VMState="running"
VMStateChangeTime="2024-02-07T03:30:50.709000000"
graphicscontroller="vboxvga"
monitorcount=1
accelerate3d="off"
accelerate2dvideo="off"
teleporterenabled="off"
teleporterport=0
teleporteraddress=""
teleporterpassword=""
tracing-enabled="off"
tracing-allow-vm-access="off"
tracing-config=""
autostart-enabled="off"
autostart-delay=0
defaultfrontend=""
vmprocpriority="default"
storagecontrollername0="SATA"
storagecontrollertype0="IntelAhci"
storagecontrollerinstance0="0"
storagecontrollermaxportcount0="30"
storagecontrollerportcount0="30"
storagecontrollerbootable0="on"
"SATA-0-0"="/Users/yaswitha/.minikube/machines/minikube/boot2docker.iso"
"SATA-ImageUUID-0-0"="e2e6d007-9e65-4ada-ac79-de64d81e96bf"
"SATA-tempeject"="off"
"SATA-IsEjected"="off"
"SATA-1-0"="/Users/yaswitha/.minikube/machines/minikube/disk.vmdk"
"SATA-ImageUUID-1-0"="b71bfcd5-e591-40ba-89ef-042783c5fb47"
"SATA-2-0"="none"
"SATA-3-0"="none"
"SATA-4-0"="none"
"SATA-5-0"="none"
"SATA-6-0"="none"
"SATA-7-0"="none"
"SATA-8-0"="none"
"SATA-9-0"="none"
"SATA-10-0"="none"
"SATA-11-0"="none"
"SATA-12-0"="none"
"SATA-13-0"="none"
"SATA-14-0"="none"
"SATA-15-0"="none"
"SATA-16-0"="none"
"SATA-17-0"="none"
"SATA-18-0"="none"
"SATA-19-0"="none"
"SATA-20-0"="none"
"SATA-21-0"="none"
"SATA-22-0"="none"
"SATA-23-0"="none"
"SATA-24-0"="none"
"SATA-25-0"="none"
"SATA-26-0"="none"
"SATA-27-0"="none"
"SATA-28-0"="none"
"SATA-29-0"="none"
natnet1="nat"
macaddress1="080027C1DD90"
cableconnected1="on"
nic1="nat"
nictype1="virtio"
nicspeed1="0"
mtu="0"
sockSnd="64"
sockRcv="64"
tcpWndSnd="64"
tcpWndRcv="64"
Forwarding(0)="ssh,tcp,127.0.0.1,59633,,22"
hostonlyadapter2="vboxnet0"
macaddress2="080027594EE2"
cableconnected2="on"
nic2="hostonly"
nictype2="virtio"
nicspeed2="0"
nic3="none"
nic4="none"
nic5="none"
nic6="none"
nic7="none"
nic8="none"
hidpointing="ps2mouse"
hidkeyboard="ps2kbd"
uart1="off"
uart2="off"
uart3="off"
uart4="off"
lpt1="off"
lpt2="off"
audio="coreaudio"
audio_out="off"
audio_in="off"
clipboard="disabled"
draganddrop="disabled"
SessionName="headless"
VideoMode="720,400,0"@0,0 1
vrde="off"
usb="off"
ehci="off"
xhci="off"
SharedFolderNameMachineMapping1="Users"
SharedFolderPathMachineMapping1="/Users"
VRDEActiveConnection="off"
VRDEClients==0
videocap="off"
videocapaudio="off"
capturescreens="0"
capturefilename="/Users/yaswitha/.minikube/machines/minikube/minikube/minikube.webm"
captureres="1024x768"
capturevideorate=512
capturevideofps=25
captureopts=""
GuestMemoryBalloon=0
GuestOSType="Linux26_64"
GuestAdditionsRunLevel=2
GuestAdditionsVersion="6.0.0 r127566"
GuestAdditionsFacility_VirtualBox Base Driver=50,1707276680237
GuestAdditionsFacility_VirtualBox System Service=50,1707276680982
GuestAdditionsFacility_Seamless Mode=0,1707276680089
GuestAdditionsFacility_Graphics Mode=0,1707276680088
}
I0207 11:31:26.519543    3511 main.go:141] libmachine: STDERR:
{
}
I0207 11:31:26.519951    3511 main.go:141] libmachine: Host-only MAC: 080027594ee2

I0207 11:31:26.520049    3511 main.go:141] libmachine: Using SSH client type: native
I0207 11:31:26.520923    3511 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x1405ca0] 0x1408980 <nil>  [] 0s} 127.0.0.1 59633 <nil> <nil>}
I0207 11:31:26.520936    3511 main.go:141] libmachine: About to run SSH command:
ip addr show
I0207 11:31:26.615165    3511 main.go:141] libmachine: SSH cmd err, output: <nil>: 1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
2: eth0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP group default qlen 1000
    link/ether 08:00:27:c1:dd:90 brd ff:ff:ff:ff:ff:ff
    inet 10.0.2.15/24 brd 10.0.2.255 scope global dynamic eth0
       valid_lft 86394sec preferred_lft 86394sec
3: eth1: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP group default qlen 1000
    link/ether 08:00:27:59:4e:e2 brd ff:ff:ff:ff:ff:ff
    inet 192.168.59.100/24 brd 192.168.59.255 scope global dynamic eth1
       valid_lft 594sec preferred_lft 594sec
4: sit0@NONE: <NOARP> mtu 1480 qdisc noop state DOWN group default qlen 1000
    link/sit 0.0.0.0 brd 0.0.0.0

I0207 11:31:26.615244    3511 main.go:141] libmachine: SSH returned: 1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
2: eth0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP group default qlen 1000
    link/ether 08:00:27:c1:dd:90 brd ff:ff:ff:ff:ff:ff
    inet 10.0.2.15/24 brd 10.0.2.255 scope global dynamic eth0
       valid_lft 86394sec preferred_lft 86394sec
3: eth1: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP group default qlen 1000
    link/ether 08:00:27:59:4e:e2 brd ff:ff:ff:ff:ff:ff
    inet 192.168.59.100/24 brd 192.168.59.255 scope global dynamic eth1
       valid_lft 594sec preferred_lft 594sec
4: sit0@NONE: <NOARP> mtu 1480 qdisc noop state DOWN group default qlen 1000
    link/sit 0.0.0.0 brd 0.0.0.0

END SSH

I0207 11:31:26.615264    3511 provision.go:138] copyHostCerts
I0207 11:31:26.622143    3511 exec_runner.go:144] found /Users/yaswitha/.minikube/ca.pem, removing ...
I0207 11:31:26.622163    3511 exec_runner.go:203] rm: /Users/yaswitha/.minikube/ca.pem
I0207 11:31:26.622859    3511 exec_runner.go:151] cp: /Users/yaswitha/.minikube/certs/ca.pem --> /Users/yaswitha/.minikube/ca.pem (1082 bytes)
I0207 11:31:26.659535    3511 exec_runner.go:144] found /Users/yaswitha/.minikube/cert.pem, removing ...
I0207 11:31:26.659546    3511 exec_runner.go:203] rm: /Users/yaswitha/.minikube/cert.pem
I0207 11:31:26.659805    3511 exec_runner.go:151] cp: /Users/yaswitha/.minikube/certs/cert.pem --> /Users/yaswitha/.minikube/cert.pem (1127 bytes)
I0207 11:31:26.705956    3511 exec_runner.go:144] found /Users/yaswitha/.minikube/key.pem, removing ...
I0207 11:31:26.705978    3511 exec_runner.go:203] rm: /Users/yaswitha/.minikube/key.pem
I0207 11:31:26.706377    3511 exec_runner.go:151] cp: /Users/yaswitha/.minikube/certs/key.pem --> /Users/yaswitha/.minikube/key.pem (1675 bytes)
I0207 11:31:26.706848    3511 provision.go:112] generating server cert: /Users/yaswitha/.minikube/machines/server.pem ca-key=/Users/yaswitha/.minikube/certs/ca.pem private-key=/Users/yaswitha/.minikube/certs/ca-key.pem org=yaswitha.minikube san=[192.168.59.100 127.0.0.1 localhost 127.0.0.1 minikube minikube]
I0207 11:31:27.061810    3511 provision.go:172] copyRemoteCerts
I0207 11:31:27.064451    3511 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I0207 11:31:27.064493    3511 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:59633 SSHKeyPath:/Users/yaswitha/.minikube/machines/minikube/id_rsa Username:docker}
I0207 11:31:27.123956    3511 ssh_runner.go:362] scp /Users/yaswitha/.minikube/certs/ca.pem --> /etc/docker/ca.pem (1082 bytes)
I0207 11:31:27.171441    3511 ssh_runner.go:362] scp /Users/yaswitha/.minikube/machines/server.pem --> /etc/docker/server.pem (1204 bytes)
I0207 11:31:27.210133    3511 ssh_runner.go:362] scp /Users/yaswitha/.minikube/machines/server-key.pem --> /etc/docker/server-key.pem (1675 bytes)
I0207 11:31:27.255842    3511 provision.go:86] duration metric: configureAuth took 1.176697533s
I0207 11:31:27.255889    3511 buildroot.go:189] setting minikube options for container-runtime
I0207 11:31:27.277224    3511 config.go:182] Loaded profile config "minikube": Driver=virtualbox, ContainerRuntime=docker, KubernetesVersion=v1.28.3
I0207 11:31:27.277353    3511 main.go:141] libmachine: Using SSH client type: native
I0207 11:31:27.277962    3511 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x1405ca0] 0x1408980 <nil>  [] 0s} 127.0.0.1 59633 <nil> <nil>}
I0207 11:31:27.277973    3511 main.go:141] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I0207 11:31:27.374347    3511 main.go:141] libmachine: SSH cmd err, output: <nil>: tmpfs

I0207 11:31:27.374361    3511 buildroot.go:70] root file system type: tmpfs
I0207 11:31:27.374571    3511 provision.go:309] Updating docker unit: /lib/systemd/system/docker.service ...
I0207 11:31:27.374657    3511 main.go:141] libmachine: Using SSH client type: native
I0207 11:31:27.375515    3511 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x1405ca0] 0x1408980 <nil>  [] 0s} 127.0.0.1 59633 <nil> <nil>}
I0207 11:31:27.375657    3511 main.go:141] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %!s(MISSING) "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
After=network.target  minikube-automount.service docker.socket
Requires= minikube-automount.service docker.socket 
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=virtualbox --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I0207 11:31:27.493472    3511 main.go:141] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
After=network.target  minikube-automount.service docker.socket
Requires= minikube-automount.service docker.socket 
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=virtualbox --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target

I0207 11:31:27.493607    3511 main.go:141] libmachine: Using SSH client type: native
I0207 11:31:27.494761    3511 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x1405ca0] 0x1408980 <nil>  [] 0s} 127.0.0.1 59633 <nil> <nil>}
I0207 11:31:27.494796    3511 main.go:141] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I0207 11:31:30.363631    3511 main.go:141] libmachine: SSH cmd err, output: <nil>: diff: can't stat '/lib/systemd/system/docker.service': No such file or directory
Created symlink /etc/systemd/system/multi-user.target.wants/docker.service ‚Üí /usr/lib/systemd/system/docker.service.

I0207 11:31:30.363696    3511 machine.go:91] provisioned docker machine in 4.493102126s
I0207 11:31:30.363723    3511 start.go:300] post-start starting for "minikube" (driver="virtualbox")
I0207 11:31:30.363751    3511 start.go:329] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I0207 11:31:30.363884    3511 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I0207 11:31:30.363918    3511 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:59633 SSHKeyPath:/Users/yaswitha/.minikube/machines/minikube/id_rsa Username:docker}
I0207 11:31:30.455611    3511 ssh_runner.go:195] Run: cat /etc/os-release
I0207 11:31:30.474215    3511 info.go:137] Remote host: Buildroot 2021.02.12
I0207 11:31:30.474252    3511 filesync.go:126] Scanning /Users/yaswitha/.minikube/addons for local assets ...
I0207 11:31:30.475000    3511 filesync.go:126] Scanning /Users/yaswitha/.minikube/files for local assets ...
I0207 11:31:30.476623    3511 start.go:303] post-start completed in 112.877915ms
I0207 11:31:30.476679    3511 fix.go:56] fixHost completed within 44.913336109s
I0207 11:31:30.476825    3511 main.go:141] libmachine: Using SSH client type: native
I0207 11:31:30.478458    3511 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x1405ca0] 0x1408980 <nil>  [] 0s} 127.0.0.1 59633 <nil> <nil>}
I0207 11:31:30.478484    3511 main.go:141] libmachine: About to run SSH command:
date +%!s(MISSING).%!N(MISSING)
I0207 11:31:30.617811    3511 main.go:141] libmachine: SSH cmd err, output: <nil>: 1707276690.615432317

I0207 11:31:30.617828    3511 fix.go:206] guest clock: 1707276690.615432317
I0207 11:31:30.617839    3511 fix.go:219] Guest: 2024-02-07 11:31:30.615432317 +0800 +08 Remote: 2024-02-07 11:31:30.476715 +0800 +08 m=+47.194898080 (delta=138.717317ms)
I0207 11:31:30.618019    3511 fix.go:190] guest clock delta is within tolerance: 138.717317ms
I0207 11:31:30.618028    3511 start.go:83] releasing machines lock for "minikube", held for 45.054730212s
I0207 11:31:30.618161    3511 main.go:141] libmachine: COMMAND: /usr/local/bin/VBoxManage showvminfo minikube --machinereadable
I0207 11:31:30.901826    3511 main.go:141] libmachine: STDOUT:
{
name="minikube"
groups="/"
ostype="Linux 2.6 / 3.x / 4.x (64-bit)"
UUID="7785180f-7acb-4aa1-be56-47fec5875cea"
CfgFile="/Users/yaswitha/.minikube/machines/minikube/minikube/minikube.vbox"
SnapFldr="/Users/yaswitha/.minikube/machines/minikube/minikube/Snapshots"
LogFldr="/Users/yaswitha/.minikube/machines/minikube/minikube/Logs"
hardwareuuid="7785180f-7acb-4aa1-be56-47fec5875cea"
memory=4096
pagefusion="off"
vram=8
cpuexecutioncap=100
hpet="on"
cpu-profile="host"
chipset="piix3"
firmware="BIOS"
cpus=2
pae="on"
longmode="on"
triplefaultreset="off"
apic="on"
x2apic="off"
nested-hw-virt="off"
cpuid-portability-level=0
bootmenu="disabled"
boot1="dvd"
boot2="dvd"
boot3="disk"
boot4="none"
acpi="on"
ioapic="on"
biosapic="apic"
biossystemtimeoffset=0
rtcuseutc="on"
hwvirtex="on"
nestedpaging="on"
largepages="on"
vtxvpid="on"
vtxux="on"
paravirtprovider="default"
effparavirtprovider="kvm"
VMState="running"
VMStateChangeTime="2024-02-07T03:30:50.709000000"
graphicscontroller="vboxvga"
monitorcount=1
accelerate3d="off"
accelerate2dvideo="off"
teleporterenabled="off"
teleporterport=0
teleporteraddress=""
teleporterpassword=""
tracing-enabled="off"
tracing-allow-vm-access="off"
tracing-config=""
autostart-enabled="off"
autostart-delay=0
defaultfrontend=""
vmprocpriority="default"
storagecontrollername0="SATA"
storagecontrollertype0="IntelAhci"
storagecontrollerinstance0="0"
storagecontrollermaxportcount0="30"
storagecontrollerportcount0="30"
storagecontrollerbootable0="on"
"SATA-0-0"="/Users/yaswitha/.minikube/machines/minikube/boot2docker.iso"
"SATA-ImageUUID-0-0"="e2e6d007-9e65-4ada-ac79-de64d81e96bf"
"SATA-tempeject"="off"
"SATA-IsEjected"="off"
"SATA-1-0"="/Users/yaswitha/.minikube/machines/minikube/disk.vmdk"
"SATA-ImageUUID-1-0"="b71bfcd5-e591-40ba-89ef-042783c5fb47"
"SATA-2-0"="none"
"SATA-3-0"="none"
"SATA-4-0"="none"
"SATA-5-0"="none"
"SATA-6-0"="none"
"SATA-7-0"="none"
"SATA-8-0"="none"
"SATA-9-0"="none"
"SATA-10-0"="none"
"SATA-11-0"="none"
"SATA-12-0"="none"
"SATA-13-0"="none"
"SATA-14-0"="none"
"SATA-15-0"="none"
"SATA-16-0"="none"
"SATA-17-0"="none"
"SATA-18-0"="none"
"SATA-19-0"="none"
"SATA-20-0"="none"
"SATA-21-0"="none"
"SATA-22-0"="none"
"SATA-23-0"="none"
"SATA-24-0"="none"
"SATA-25-0"="none"
"SATA-26-0"="none"
"SATA-27-0"="none"
"SATA-28-0"="none"
"SATA-29-0"="none"
natnet1="nat"
macaddress1="080027C1DD90"
cableconnected1="on"
nic1="nat"
nictype1="virtio"
nicspeed1="0"
mtu="0"
sockSnd="64"
sockRcv="64"
tcpWndSnd="64"
tcpWndRcv="64"
Forwarding(0)="ssh,tcp,127.0.0.1,59633,,22"
hostonlyadapter2="vboxnet0"
macaddress2="080027594EE2"
cableconnected2="on"
nic2="hostonly"
nictype2="virtio"
nicspeed2="0"
nic3="none"
nic4="none"
nic5="none"
nic6="none"
nic7="none"
nic8="none"
hidpointing="ps2mouse"
hidkeyboard="ps2kbd"
uart1="off"
uart2="off"
uart3="off"
uart4="off"
lpt1="off"
lpt2="off"
audio="coreaudio"
audio_out="off"
audio_in="off"
clipboard="disabled"
draganddrop="disabled"
SessionName="headless"
VideoMode="720,400,0"@0,0 1
vrde="off"
usb="off"
ehci="off"
xhci="off"
SharedFolderNameMachineMapping1="Users"
SharedFolderPathMachineMapping1="/Users"
VRDEActiveConnection="off"
VRDEClients==0
videocap="off"
videocapaudio="off"
capturescreens="0"
capturefilename="/Users/yaswitha/.minikube/machines/minikube/minikube/minikube.webm"
captureres="1024x768"
capturevideorate=512
capturevideofps=25
captureopts=""
GuestMemoryBalloon=0
GuestOSType="Linux26_64"
GuestAdditionsRunLevel=2
GuestAdditionsVersion="6.0.0 r127566"
GuestAdditionsFacility_VirtualBox Base Driver=50,1707276680237
GuestAdditionsFacility_VirtualBox System Service=50,1707276680982
GuestAdditionsFacility_Seamless Mode=0,1707276680089
GuestAdditionsFacility_Graphics Mode=0,1707276680088
}
I0207 11:31:30.901929    3511 main.go:141] libmachine: STDERR:
{
}
I0207 11:31:30.902173    3511 main.go:141] libmachine: COMMAND: /usr/local/bin/VBoxManage showvminfo minikube --machinereadable
I0207 11:31:31.064884    3511 main.go:141] libmachine: STDOUT:
{
name="minikube"
groups="/"
ostype="Linux 2.6 / 3.x / 4.x (64-bit)"
UUID="7785180f-7acb-4aa1-be56-47fec5875cea"
CfgFile="/Users/yaswitha/.minikube/machines/minikube/minikube/minikube.vbox"
SnapFldr="/Users/yaswitha/.minikube/machines/minikube/minikube/Snapshots"
LogFldr="/Users/yaswitha/.minikube/machines/minikube/minikube/Logs"
hardwareuuid="7785180f-7acb-4aa1-be56-47fec5875cea"
memory=4096
pagefusion="off"
vram=8
cpuexecutioncap=100
hpet="on"
cpu-profile="host"
chipset="piix3"
firmware="BIOS"
cpus=2
pae="on"
longmode="on"
triplefaultreset="off"
apic="on"
x2apic="off"
nested-hw-virt="off"
cpuid-portability-level=0
bootmenu="disabled"
boot1="dvd"
boot2="dvd"
boot3="disk"
boot4="none"
acpi="on"
ioapic="on"
biosapic="apic"
biossystemtimeoffset=0
rtcuseutc="on"
hwvirtex="on"
nestedpaging="on"
largepages="on"
vtxvpid="on"
vtxux="on"
paravirtprovider="default"
effparavirtprovider="kvm"
VMState="running"
VMStateChangeTime="2024-02-07T03:30:50.709000000"
graphicscontroller="vboxvga"
monitorcount=1
accelerate3d="off"
accelerate2dvideo="off"
teleporterenabled="off"
teleporterport=0
teleporteraddress=""
teleporterpassword=""
tracing-enabled="off"
tracing-allow-vm-access="off"
tracing-config=""
autostart-enabled="off"
autostart-delay=0
defaultfrontend=""
vmprocpriority="default"
storagecontrollername0="SATA"
storagecontrollertype0="IntelAhci"
storagecontrollerinstance0="0"
storagecontrollermaxportcount0="30"
storagecontrollerportcount0="30"
storagecontrollerbootable0="on"
"SATA-0-0"="/Users/yaswitha/.minikube/machines/minikube/boot2docker.iso"
"SATA-ImageUUID-0-0"="e2e6d007-9e65-4ada-ac79-de64d81e96bf"
"SATA-tempeject"="off"
"SATA-IsEjected"="off"
"SATA-1-0"="/Users/yaswitha/.minikube/machines/minikube/disk.vmdk"
"SATA-ImageUUID-1-0"="b71bfcd5-e591-40ba-89ef-042783c5fb47"
"SATA-2-0"="none"
"SATA-3-0"="none"
"SATA-4-0"="none"
"SATA-5-0"="none"
"SATA-6-0"="none"
"SATA-7-0"="none"
"SATA-8-0"="none"
"SATA-9-0"="none"
"SATA-10-0"="none"
"SATA-11-0"="none"
"SATA-12-0"="none"
"SATA-13-0"="none"
"SATA-14-0"="none"
"SATA-15-0"="none"
"SATA-16-0"="none"
"SATA-17-0"="none"
"SATA-18-0"="none"
"SATA-19-0"="none"
"SATA-20-0"="none"
"SATA-21-0"="none"
"SATA-22-0"="none"
"SATA-23-0"="none"
"SATA-24-0"="none"
"SATA-25-0"="none"
"SATA-26-0"="none"
"SATA-27-0"="none"
"SATA-28-0"="none"
"SATA-29-0"="none"
natnet1="nat"
macaddress1="080027C1DD90"
cableconnected1="on"
nic1="nat"
nictype1="virtio"
nicspeed1="0"
mtu="0"
sockSnd="64"
sockRcv="64"
tcpWndSnd="64"
tcpWndRcv="64"
Forwarding(0)="ssh,tcp,127.0.0.1,59633,,22"
hostonlyadapter2="vboxnet0"
macaddress2="080027594EE2"
cableconnected2="on"
nic2="hostonly"
nictype2="virtio"
nicspeed2="0"
nic3="none"
nic4="none"
nic5="none"
nic6="none"
nic7="none"
nic8="none"
hidpointing="ps2mouse"
hidkeyboard="ps2kbd"
uart1="off"
uart2="off"
uart3="off"
uart4="off"
lpt1="off"
lpt2="off"
audio="coreaudio"
audio_out="off"
audio_in="off"
clipboard="disabled"
draganddrop="disabled"
SessionName="headless"
VideoMode="720,400,0"@0,0 1
vrde="off"
usb="off"
ehci="off"
xhci="off"
SharedFolderNameMachineMapping1="Users"
SharedFolderPathMachineMapping1="/Users"
VRDEActiveConnection="off"
VRDEClients==0
videocap="off"
videocapaudio="off"
capturescreens="0"
capturefilename="/Users/yaswitha/.minikube/machines/minikube/minikube/minikube.webm"
captureres="1024x768"
capturevideorate=512
capturevideofps=25
captureopts=""
GuestMemoryBalloon=0
GuestOSType="Linux26_64"
GuestAdditionsRunLevel=2
GuestAdditionsVersion="6.0.0 r127566"
GuestAdditionsFacility_VirtualBox Base Driver=50,1707276680237
GuestAdditionsFacility_VirtualBox System Service=50,1707276680982
GuestAdditionsFacility_Seamless Mode=0,1707276680089
GuestAdditionsFacility_Graphics Mode=0,1707276680088
}
I0207 11:31:31.064947    3511 main.go:141] libmachine: STDERR:
{
}
I0207 11:31:31.065233    3511 main.go:141] libmachine: Host-only MAC: 080027594ee2

I0207 11:31:31.065300    3511 main.go:141] libmachine: Using SSH client type: native
I0207 11:31:31.066054    3511 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x1405ca0] 0x1408980 <nil>  [] 0s} 127.0.0.1 59633 <nil> <nil>}
I0207 11:31:31.066071    3511 main.go:141] libmachine: About to run SSH command:
ip addr show
I0207 11:31:31.159995    3511 main.go:141] libmachine: SSH cmd err, output: <nil>: 1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
2: eth0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP group default qlen 1000
    link/ether 08:00:27:c1:dd:90 brd ff:ff:ff:ff:ff:ff
    inet 10.0.2.15/24 brd 10.0.2.255 scope global dynamic eth0
       valid_lft 86390sec preferred_lft 86390sec
3: eth1: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP group default qlen 1000
    link/ether 08:00:27:59:4e:e2 brd ff:ff:ff:ff:ff:ff
    inet 192.168.59.100/24 brd 192.168.59.255 scope global dynamic eth1
       valid_lft 590sec preferred_lft 590sec
4: sit0@NONE: <NOARP> mtu 1480 qdisc noop state DOWN group default qlen 1000
    link/sit 0.0.0.0 brd 0.0.0.0
5: docker0: <NO-CARRIER,BROADCAST,MULTICAST,UP> mtu 1500 qdisc noqueue state DOWN group default 
    link/ether 02:42:6f:2c:c9:a2 brd ff:ff:ff:ff:ff:ff
    inet 172.17.0.1/16 brd 172.17.255.255 scope global docker0
       valid_lft forever preferred_lft forever

I0207 11:31:31.160111    3511 main.go:141] libmachine: SSH returned: 1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
2: eth0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP group default qlen 1000
    link/ether 08:00:27:c1:dd:90 brd ff:ff:ff:ff:ff:ff
    inet 10.0.2.15/24 brd 10.0.2.255 scope global dynamic eth0
       valid_lft 86390sec preferred_lft 86390sec
3: eth1: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP group default qlen 1000
    link/ether 08:00:27:59:4e:e2 brd ff:ff:ff:ff:ff:ff
    inet 192.168.59.100/24 brd 192.168.59.255 scope global dynamic eth1
       valid_lft 590sec preferred_lft 590sec
4: sit0@NONE: <NOARP> mtu 1480 qdisc noop state DOWN group default qlen 1000
    link/sit 0.0.0.0 brd 0.0.0.0
5: docker0: <NO-CARRIER,BROADCAST,MULTICAST,UP> mtu 1500 qdisc noqueue state DOWN group default 
    link/ether 02:42:6f:2c:c9:a2 brd ff:ff:ff:ff:ff:ff
    inet 172.17.0.1/16 brd 172.17.255.255 scope global docker0
       valid_lft forever preferred_lft forever

END SSH

I0207 11:31:31.161440    3511 ssh_runner.go:195] Run: cat /version.json
I0207 11:31:31.161467    3511 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:59633 SSHKeyPath:/Users/yaswitha/.minikube/machines/minikube/id_rsa Username:docker}
I0207 11:31:31.163441    3511 ssh_runner.go:195] Run: curl -sS -m 2 https://registry.k8s.io/
I0207 11:31:31.163521    3511 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:59633 SSHKeyPath:/Users/yaswitha/.minikube/machines/minikube/id_rsa Username:docker}
I0207 11:31:31.232744    3511 ssh_runner.go:195] Run: systemctl --version
I0207 11:31:31.440843    3511 ssh_runner.go:195] Run: sh -c "stat /etc/cni/net.d/*loopback.conf*"
W0207 11:31:31.455795    3511 cni.go:209] loopback cni configuration skipped: "/etc/cni/net.d/*loopback.conf*" not found
I0207 11:31:31.455933    3511 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f ( ( -name *bridge* -or -name *podman* ) -and -not -name *.mk_disabled ) -printf "%!p(MISSING), " -exec sh -c "sudo mv {} {}.mk_disabled" ;
I0207 11:31:31.481646    3511 cni.go:262] disabled [/etc/cni/net.d/87-podman-bridge.conflist] bridge cni config(s)
I0207 11:31:31.481663    3511 start.go:472] detecting cgroup driver to use...
I0207 11:31:31.481920    3511 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///run/containerd/containerd.sock
" | sudo tee /etc/crictl.yaml"
I0207 11:31:31.516884    3511 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)sandbox_image = .*$|\1sandbox_image = "registry.k8s.io/pause:3.9"|' /etc/containerd/config.toml"
I0207 11:31:31.538653    3511 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)restrict_oom_score_adj = .*$|\1restrict_oom_score_adj = false|' /etc/containerd/config.toml"
I0207 11:31:31.557825    3511 containerd.go:145] configuring containerd to use "cgroupfs" as cgroup driver...
I0207 11:31:31.557946    3511 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)SystemdCgroup = .*$|\1SystemdCgroup = false|g' /etc/containerd/config.toml"
I0207 11:31:31.578314    3511 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runtime.v1.linux"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0207 11:31:31.597717    3511 ssh_runner.go:195] Run: sh -c "sudo sed -i '/systemd_cgroup/d' /etc/containerd/config.toml"
I0207 11:31:31.624064    3511 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runc.v1"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0207 11:31:31.640029    3511 ssh_runner.go:195] Run: sh -c "sudo rm -rf /etc/cni/net.mk"
I0207 11:31:31.664110    3511 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)conf_dir = .*$|\1conf_dir = "/etc/cni/net.d"|g' /etc/containerd/config.toml"
I0207 11:31:31.680451    3511 ssh_runner.go:195] Run: sudo sysctl net.bridge.bridge-nf-call-iptables
I0207 11:31:31.696697    3511 ssh_runner.go:195] Run: sudo sh -c "echo 1 > /proc/sys/net/ipv4/ip_forward"
I0207 11:31:31.712695    3511 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0207 11:31:31.918377    3511 ssh_runner.go:195] Run: sudo systemctl restart containerd
I0207 11:31:31.947048    3511 start.go:472] detecting cgroup driver to use...
I0207 11:31:31.947209    3511 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I0207 11:31:31.980216    3511 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service containerd
I0207 11:31:32.006373    3511 ssh_runner.go:195] Run: sudo systemctl stop -f containerd
I0207 11:31:32.032932    3511 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service containerd
I0207 11:31:32.052405    3511 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I0207 11:31:32.072864    3511 ssh_runner.go:195] Run: sudo systemctl stop -f crio
I0207 11:31:32.117766    3511 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I0207 11:31:32.143288    3511 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I0207 11:31:32.177048    3511 ssh_runner.go:195] Run: which cri-dockerd
I0207 11:31:32.183062    3511 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/cri-docker.service.d
I0207 11:31:32.208291    3511 ssh_runner.go:362] scp memory --> /etc/systemd/system/cri-docker.service.d/10-cni.conf (189 bytes)
I0207 11:31:32.236759    3511 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I0207 11:31:32.432346    3511 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I0207 11:31:32.623197    3511 docker.go:560] configuring docker to use "cgroupfs" as cgroup driver...
I0207 11:31:32.623390    3511 ssh_runner.go:362] scp memory --> /etc/docker/daemon.json (130 bytes)
I0207 11:31:32.649898    3511 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0207 11:31:32.825674    3511 ssh_runner.go:195] Run: sudo systemctl restart docker
I0207 11:31:34.959388    3511 ssh_runner.go:235] Completed: sudo systemctl restart docker: (2.133624674s)
I0207 11:31:34.959620    3511 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I0207 11:31:35.241317    3511 ssh_runner.go:195] Run: sudo systemctl unmask cri-docker.socket
I0207 11:31:35.453052    3511 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I0207 11:31:35.653094    3511 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0207 11:31:35.905322    3511 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.socket
I0207 11:31:35.975729    3511 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0207 11:31:36.199035    3511 ssh_runner.go:195] Run: sudo systemctl restart cri-docker
I0207 11:31:36.348346    3511 start.go:519] Will wait 60s for socket path /var/run/cri-dockerd.sock
I0207 11:31:36.349973    3511 ssh_runner.go:195] Run: stat /var/run/cri-dockerd.sock
I0207 11:31:36.362053    3511 start.go:540] Will wait 60s for crictl version
I0207 11:31:36.362179    3511 ssh_runner.go:195] Run: which crictl
I0207 11:31:36.369059    3511 ssh_runner.go:195] Run: sudo /usr/bin/crictl version
I0207 11:31:36.466325    3511 start.go:556] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  24.0.7
RuntimeApiVersion:  v1
I0207 11:31:36.466413    3511 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0207 11:31:36.516297    3511 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0207 11:31:36.584326    3511 out.go:204] üê≥  Preparing Kubernetes v1.28.3 on Docker 24.0.7 ...
I0207 11:31:36.950511    3511 ssh_runner.go:195] Run: grep 192.168.59.1	host.minikube.internal$ /etc/hosts
I0207 11:31:36.957924    3511 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\thost.minikube.internal$' "/etc/hosts"; echo "192.168.59.1	host.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0207 11:31:36.984100    3511 preload.go:132] Checking if preload exists for k8s version v1.28.3 and runtime docker
I0207 11:31:36.984243    3511 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0207 11:31:37.021083    3511 docker.go:671] Got preloaded images: -- stdout --
mongo-express:latest
mysql:8.0
mysql:latest
mongo:latest
nginx:latest
registry.k8s.io/ingress-nginx/controller:<none>
registry.k8s.io/kube-apiserver:v1.28.3
registry.k8s.io/kube-controller-manager:v1.28.3
registry.k8s.io/kube-scheduler:v1.28.3
registry.k8s.io/kube-proxy:v1.28.3
registry.k8s.io/ingress-nginx/kube-webhook-certgen:<none>
registry.k8s.io/etcd:3.5.9-0
registry.k8s.io/coredns/coredns:v1.10.1
registry.k8s.io/pause:3.9
kubernetesui/dashboard:<none>
kubernetesui/metrics-scraper:<none>
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0207 11:31:37.021119    3511 docker.go:601] Images already preloaded, skipping extraction
I0207 11:31:37.021241    3511 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0207 11:31:37.060065    3511 docker.go:671] Got preloaded images: -- stdout --
mongo-express:latest
mysql:8.0
mysql:latest
mongo:latest
nginx:latest
registry.k8s.io/ingress-nginx/controller:<none>
registry.k8s.io/kube-apiserver:v1.28.3
registry.k8s.io/kube-controller-manager:v1.28.3
registry.k8s.io/kube-scheduler:v1.28.3
registry.k8s.io/kube-proxy:v1.28.3
registry.k8s.io/ingress-nginx/kube-webhook-certgen:<none>
registry.k8s.io/etcd:3.5.9-0
registry.k8s.io/coredns/coredns:v1.10.1
registry.k8s.io/pause:3.9
kubernetesui/dashboard:<none>
kubernetesui/metrics-scraper:<none>
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0207 11:31:37.060106    3511 cache_images.go:84] Images are preloaded, skipping loading
I0207 11:31:37.060289    3511 ssh_runner.go:195] Run: docker info --format {{.CgroupDriver}}
I0207 11:31:37.103901    3511 cni.go:84] Creating CNI manager for ""
I0207 11:31:37.103937    3511 cni.go:158] "virtualbox" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0207 11:31:37.103978    3511 kubeadm.go:87] Using pod CIDR: 10.244.0.0/16
I0207 11:31:37.104005    3511 kubeadm.go:176] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.59.100 APIServerPort:8443 KubernetesVersion:v1.28.3 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube DNSDomain:cluster.local CRISocket:/var/run/cri-dockerd.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.59.100"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NodeIP:192.168.59.100 CgroupDriver:cgroupfs ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[] ResolvConfSearchRegression:false KubeletConfigOpts:map[hairpinMode:hairpin-veth runtimeRequestTimeout:15m] PrependCriSocketUnix:true}
I0207 11:31:37.104442    3511 kubeadm.go:181] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta3
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.59.100
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: unix:///var/run/cri-dockerd.sock
  name: "minikube"
  kubeletExtraArgs:
    node-ip: 192.168.59.100
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta3
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.59.100"]
  extraArgs:
    enable-admission-plugins: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    allocate-node-cidrs: "true"
    leader-elect: "false"
scheduler:
  extraArgs:
    leader-elect: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
    extraArgs:
      proxy-refresh-interval: "70000"
kubernetesVersion: v1.28.3
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: cgroupfs
hairpinMode: hairpin-veth
runtimeRequestTimeout: 15m
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%!"(MISSING)
  nodefs.inodesFree: "0%!"(MISSING)
  imagefs.available: "0%!"(MISSING)
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I0207 11:31:37.104654    3511 kubeadm.go:976] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.28.3/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --container-runtime-endpoint=unix:///var/run/cri-dockerd.sock --hostname-override=minikube --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.59.100

[Install]
 config:
{KubernetesVersion:v1.28.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:}
I0207 11:31:37.104732    3511 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.28.3
I0207 11:31:37.123157    3511 binaries.go:44] Found k8s binaries, skipping transfer
I0207 11:31:37.123300    3511 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system /var/tmp/minikube
I0207 11:31:37.142892    3511 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (371 bytes)
I0207 11:31:37.175681    3511 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I0207 11:31:37.203106    3511 ssh_runner.go:362] scp memory --> /var/tmp/minikube/kubeadm.yaml.new (2097 bytes)
I0207 11:31:37.237640    3511 ssh_runner.go:195] Run: grep 192.168.59.100	control-plane.minikube.internal$ /etc/hosts
I0207 11:31:37.243568    3511 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\tcontrol-plane.minikube.internal$' "/etc/hosts"; echo "192.168.59.100	control-plane.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0207 11:31:37.269438    3511 certs.go:56] Setting up /Users/yaswitha/.minikube/profiles/minikube for IP: 192.168.59.100
I0207 11:31:37.269495    3511 certs.go:190] acquiring lock for shared ca certs: {Name:mkda34a008c6837355ac465172cd0b51a00f61d9 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0207 11:31:37.315195    3511 certs.go:199] skipping minikubeCA CA generation: /Users/yaswitha/.minikube/ca.key
I0207 11:31:37.359471    3511 certs.go:199] skipping proxyClientCA CA generation: /Users/yaswitha/.minikube/proxy-client-ca.key
I0207 11:31:37.363202    3511 certs.go:315] skipping minikube-user signed cert generation: /Users/yaswitha/.minikube/profiles/minikube/client.key
I0207 11:31:37.367008    3511 certs.go:315] skipping minikube signed cert generation: /Users/yaswitha/.minikube/profiles/minikube/apiserver.key.ee221796
I0207 11:31:37.370701    3511 certs.go:315] skipping aggregator signed cert generation: /Users/yaswitha/.minikube/profiles/minikube/proxy-client.key
I0207 11:31:37.375651    3511 certs.go:437] found cert: /Users/yaswitha/.minikube/certs/Users/yaswitha/.minikube/certs/ca-key.pem (1679 bytes)
I0207 11:31:37.377279    3511 certs.go:437] found cert: /Users/yaswitha/.minikube/certs/Users/yaswitha/.minikube/certs/ca.pem (1082 bytes)
I0207 11:31:37.378930    3511 certs.go:437] found cert: /Users/yaswitha/.minikube/certs/Users/yaswitha/.minikube/certs/cert.pem (1127 bytes)
I0207 11:31:37.380603    3511 certs.go:437] found cert: /Users/yaswitha/.minikube/certs/Users/yaswitha/.minikube/certs/key.pem (1675 bytes)
I0207 11:31:37.383258    3511 ssh_runner.go:362] scp /Users/yaswitha/.minikube/profiles/minikube/apiserver.crt --> /var/lib/minikube/certs/apiserver.crt (1399 bytes)
I0207 11:31:37.424205    3511 ssh_runner.go:362] scp /Users/yaswitha/.minikube/profiles/minikube/apiserver.key --> /var/lib/minikube/certs/apiserver.key (1679 bytes)
I0207 11:31:37.466990    3511 ssh_runner.go:362] scp /Users/yaswitha/.minikube/profiles/minikube/proxy-client.crt --> /var/lib/minikube/certs/proxy-client.crt (1147 bytes)
I0207 11:31:37.506799    3511 ssh_runner.go:362] scp /Users/yaswitha/.minikube/profiles/minikube/proxy-client.key --> /var/lib/minikube/certs/proxy-client.key (1675 bytes)
I0207 11:31:37.556068    3511 ssh_runner.go:362] scp /Users/yaswitha/.minikube/ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I0207 11:31:37.619282    3511 ssh_runner.go:362] scp /Users/yaswitha/.minikube/ca.key --> /var/lib/minikube/certs/ca.key (1679 bytes)
I0207 11:31:37.668626    3511 ssh_runner.go:362] scp /Users/yaswitha/.minikube/proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I0207 11:31:37.709845    3511 ssh_runner.go:362] scp /Users/yaswitha/.minikube/proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1675 bytes)
I0207 11:31:37.759964    3511 ssh_runner.go:362] scp /Users/yaswitha/.minikube/ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I0207 11:31:37.826247    3511 ssh_runner.go:362] scp memory --> /var/lib/minikube/kubeconfig (738 bytes)
I0207 11:31:37.882098    3511 ssh_runner.go:195] Run: openssl version
I0207 11:31:37.937816    3511 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I0207 11:31:37.974756    3511 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I0207 11:31:37.988799    3511 certs.go:480] hashing: -rw-r--r-- 1 root root 1111 Feb  4 06:55 /usr/share/ca-certificates/minikubeCA.pem
I0207 11:31:37.989026    3511 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I0207 11:31:38.003649    3511 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I0207 11:31:38.024237    3511 ssh_runner.go:195] Run: ls /var/lib/minikube/certs/etcd
I0207 11:31:38.038955    3511 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-etcd-client.crt -checkend 86400
I0207 11:31:38.052485    3511 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-kubelet-client.crt -checkend 86400
I0207 11:31:38.068997    3511 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/server.crt -checkend 86400
I0207 11:31:38.083501    3511 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/healthcheck-client.crt -checkend 86400
I0207 11:31:38.095019    3511 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/peer.crt -checkend 86400
I0207 11:31:38.108695    3511 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/front-proxy-client.crt -checkend 86400
I0207 11:31:38.118949    3511 kubeadm.go:404] StartCluster: {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO:https://storage.googleapis.com/minikube/iso/minikube-v1.32.1-amd64.iso KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 Memory:4096 CPUs:2 DiskSize:20000 VMDriver: Driver:virtualbox HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.28.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.59.100 Port:8443 KubernetesVersion:v1.28.3 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[dashboard:true default-storageclass:true ingress:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/Users:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 AutoPauseInterval:1m0s GPUs:}
I0207 11:31:38.119256    3511 ssh_runner.go:195] Run: docker ps --filter status=paused --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I0207 11:31:38.156894    3511 ssh_runner.go:195] Run: sudo ls /var/lib/kubelet/kubeadm-flags.env /var/lib/kubelet/config.yaml /var/lib/minikube/etcd
I0207 11:31:38.176298    3511 kubeadm.go:419] found existing configuration files, will attempt cluster restart
I0207 11:31:38.176339    3511 kubeadm.go:636] restartCluster start
I0207 11:31:38.176440    3511 ssh_runner.go:195] Run: sudo test -d /data/minikube
I0207 11:31:38.191268    3511 kubeadm.go:127] /data/minikube skipping compat symlinks: sudo test -d /data/minikube: Process exited with status 1
stdout:

stderr:
I0207 11:31:38.194494    3511 kubeconfig.go:135] verify returned: extract IP: "minikube" does not appear in /Users/yaswitha/.kube/config
I0207 11:31:38.195445    3511 kubeconfig.go:146] "minikube" context is missing from /Users/yaswitha/.kube/config - will repair!
I0207 11:31:38.196602    3511 lock.go:35] WriteFile acquiring /Users/yaswitha/.kube/config: {Name:mk189ae64b0382c9a980228bfb8a7099358d916d Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0207 11:31:38.202806    3511 ssh_runner.go:195] Run: sudo diff -u /var/tmp/minikube/kubeadm.yaml /var/tmp/minikube/kubeadm.yaml.new
I0207 11:31:38.218934    3511 api_server.go:166] Checking apiserver status ...
I0207 11:31:38.219037    3511 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0207 11:31:38.239707    3511 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0207 11:31:38.239744    3511 api_server.go:166] Checking apiserver status ...
I0207 11:31:38.239829    3511 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0207 11:31:38.265753    3511 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0207 11:31:38.766122    3511 api_server.go:166] Checking apiserver status ...
I0207 11:31:38.766227    3511 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0207 11:31:38.809177    3511 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0207 11:31:39.265926    3511 api_server.go:166] Checking apiserver status ...
I0207 11:31:39.266047    3511 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0207 11:31:39.287305    3511 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0207 11:31:39.766615    3511 api_server.go:166] Checking apiserver status ...
I0207 11:31:39.767454    3511 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0207 11:31:39.786988    3511 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0207 11:31:40.266007    3511 api_server.go:166] Checking apiserver status ...
I0207 11:31:40.266169    3511 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0207 11:31:40.289655    3511 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0207 11:31:40.766571    3511 api_server.go:166] Checking apiserver status ...
I0207 11:31:40.766659    3511 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0207 11:31:40.788036    3511 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0207 11:31:41.266279    3511 api_server.go:166] Checking apiserver status ...
I0207 11:31:41.266376    3511 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0207 11:31:41.288210    3511 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0207 11:31:41.766971    3511 api_server.go:166] Checking apiserver status ...
I0207 11:31:41.767115    3511 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0207 11:31:41.807507    3511 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0207 11:31:42.266846    3511 api_server.go:166] Checking apiserver status ...
I0207 11:31:42.266953    3511 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0207 11:31:42.286565    3511 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0207 11:31:42.766882    3511 api_server.go:166] Checking apiserver status ...
I0207 11:31:42.766985    3511 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0207 11:31:42.788968    3511 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0207 11:31:43.266242    3511 api_server.go:166] Checking apiserver status ...
I0207 11:31:43.266484    3511 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0207 11:31:43.288846    3511 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0207 11:31:43.766784    3511 api_server.go:166] Checking apiserver status ...
I0207 11:31:43.766887    3511 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0207 11:31:43.788519    3511 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0207 11:31:44.266620    3511 api_server.go:166] Checking apiserver status ...
I0207 11:31:44.266785    3511 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0207 11:31:44.290726    3511 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0207 11:31:44.766234    3511 api_server.go:166] Checking apiserver status ...
I0207 11:31:44.766335    3511 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0207 11:31:44.793989    3511 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0207 11:31:45.266067    3511 api_server.go:166] Checking apiserver status ...
I0207 11:31:45.266240    3511 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0207 11:31:45.288067    3511 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0207 11:31:45.766624    3511 api_server.go:166] Checking apiserver status ...
I0207 11:31:45.766719    3511 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0207 11:31:45.795941    3511 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0207 11:31:46.266095    3511 api_server.go:166] Checking apiserver status ...
I0207 11:31:46.266206    3511 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0207 11:31:46.290575    3511 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0207 11:31:46.766180    3511 api_server.go:166] Checking apiserver status ...
I0207 11:31:46.766408    3511 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0207 11:31:46.788966    3511 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0207 11:31:47.266309    3511 api_server.go:166] Checking apiserver status ...
I0207 11:31:47.266552    3511 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0207 11:31:47.287742    3511 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0207 11:31:47.766362    3511 api_server.go:166] Checking apiserver status ...
I0207 11:31:47.766538    3511 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0207 11:31:47.787166    3511 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0207 11:31:48.219721    3511 kubeadm.go:611] needs reconfigure: apiserver error: context deadline exceeded
I0207 11:31:48.219742    3511 kubeadm.go:1128] stopping kube-system containers ...
I0207 11:31:48.219846    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I0207 11:31:48.259692    3511 docker.go:469] Stopping containers: [ec52b9ed7e21 0b85673627c7 06fb15ac4a32 06f9e3cf4f3c 1db52edcdad4 b84b69f9db1a 0f2709b87b9b e1bb8361945e 1d1e9abd9836 1b36eac9fea0 1eed8f4e884e 3a4b28d8f433 dac32809c63f b94028402ad3 407d81ee6871 dacc030a6bab 2a8210ae6c39 ee7c2215045a 5cf1eff8b362 885389110d12 39d1988a2fab 2fd6ede1fe50 4cb5ab785be7 518b5968b7e1 dc86498da168]
I0207 11:31:48.259850    3511 ssh_runner.go:195] Run: docker stop ec52b9ed7e21 0b85673627c7 06fb15ac4a32 06f9e3cf4f3c 1db52edcdad4 b84b69f9db1a 0f2709b87b9b e1bb8361945e 1d1e9abd9836 1b36eac9fea0 1eed8f4e884e 3a4b28d8f433 dac32809c63f b94028402ad3 407d81ee6871 dacc030a6bab 2a8210ae6c39 ee7c2215045a 5cf1eff8b362 885389110d12 39d1988a2fab 2fd6ede1fe50 4cb5ab785be7 518b5968b7e1 dc86498da168
I0207 11:31:48.314658    3511 ssh_runner.go:195] Run: sudo systemctl stop kubelet
I0207 11:31:48.344690    3511 ssh_runner.go:195] Run: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf
I0207 11:31:48.382165    3511 kubeadm.go:152] config check failed, skipping stale config cleanup: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf: Process exited with status 2
stdout:

stderr:
ls: cannot access '/etc/kubernetes/admin.conf': No such file or directory
ls: cannot access '/etc/kubernetes/kubelet.conf': No such file or directory
ls: cannot access '/etc/kubernetes/controller-manager.conf': No such file or directory
ls: cannot access '/etc/kubernetes/scheduler.conf': No such file or directory
I0207 11:31:48.382276    3511 ssh_runner.go:195] Run: sudo cp /var/tmp/minikube/kubeadm.yaml.new /var/tmp/minikube/kubeadm.yaml
I0207 11:31:48.406314    3511 kubeadm.go:713] reconfiguring cluster from /var/tmp/minikube/kubeadm.yaml
I0207 11:31:48.406334    3511 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.28.3:$PATH" kubeadm init phase certs all --config /var/tmp/minikube/kubeadm.yaml"
I0207 11:31:49.240012    3511 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.28.3:$PATH" kubeadm init phase kubeconfig all --config /var/tmp/minikube/kubeadm.yaml"
I0207 11:31:51.807101    3511 ssh_runner.go:235] Completed: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.28.3:$PATH" kubeadm init phase kubeconfig all --config /var/tmp/minikube/kubeadm.yaml": (2.567004064s)
I0207 11:31:51.807182    3511 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.28.3:$PATH" kubeadm init phase kubelet-start --config /var/tmp/minikube/kubeadm.yaml"
I0207 11:31:52.160323    3511 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.28.3:$PATH" kubeadm init phase control-plane all --config /var/tmp/minikube/kubeadm.yaml"
I0207 11:31:52.340709    3511 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.28.3:$PATH" kubeadm init phase etcd local --config /var/tmp/minikube/kubeadm.yaml"
I0207 11:31:52.487994    3511 api_server.go:52] waiting for apiserver process to appear ...
I0207 11:31:52.488144    3511 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0207 11:31:52.528444    3511 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0207 11:31:53.065001    3511 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0207 11:31:53.564732    3511 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0207 11:31:54.064757    3511 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0207 11:31:54.564813    3511 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0207 11:31:55.065620    3511 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0207 11:31:55.565194    3511 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0207 11:31:56.064865    3511 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0207 11:31:56.564916    3511 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0207 11:31:57.064814    3511 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0207 11:31:57.566401    3511 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0207 11:31:58.064841    3511 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0207 11:31:58.565323    3511 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0207 11:31:59.064924    3511 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0207 11:31:59.564730    3511 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0207 11:32:00.065471    3511 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0207 11:32:00.565741    3511 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0207 11:32:01.065402    3511 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0207 11:32:01.564795    3511 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0207 11:32:02.064923    3511 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0207 11:32:02.565070    3511 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0207 11:32:03.065649    3511 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0207 11:32:03.565881    3511 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0207 11:32:04.065288    3511 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0207 11:32:04.565131    3511 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0207 11:32:05.065324    3511 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0207 11:32:05.565542    3511 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0207 11:32:06.066059    3511 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0207 11:32:06.564949    3511 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0207 11:32:07.065696    3511 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0207 11:32:07.564834    3511 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0207 11:32:08.065600    3511 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0207 11:32:08.565458    3511 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0207 11:32:09.064872    3511 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0207 11:32:09.564840    3511 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0207 11:32:10.066074    3511 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0207 11:32:10.565819    3511 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0207 11:32:11.065492    3511 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0207 11:32:11.565023    3511 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0207 11:32:12.065228    3511 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0207 11:32:12.565660    3511 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0207 11:32:13.065068    3511 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0207 11:32:13.565278    3511 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0207 11:32:14.065156    3511 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0207 11:32:14.565103    3511 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0207 11:32:15.065110    3511 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0207 11:32:15.565148    3511 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0207 11:32:16.065873    3511 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0207 11:32:16.565909    3511 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0207 11:32:17.065548    3511 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0207 11:32:17.565692    3511 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0207 11:32:18.065405    3511 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0207 11:32:18.565841    3511 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0207 11:32:19.065719    3511 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0207 11:32:19.566012    3511 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0207 11:32:20.065884    3511 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0207 11:32:20.565122    3511 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0207 11:32:21.065354    3511 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0207 11:32:21.566352    3511 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0207 11:32:22.065207    3511 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0207 11:32:22.566339    3511 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0207 11:32:23.065912    3511 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0207 11:32:23.566049    3511 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0207 11:32:24.065693    3511 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0207 11:32:24.565111    3511 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0207 11:32:25.065501    3511 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0207 11:32:25.565756    3511 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0207 11:32:26.065646    3511 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0207 11:32:26.565845    3511 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0207 11:32:27.065312    3511 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0207 11:32:27.565987    3511 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0207 11:32:28.065403    3511 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0207 11:32:28.565866    3511 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0207 11:32:29.066054    3511 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0207 11:32:29.567331    3511 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0207 11:32:30.065512    3511 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0207 11:32:30.565498    3511 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0207 11:32:31.065399    3511 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0207 11:32:31.565293    3511 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0207 11:32:32.065298    3511 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0207 11:32:32.565280    3511 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0207 11:32:33.065256    3511 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0207 11:32:33.565835    3511 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0207 11:32:34.065781    3511 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0207 11:32:34.565994    3511 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0207 11:32:35.065663    3511 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0207 11:32:35.565413    3511 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0207 11:32:36.066404    3511 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0207 11:32:36.565916    3511 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0207 11:32:37.065682    3511 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0207 11:32:37.566118    3511 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0207 11:32:38.068020    3511 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0207 11:32:38.566310    3511 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0207 11:32:39.066354    3511 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0207 11:32:39.565669    3511 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0207 11:32:40.065416    3511 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0207 11:32:40.566361    3511 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0207 11:32:41.065757    3511 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0207 11:32:41.566387    3511 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0207 11:32:42.066186    3511 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0207 11:32:42.565559    3511 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0207 11:32:43.066138    3511 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0207 11:32:43.566082    3511 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0207 11:32:44.065667    3511 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0207 11:32:44.566492    3511 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0207 11:32:45.065516    3511 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0207 11:32:45.565519    3511 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0207 11:32:46.065658    3511 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0207 11:32:46.565878    3511 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0207 11:32:47.065970    3511 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0207 11:32:47.566248    3511 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0207 11:32:48.066467    3511 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0207 11:32:48.565844    3511 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0207 11:32:49.066389    3511 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0207 11:32:49.566143    3511 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0207 11:32:50.065949    3511 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0207 11:32:50.565667    3511 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0207 11:32:51.065725    3511 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0207 11:32:51.565709    3511 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0207 11:32:52.065751    3511 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0207 11:32:52.566275    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-apiserver --format={{.ID}}
I0207 11:32:52.596880    3511 logs.go:284] 2 containers: [1b36eac9fea0 39d1988a2fab]
I0207 11:32:52.597010    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_etcd --format={{.ID}}
I0207 11:32:52.626838    3511 logs.go:284] 2 containers: [1eed8f4e884e 5cf1eff8b362]
I0207 11:32:52.626958    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_coredns --format={{.ID}}
I0207 11:32:52.656606    3511 logs.go:284] 2 containers: [06fb15ac4a32 b94028402ad3]
I0207 11:32:52.656738    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-scheduler --format={{.ID}}
I0207 11:32:52.688594    3511 logs.go:284] 2 containers: [1d1e9abd9836 ee7c2215045a]
I0207 11:32:52.688670    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-proxy --format={{.ID}}
I0207 11:32:52.723876    3511 logs.go:284] 2 containers: [1db52edcdad4 407d81ee6871]
I0207 11:32:52.724044    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-controller-manager --format={{.ID}}
I0207 11:32:52.767045    3511 logs.go:284] 2 containers: [e1bb8361945e 885389110d12]
I0207 11:32:52.767168    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kindnet --format={{.ID}}
I0207 11:32:52.797241    3511 logs.go:284] 0 containers: []
W0207 11:32:52.797297    3511 logs.go:286] No container was found matching "kindnet"
I0207 11:32:52.797469    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_storage-provisioner --format={{.ID}}
I0207 11:32:52.841099    3511 logs.go:284] 2 containers: [ec52b9ed7e21 0b85673627c7]
I0207 11:32:52.841233    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kubernetes-dashboard --format={{.ID}}
I0207 11:32:52.887845    3511 logs.go:284] 2 containers: [5631cbc21ffd f4b96dcbc9f2]
I0207 11:32:52.887963    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_controller_ingress --format={{.ID}}
I0207 11:32:52.927840    3511 logs.go:284] 1 containers: [8d50139a5311]
I0207 11:32:52.927880    3511 logs.go:123] Gathering logs for dmesg ...
I0207 11:32:52.927972    3511 ssh_runner.go:195] Run: /bin/bash -c "sudo dmesg -PH -L=never --level warn,err,crit,alert,emerg | tail -n 400"
I0207 11:32:52.998053    3511 logs.go:123] Gathering logs for etcd [5cf1eff8b362] ...
I0207 11:32:52.998088    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 5cf1eff8b362"
I0207 11:32:53.097023    3511 logs.go:123] Gathering logs for controller_ingress [8d50139a5311] ...
I0207 11:32:53.097043    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 8d50139a5311"
I0207 11:32:53.219807    3511 logs.go:123] Gathering logs for kube-scheduler [1d1e9abd9836] ...
I0207 11:32:53.219905    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 1d1e9abd9836"
I0207 11:32:53.336814    3511 logs.go:123] Gathering logs for Docker ...
I0207 11:32:53.336830    3511 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u docker -u cri-docker -n 400"
I0207 11:32:53.401175    3511 logs.go:123] Gathering logs for describe nodes ...
I0207 11:32:53.401190    3511 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig"
W0207 11:32:54.159407    3511 logs.go:130] failed describe nodes: command: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig" /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig": Process exited with status 1
stdout:

stderr:
E0207 03:32:54.147121    3164 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:32:54.149834    3164 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:32:54.151114    3164 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:32:54.151797    3164 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:32:54.154057    3164 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
The connection to the server localhost:8443 was refused - did you specify the right host or port?
 output: 
** stderr ** 
E0207 03:32:54.147121    3164 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:32:54.149834    3164 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:32:54.151114    3164 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:32:54.151797    3164 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:32:54.154057    3164 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
The connection to the server localhost:8443 was refused - did you specify the right host or port?

** /stderr **
I0207 11:32:54.159447    3511 logs.go:123] Gathering logs for kube-apiserver [39d1988a2fab] ...
I0207 11:32:54.159493    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 39d1988a2fab"
I0207 11:32:54.225857    3511 logs.go:123] Gathering logs for coredns [b94028402ad3] ...
I0207 11:32:54.225902    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 b94028402ad3"
I0207 11:32:54.285019    3511 logs.go:123] Gathering logs for storage-provisioner [0b85673627c7] ...
I0207 11:32:54.285034    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 0b85673627c7"
I0207 11:32:54.386973    3511 logs.go:123] Gathering logs for kubernetes-dashboard [5631cbc21ffd] ...
I0207 11:32:54.386989    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 5631cbc21ffd"
I0207 11:32:54.433319    3511 logs.go:123] Gathering logs for kubernetes-dashboard [f4b96dcbc9f2] ...
I0207 11:32:54.433336    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 f4b96dcbc9f2"
I0207 11:32:54.523508    3511 logs.go:123] Gathering logs for container status ...
I0207 11:32:54.523523    3511 ssh_runner.go:195] Run: /bin/bash -c "sudo `which crictl || echo crictl` ps -a || sudo docker ps -a"
I0207 11:32:54.694638    3511 logs.go:123] Gathering logs for etcd [1eed8f4e884e] ...
I0207 11:32:54.694659    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 1eed8f4e884e"
I0207 11:32:54.917461    3511 logs.go:123] Gathering logs for kube-controller-manager [e1bb8361945e] ...
I0207 11:32:54.917478    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 e1bb8361945e"
I0207 11:32:55.099771    3511 logs.go:123] Gathering logs for kube-controller-manager [885389110d12] ...
I0207 11:32:55.099794    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 885389110d12"
I0207 11:32:55.243370    3511 logs.go:123] Gathering logs for kube-scheduler [ee7c2215045a] ...
I0207 11:32:55.243399    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 ee7c2215045a"
I0207 11:32:55.326468    3511 logs.go:123] Gathering logs for kube-proxy [1db52edcdad4] ...
I0207 11:32:55.326484    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 1db52edcdad4"
I0207 11:32:55.377487    3511 logs.go:123] Gathering logs for kube-proxy [407d81ee6871] ...
I0207 11:32:55.377526    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 407d81ee6871"
I0207 11:32:55.418963    3511 logs.go:123] Gathering logs for storage-provisioner [ec52b9ed7e21] ...
I0207 11:32:55.418978    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 ec52b9ed7e21"
I0207 11:32:55.459284    3511 logs.go:123] Gathering logs for kubelet ...
I0207 11:32:55.459310    3511 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u kubelet -n 400"
I0207 11:32:55.542812    3511 logs.go:123] Gathering logs for kube-apiserver [1b36eac9fea0] ...
I0207 11:32:55.542832    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 1b36eac9fea0"
I0207 11:32:55.641596    3511 logs.go:123] Gathering logs for coredns [06fb15ac4a32] ...
I0207 11:32:55.641621    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 06fb15ac4a32"
I0207 11:32:58.228705    3511 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0207 11:32:58.252213    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-apiserver --format={{.ID}}
I0207 11:32:58.287629    3511 logs.go:284] 2 containers: [1b36eac9fea0 39d1988a2fab]
I0207 11:32:58.287773    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_etcd --format={{.ID}}
I0207 11:32:58.320640    3511 logs.go:284] 2 containers: [1eed8f4e884e 5cf1eff8b362]
I0207 11:32:58.320735    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_coredns --format={{.ID}}
I0207 11:32:58.348729    3511 logs.go:284] 2 containers: [06fb15ac4a32 b94028402ad3]
I0207 11:32:58.348857    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-scheduler --format={{.ID}}
I0207 11:32:58.378603    3511 logs.go:284] 2 containers: [1d1e9abd9836 ee7c2215045a]
I0207 11:32:58.378726    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-proxy --format={{.ID}}
I0207 11:32:58.410983    3511 logs.go:284] 2 containers: [1db52edcdad4 407d81ee6871]
I0207 11:32:58.411113    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-controller-manager --format={{.ID}}
I0207 11:32:58.441863    3511 logs.go:284] 2 containers: [e1bb8361945e 885389110d12]
I0207 11:32:58.441956    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kindnet --format={{.ID}}
I0207 11:32:58.471055    3511 logs.go:284] 0 containers: []
W0207 11:32:58.471072    3511 logs.go:286] No container was found matching "kindnet"
I0207 11:32:58.471179    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kubernetes-dashboard --format={{.ID}}
I0207 11:32:58.507051    3511 logs.go:284] 2 containers: [5631cbc21ffd f4b96dcbc9f2]
I0207 11:32:58.507155    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_controller_ingress --format={{.ID}}
I0207 11:32:58.537123    3511 logs.go:284] 1 containers: [8d50139a5311]
I0207 11:32:58.537252    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_storage-provisioner --format={{.ID}}
I0207 11:32:58.565804    3511 logs.go:284] 2 containers: [ec52b9ed7e21 0b85673627c7]
I0207 11:32:58.565846    3511 logs.go:123] Gathering logs for kubernetes-dashboard [5631cbc21ffd] ...
I0207 11:32:58.565858    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 5631cbc21ffd"
I0207 11:32:58.604873    3511 logs.go:123] Gathering logs for kubernetes-dashboard [f4b96dcbc9f2] ...
I0207 11:32:58.604889    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 f4b96dcbc9f2"
I0207 11:32:58.668648    3511 logs.go:123] Gathering logs for kubelet ...
I0207 11:32:58.668663    3511 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u kubelet -n 400"
I0207 11:32:58.778257    3511 logs.go:123] Gathering logs for describe nodes ...
I0207 11:32:58.778275    3511 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig"
W0207 11:32:58.969606    3511 logs.go:130] failed describe nodes: command: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig" /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig": Process exited with status 1
stdout:

stderr:
E0207 03:32:58.956609    3459 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:32:58.957563    3459 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:32:58.959458    3459 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:32:58.961264    3459 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:32:58.962185    3459 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
The connection to the server localhost:8443 was refused - did you specify the right host or port?
 output: 
** stderr ** 
E0207 03:32:58.956609    3459 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:32:58.957563    3459 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:32:58.959458    3459 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:32:58.961264    3459 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:32:58.962185    3459 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
The connection to the server localhost:8443 was refused - did you specify the right host or port?

** /stderr **
I0207 11:32:58.969629    3511 logs.go:123] Gathering logs for kube-apiserver [39d1988a2fab] ...
I0207 11:32:58.969646    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 39d1988a2fab"
I0207 11:32:59.023739    3511 logs.go:123] Gathering logs for kube-scheduler [ee7c2215045a] ...
I0207 11:32:59.023754    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 ee7c2215045a"
I0207 11:32:59.070236    3511 logs.go:123] Gathering logs for kube-proxy [407d81ee6871] ...
I0207 11:32:59.070251    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 407d81ee6871"
I0207 11:32:59.106529    3511 logs.go:123] Gathering logs for dmesg ...
I0207 11:32:59.106544    3511 ssh_runner.go:195] Run: /bin/bash -c "sudo dmesg -PH -L=never --level warn,err,crit,alert,emerg | tail -n 400"
I0207 11:32:59.157452    3511 logs.go:123] Gathering logs for kube-apiserver [1b36eac9fea0] ...
I0207 11:32:59.157469    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 1b36eac9fea0"
I0207 11:32:59.219796    3511 logs.go:123] Gathering logs for etcd [5cf1eff8b362] ...
I0207 11:32:59.219811    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 5cf1eff8b362"
I0207 11:32:59.291750    3511 logs.go:123] Gathering logs for kube-controller-manager [885389110d12] ...
I0207 11:32:59.291765    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 885389110d12"
I0207 11:32:59.366857    3511 logs.go:123] Gathering logs for storage-provisioner [0b85673627c7] ...
I0207 11:32:59.366873    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 0b85673627c7"
I0207 11:32:59.430449    3511 logs.go:123] Gathering logs for controller_ingress [8d50139a5311] ...
I0207 11:32:59.430468    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 8d50139a5311"
I0207 11:32:59.503821    3511 logs.go:123] Gathering logs for storage-provisioner [ec52b9ed7e21] ...
I0207 11:32:59.503836    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 ec52b9ed7e21"
I0207 11:32:59.539513    3511 logs.go:123] Gathering logs for Docker ...
I0207 11:32:59.539527    3511 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u docker -u cri-docker -n 400"
I0207 11:32:59.628834    3511 logs.go:123] Gathering logs for etcd [1eed8f4e884e] ...
I0207 11:32:59.628862    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 1eed8f4e884e"
I0207 11:32:59.816685    3511 logs.go:123] Gathering logs for coredns [06fb15ac4a32] ...
I0207 11:32:59.816708    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 06fb15ac4a32"
I0207 11:32:59.875649    3511 logs.go:123] Gathering logs for coredns [b94028402ad3] ...
I0207 11:32:59.875665    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 b94028402ad3"
I0207 11:32:59.942708    3511 logs.go:123] Gathering logs for kube-scheduler [1d1e9abd9836] ...
I0207 11:32:59.942746    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 1d1e9abd9836"
I0207 11:33:00.007996    3511 logs.go:123] Gathering logs for kube-proxy [1db52edcdad4] ...
I0207 11:33:00.008027    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 1db52edcdad4"
I0207 11:33:00.053041    3511 logs.go:123] Gathering logs for container status ...
I0207 11:33:00.053056    3511 ssh_runner.go:195] Run: /bin/bash -c "sudo `which crictl || echo crictl` ps -a || sudo docker ps -a"
I0207 11:33:00.210903    3511 logs.go:123] Gathering logs for kube-controller-manager [e1bb8361945e] ...
I0207 11:33:00.210923    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 e1bb8361945e"
I0207 11:33:02.826312    3511 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0207 11:33:02.856363    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-apiserver --format={{.ID}}
I0207 11:33:02.899774    3511 logs.go:284] 2 containers: [1b36eac9fea0 39d1988a2fab]
I0207 11:33:02.899904    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_etcd --format={{.ID}}
I0207 11:33:02.936497    3511 logs.go:284] 2 containers: [1eed8f4e884e 5cf1eff8b362]
I0207 11:33:02.936629    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_coredns --format={{.ID}}
I0207 11:33:02.974150    3511 logs.go:284] 2 containers: [06fb15ac4a32 b94028402ad3]
I0207 11:33:02.974305    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-scheduler --format={{.ID}}
I0207 11:33:03.010232    3511 logs.go:284] 2 containers: [1d1e9abd9836 ee7c2215045a]
I0207 11:33:03.010317    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-proxy --format={{.ID}}
I0207 11:33:03.040697    3511 logs.go:284] 2 containers: [1db52edcdad4 407d81ee6871]
I0207 11:33:03.040873    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-controller-manager --format={{.ID}}
I0207 11:33:03.067943    3511 logs.go:284] 2 containers: [e1bb8361945e 885389110d12]
I0207 11:33:03.068096    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kindnet --format={{.ID}}
I0207 11:33:03.100425    3511 logs.go:284] 0 containers: []
W0207 11:33:03.100457    3511 logs.go:286] No container was found matching "kindnet"
I0207 11:33:03.100538    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kubernetes-dashboard --format={{.ID}}
I0207 11:33:03.128926    3511 logs.go:284] 2 containers: [5631cbc21ffd f4b96dcbc9f2]
I0207 11:33:03.129018    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_controller_ingress --format={{.ID}}
I0207 11:33:03.169166    3511 logs.go:284] 1 containers: [8d50139a5311]
I0207 11:33:03.169271    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_storage-provisioner --format={{.ID}}
I0207 11:33:03.206501    3511 logs.go:284] 2 containers: [ec52b9ed7e21 0b85673627c7]
I0207 11:33:03.206535    3511 logs.go:123] Gathering logs for kube-proxy [407d81ee6871] ...
I0207 11:33:03.206549    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 407d81ee6871"
I0207 11:33:03.243639    3511 logs.go:123] Gathering logs for controller_ingress [8d50139a5311] ...
I0207 11:33:03.243681    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 8d50139a5311"
I0207 11:33:03.325945    3511 logs.go:123] Gathering logs for kube-apiserver [39d1988a2fab] ...
I0207 11:33:03.325968    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 39d1988a2fab"
I0207 11:33:03.392719    3511 logs.go:123] Gathering logs for coredns [06fb15ac4a32] ...
I0207 11:33:03.392734    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 06fb15ac4a32"
I0207 11:33:03.486825    3511 logs.go:123] Gathering logs for coredns [b94028402ad3] ...
I0207 11:33:03.486842    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 b94028402ad3"
I0207 11:33:03.540093    3511 logs.go:123] Gathering logs for kube-scheduler [1d1e9abd9836] ...
I0207 11:33:03.540110    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 1d1e9abd9836"
I0207 11:33:03.632375    3511 logs.go:123] Gathering logs for kube-scheduler [ee7c2215045a] ...
I0207 11:33:03.632405    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 ee7c2215045a"
I0207 11:33:03.692724    3511 logs.go:123] Gathering logs for describe nodes ...
I0207 11:33:03.692743    3511 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig"
W0207 11:33:03.804408    3511 logs.go:130] failed describe nodes: command: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig" /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig": Process exited with status 1
stdout:

stderr:
E0207 03:33:03.792066    3798 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:33:03.793437    3798 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:33:03.794268    3798 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:33:03.796618    3798 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:33:03.797378    3798 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
The connection to the server localhost:8443 was refused - did you specify the right host or port?
 output: 
** stderr ** 
E0207 03:33:03.792066    3798 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:33:03.793437    3798 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:33:03.794268    3798 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:33:03.796618    3798 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:33:03.797378    3798 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
The connection to the server localhost:8443 was refused - did you specify the right host or port?

** /stderr **
I0207 11:33:03.804443    3511 logs.go:123] Gathering logs for etcd [5cf1eff8b362] ...
I0207 11:33:03.804470    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 5cf1eff8b362"
I0207 11:33:03.868353    3511 logs.go:123] Gathering logs for kube-proxy [1db52edcdad4] ...
I0207 11:33:03.868371    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 1db52edcdad4"
I0207 11:33:03.926165    3511 logs.go:123] Gathering logs for kube-controller-manager [885389110d12] ...
I0207 11:33:03.926181    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 885389110d12"
I0207 11:33:03.998391    3511 logs.go:123] Gathering logs for kubernetes-dashboard [5631cbc21ffd] ...
I0207 11:33:03.998406    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 5631cbc21ffd"
I0207 11:33:04.038278    3511 logs.go:123] Gathering logs for kube-apiserver [1b36eac9fea0] ...
I0207 11:33:04.038298    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 1b36eac9fea0"
I0207 11:33:04.098359    3511 logs.go:123] Gathering logs for kube-controller-manager [e1bb8361945e] ...
I0207 11:33:04.098386    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 e1bb8361945e"
I0207 11:33:04.177032    3511 logs.go:123] Gathering logs for kubernetes-dashboard [f4b96dcbc9f2] ...
I0207 11:33:04.177060    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 f4b96dcbc9f2"
I0207 11:33:04.266287    3511 logs.go:123] Gathering logs for Docker ...
I0207 11:33:04.266303    3511 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u docker -u cri-docker -n 400"
I0207 11:33:04.354348    3511 logs.go:123] Gathering logs for container status ...
I0207 11:33:04.354371    3511 ssh_runner.go:195] Run: /bin/bash -c "sudo `which crictl || echo crictl` ps -a || sudo docker ps -a"
I0207 11:33:04.550885    3511 logs.go:123] Gathering logs for kubelet ...
I0207 11:33:04.550915    3511 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u kubelet -n 400"
I0207 11:33:04.637246    3511 logs.go:123] Gathering logs for dmesg ...
I0207 11:33:04.637260    3511 ssh_runner.go:195] Run: /bin/bash -c "sudo dmesg -PH -L=never --level warn,err,crit,alert,emerg | tail -n 400"
I0207 11:33:04.681557    3511 logs.go:123] Gathering logs for etcd [1eed8f4e884e] ...
I0207 11:33:04.681582    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 1eed8f4e884e"
I0207 11:33:04.967708    3511 logs.go:123] Gathering logs for storage-provisioner [ec52b9ed7e21] ...
I0207 11:33:04.967736    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 ec52b9ed7e21"
I0207 11:33:05.014054    3511 logs.go:123] Gathering logs for storage-provisioner [0b85673627c7] ...
I0207 11:33:05.014078    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 0b85673627c7"
I0207 11:33:07.583122    3511 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0207 11:33:07.603674    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-apiserver --format={{.ID}}
I0207 11:33:07.638624    3511 logs.go:284] 2 containers: [1b36eac9fea0 39d1988a2fab]
I0207 11:33:07.638733    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_etcd --format={{.ID}}
I0207 11:33:07.671255    3511 logs.go:284] 2 containers: [1eed8f4e884e 5cf1eff8b362]
I0207 11:33:07.671355    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_coredns --format={{.ID}}
I0207 11:33:07.699188    3511 logs.go:284] 2 containers: [06fb15ac4a32 b94028402ad3]
I0207 11:33:07.699334    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-scheduler --format={{.ID}}
I0207 11:33:07.732584    3511 logs.go:284] 2 containers: [1d1e9abd9836 ee7c2215045a]
I0207 11:33:07.732695    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-proxy --format={{.ID}}
I0207 11:33:07.768015    3511 logs.go:284] 2 containers: [1db52edcdad4 407d81ee6871]
I0207 11:33:07.768130    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-controller-manager --format={{.ID}}
I0207 11:33:07.798193    3511 logs.go:284] 2 containers: [e1bb8361945e 885389110d12]
I0207 11:33:07.798283    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kindnet --format={{.ID}}
I0207 11:33:07.824354    3511 logs.go:284] 0 containers: []
W0207 11:33:07.824373    3511 logs.go:286] No container was found matching "kindnet"
I0207 11:33:07.824471    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_controller_ingress --format={{.ID}}
I0207 11:33:07.857551    3511 logs.go:284] 1 containers: [8d50139a5311]
I0207 11:33:07.857740    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_storage-provisioner --format={{.ID}}
I0207 11:33:07.910404    3511 logs.go:284] 2 containers: [ec52b9ed7e21 0b85673627c7]
I0207 11:33:07.910522    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kubernetes-dashboard --format={{.ID}}
I0207 11:33:07.960414    3511 logs.go:284] 2 containers: [5631cbc21ffd f4b96dcbc9f2]
I0207 11:33:07.960443    3511 logs.go:123] Gathering logs for dmesg ...
I0207 11:33:07.960457    3511 ssh_runner.go:195] Run: /bin/bash -c "sudo dmesg -PH -L=never --level warn,err,crit,alert,emerg | tail -n 400"
I0207 11:33:08.141129    3511 logs.go:123] Gathering logs for etcd [5cf1eff8b362] ...
I0207 11:33:08.141146    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 5cf1eff8b362"
I0207 11:33:08.225743    3511 logs.go:123] Gathering logs for kube-scheduler [1d1e9abd9836] ...
I0207 11:33:08.225766    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 1d1e9abd9836"
I0207 11:33:08.317566    3511 logs.go:123] Gathering logs for kube-controller-manager [885389110d12] ...
I0207 11:33:08.317582    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 885389110d12"
I0207 11:33:08.382982    3511 logs.go:123] Gathering logs for kubernetes-dashboard [5631cbc21ffd] ...
I0207 11:33:08.383015    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 5631cbc21ffd"
I0207 11:33:08.422843    3511 logs.go:123] Gathering logs for kubelet ...
I0207 11:33:08.422864    3511 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u kubelet -n 400"
I0207 11:33:08.516776    3511 logs.go:123] Gathering logs for kube-proxy [407d81ee6871] ...
I0207 11:33:08.516791    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 407d81ee6871"
I0207 11:33:08.559703    3511 logs.go:123] Gathering logs for kube-controller-manager [e1bb8361945e] ...
I0207 11:33:08.559719    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 e1bb8361945e"
I0207 11:33:08.658402    3511 logs.go:123] Gathering logs for Docker ...
I0207 11:33:08.658417    3511 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u docker -u cri-docker -n 400"
I0207 11:33:08.727621    3511 logs.go:123] Gathering logs for storage-provisioner [ec52b9ed7e21] ...
I0207 11:33:08.727636    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 ec52b9ed7e21"
I0207 11:33:08.764948    3511 logs.go:123] Gathering logs for storage-provisioner [0b85673627c7] ...
I0207 11:33:08.764965    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 0b85673627c7"
I0207 11:33:08.823323    3511 logs.go:123] Gathering logs for describe nodes ...
I0207 11:33:08.823340    3511 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig"
W0207 11:33:08.936634    3511 logs.go:130] failed describe nodes: command: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig" /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig": Process exited with status 1
stdout:

stderr:
E0207 03:33:08.924617    4112 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:33:08.925968    4112 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:33:08.927062    4112 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:33:08.928426    4112 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:33:08.929507    4112 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
The connection to the server localhost:8443 was refused - did you specify the right host or port?
 output: 
** stderr ** 
E0207 03:33:08.924617    4112 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:33:08.925968    4112 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:33:08.927062    4112 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:33:08.928426    4112 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:33:08.929507    4112 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
The connection to the server localhost:8443 was refused - did you specify the right host or port?

** /stderr **
I0207 11:33:08.936652    3511 logs.go:123] Gathering logs for kube-apiserver [39d1988a2fab] ...
I0207 11:33:08.936667    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 39d1988a2fab"
I0207 11:33:08.987394    3511 logs.go:123] Gathering logs for etcd [1eed8f4e884e] ...
I0207 11:33:08.987419    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 1eed8f4e884e"
I0207 11:33:09.157245    3511 logs.go:123] Gathering logs for coredns [06fb15ac4a32] ...
I0207 11:33:09.157269    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 06fb15ac4a32"
I0207 11:33:09.229145    3511 logs.go:123] Gathering logs for coredns [b94028402ad3] ...
I0207 11:33:09.229180    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 b94028402ad3"
I0207 11:33:09.289833    3511 logs.go:123] Gathering logs for kube-scheduler [ee7c2215045a] ...
I0207 11:33:09.289855    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 ee7c2215045a"
I0207 11:33:09.378171    3511 logs.go:123] Gathering logs for kubernetes-dashboard [f4b96dcbc9f2] ...
I0207 11:33:09.378198    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 f4b96dcbc9f2"
I0207 11:33:09.475670    3511 logs.go:123] Gathering logs for container status ...
I0207 11:33:09.475686    3511 ssh_runner.go:195] Run: /bin/bash -c "sudo `which crictl || echo crictl` ps -a || sudo docker ps -a"
I0207 11:33:09.651491    3511 logs.go:123] Gathering logs for kube-apiserver [1b36eac9fea0] ...
I0207 11:33:09.651530    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 1b36eac9fea0"
I0207 11:33:09.715110    3511 logs.go:123] Gathering logs for kube-proxy [1db52edcdad4] ...
I0207 11:33:09.715135    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 1db52edcdad4"
I0207 11:33:09.755251    3511 logs.go:123] Gathering logs for controller_ingress [8d50139a5311] ...
I0207 11:33:09.755287    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 8d50139a5311"
I0207 11:33:12.320597    3511 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0207 11:33:12.369371    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-apiserver --format={{.ID}}
I0207 11:33:12.414479    3511 logs.go:284] 2 containers: [1b36eac9fea0 39d1988a2fab]
I0207 11:33:12.414581    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_etcd --format={{.ID}}
I0207 11:33:12.445226    3511 logs.go:284] 2 containers: [1eed8f4e884e 5cf1eff8b362]
I0207 11:33:12.445375    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_coredns --format={{.ID}}
I0207 11:33:12.478794    3511 logs.go:284] 2 containers: [06fb15ac4a32 b94028402ad3]
I0207 11:33:12.478905    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-scheduler --format={{.ID}}
I0207 11:33:12.515087    3511 logs.go:284] 2 containers: [1d1e9abd9836 ee7c2215045a]
I0207 11:33:12.515192    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-proxy --format={{.ID}}
I0207 11:33:12.549612    3511 logs.go:284] 2 containers: [1db52edcdad4 407d81ee6871]
I0207 11:33:12.549729    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-controller-manager --format={{.ID}}
I0207 11:33:12.579774    3511 logs.go:284] 2 containers: [e1bb8361945e 885389110d12]
I0207 11:33:12.579900    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kindnet --format={{.ID}}
I0207 11:33:12.606986    3511 logs.go:284] 0 containers: []
W0207 11:33:12.607018    3511 logs.go:286] No container was found matching "kindnet"
I0207 11:33:12.607099    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kubernetes-dashboard --format={{.ID}}
I0207 11:33:12.635250    3511 logs.go:284] 2 containers: [5631cbc21ffd f4b96dcbc9f2]
I0207 11:33:12.635377    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_controller_ingress --format={{.ID}}
I0207 11:33:12.664743    3511 logs.go:284] 1 containers: [8d50139a5311]
I0207 11:33:12.664835    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_storage-provisioner --format={{.ID}}
I0207 11:33:12.703621    3511 logs.go:284] 2 containers: [ec52b9ed7e21 0b85673627c7]
I0207 11:33:12.703648    3511 logs.go:123] Gathering logs for kube-controller-manager [885389110d12] ...
I0207 11:33:12.703697    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 885389110d12"
I0207 11:33:12.765202    3511 logs.go:123] Gathering logs for container status ...
I0207 11:33:12.765252    3511 ssh_runner.go:195] Run: /bin/bash -c "sudo `which crictl || echo crictl` ps -a || sudo docker ps -a"
I0207 11:33:12.927439    3511 logs.go:123] Gathering logs for dmesg ...
I0207 11:33:12.927456    3511 ssh_runner.go:195] Run: /bin/bash -c "sudo dmesg -PH -L=never --level warn,err,crit,alert,emerg | tail -n 400"
I0207 11:33:12.979844    3511 logs.go:123] Gathering logs for etcd [1eed8f4e884e] ...
I0207 11:33:12.979860    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 1eed8f4e884e"
I0207 11:33:13.132479    3511 logs.go:123] Gathering logs for kube-scheduler [ee7c2215045a] ...
I0207 11:33:13.132494    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 ee7c2215045a"
I0207 11:33:13.211337    3511 logs.go:123] Gathering logs for kube-proxy [1db52edcdad4] ...
I0207 11:33:13.211371    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 1db52edcdad4"
I0207 11:33:13.260098    3511 logs.go:123] Gathering logs for coredns [06fb15ac4a32] ...
I0207 11:33:13.260118    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 06fb15ac4a32"
I0207 11:33:13.332126    3511 logs.go:123] Gathering logs for coredns [b94028402ad3] ...
I0207 11:33:13.332145    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 b94028402ad3"
I0207 11:33:13.402667    3511 logs.go:123] Gathering logs for controller_ingress [8d50139a5311] ...
I0207 11:33:13.402693    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 8d50139a5311"
I0207 11:33:13.492651    3511 logs.go:123] Gathering logs for storage-provisioner [ec52b9ed7e21] ...
I0207 11:33:13.492667    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 ec52b9ed7e21"
I0207 11:33:13.529626    3511 logs.go:123] Gathering logs for storage-provisioner [0b85673627c7] ...
I0207 11:33:13.529641    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 0b85673627c7"
I0207 11:33:13.613690    3511 logs.go:123] Gathering logs for kube-apiserver [1b36eac9fea0] ...
I0207 11:33:13.613706    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 1b36eac9fea0"
I0207 11:33:13.681384    3511 logs.go:123] Gathering logs for etcd [5cf1eff8b362] ...
I0207 11:33:13.681404    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 5cf1eff8b362"
I0207 11:33:13.763132    3511 logs.go:123] Gathering logs for kube-controller-manager [e1bb8361945e] ...
I0207 11:33:13.763148    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 e1bb8361945e"
I0207 11:33:13.858294    3511 logs.go:123] Gathering logs for kubernetes-dashboard [f4b96dcbc9f2] ...
I0207 11:33:13.858310    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 f4b96dcbc9f2"
I0207 11:33:13.968675    3511 logs.go:123] Gathering logs for kube-proxy [407d81ee6871] ...
I0207 11:33:13.968706    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 407d81ee6871"
I0207 11:33:14.007168    3511 logs.go:123] Gathering logs for kubernetes-dashboard [5631cbc21ffd] ...
I0207 11:33:14.007182    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 5631cbc21ffd"
I0207 11:33:14.039718    3511 logs.go:123] Gathering logs for Docker ...
I0207 11:33:14.039735    3511 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u docker -u cri-docker -n 400"
I0207 11:33:14.137849    3511 logs.go:123] Gathering logs for kubelet ...
I0207 11:33:14.137875    3511 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u kubelet -n 400"
I0207 11:33:14.239444    3511 logs.go:123] Gathering logs for describe nodes ...
I0207 11:33:14.239462    3511 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig"
W0207 11:33:14.363843    3511 logs.go:130] failed describe nodes: command: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig" /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig": Process exited with status 1
stdout:

stderr:
E0207 03:33:14.352177    4492 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:33:14.352747    4492 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:33:14.354298    4492 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:33:14.355204    4492 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:33:14.357335    4492 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
The connection to the server localhost:8443 was refused - did you specify the right host or port?
 output: 
** stderr ** 
E0207 03:33:14.352177    4492 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:33:14.352747    4492 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:33:14.354298    4492 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:33:14.355204    4492 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:33:14.357335    4492 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
The connection to the server localhost:8443 was refused - did you specify the right host or port?

** /stderr **
I0207 11:33:14.363860    3511 logs.go:123] Gathering logs for kube-apiserver [39d1988a2fab] ...
I0207 11:33:14.363873    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 39d1988a2fab"
I0207 11:33:14.425028    3511 logs.go:123] Gathering logs for kube-scheduler [1d1e9abd9836] ...
I0207 11:33:14.425052    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 1d1e9abd9836"
I0207 11:33:16.985939    3511 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0207 11:33:17.027911    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-apiserver --format={{.ID}}
I0207 11:33:17.107447    3511 logs.go:284] 2 containers: [1b36eac9fea0 39d1988a2fab]
I0207 11:33:17.107561    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_etcd --format={{.ID}}
I0207 11:33:17.139779    3511 logs.go:284] 2 containers: [1eed8f4e884e 5cf1eff8b362]
I0207 11:33:17.139868    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_coredns --format={{.ID}}
I0207 11:33:17.180249    3511 logs.go:284] 2 containers: [06fb15ac4a32 b94028402ad3]
I0207 11:33:17.180348    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-scheduler --format={{.ID}}
I0207 11:33:17.210125    3511 logs.go:284] 2 containers: [1d1e9abd9836 ee7c2215045a]
I0207 11:33:17.210238    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-proxy --format={{.ID}}
I0207 11:33:17.240472    3511 logs.go:284] 2 containers: [1db52edcdad4 407d81ee6871]
I0207 11:33:17.240613    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-controller-manager --format={{.ID}}
I0207 11:33:17.277515    3511 logs.go:284] 2 containers: [e1bb8361945e 885389110d12]
I0207 11:33:17.277609    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kindnet --format={{.ID}}
I0207 11:33:17.313441    3511 logs.go:284] 0 containers: []
W0207 11:33:17.313459    3511 logs.go:286] No container was found matching "kindnet"
I0207 11:33:17.313560    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_controller_ingress --format={{.ID}}
I0207 11:33:17.344277    3511 logs.go:284] 1 containers: [8d50139a5311]
I0207 11:33:17.344402    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_storage-provisioner --format={{.ID}}
I0207 11:33:17.375252    3511 logs.go:284] 2 containers: [ec52b9ed7e21 0b85673627c7]
I0207 11:33:17.375330    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kubernetes-dashboard --format={{.ID}}
I0207 11:33:17.412114    3511 logs.go:284] 2 containers: [5631cbc21ffd f4b96dcbc9f2]
I0207 11:33:17.412180    3511 logs.go:123] Gathering logs for kube-apiserver [39d1988a2fab] ...
I0207 11:33:17.412211    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 39d1988a2fab"
I0207 11:33:17.484922    3511 logs.go:123] Gathering logs for etcd [1eed8f4e884e] ...
I0207 11:33:17.484938    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 1eed8f4e884e"
I0207 11:33:17.635742    3511 logs.go:123] Gathering logs for kube-proxy [1db52edcdad4] ...
I0207 11:33:17.635764    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 1db52edcdad4"
I0207 11:33:17.668925    3511 logs.go:123] Gathering logs for storage-provisioner [0b85673627c7] ...
I0207 11:33:17.668952    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 0b85673627c7"
I0207 11:33:17.743721    3511 logs.go:123] Gathering logs for kubernetes-dashboard [f4b96dcbc9f2] ...
I0207 11:33:17.743740    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 f4b96dcbc9f2"
I0207 11:33:17.806795    3511 logs.go:123] Gathering logs for kube-scheduler [1d1e9abd9836] ...
I0207 11:33:17.806820    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 1d1e9abd9836"
I0207 11:33:17.864571    3511 logs.go:123] Gathering logs for kube-scheduler [ee7c2215045a] ...
I0207 11:33:17.864588    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 ee7c2215045a"
I0207 11:33:17.936723    3511 logs.go:123] Gathering logs for storage-provisioner [ec52b9ed7e21] ...
I0207 11:33:17.936744    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 ec52b9ed7e21"
I0207 11:33:17.979494    3511 logs.go:123] Gathering logs for kubernetes-dashboard [5631cbc21ffd] ...
I0207 11:33:17.979517    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 5631cbc21ffd"
I0207 11:33:18.016349    3511 logs.go:123] Gathering logs for container status ...
I0207 11:33:18.016371    3511 ssh_runner.go:195] Run: /bin/bash -c "sudo `which crictl || echo crictl` ps -a || sudo docker ps -a"
I0207 11:33:18.209482    3511 logs.go:123] Gathering logs for kube-controller-manager [885389110d12] ...
I0207 11:33:18.209506    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 885389110d12"
I0207 11:33:18.267976    3511 logs.go:123] Gathering logs for kubelet ...
I0207 11:33:18.267990    3511 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u kubelet -n 400"
I0207 11:33:18.351135    3511 logs.go:123] Gathering logs for describe nodes ...
I0207 11:33:18.351150    3511 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig"
W0207 11:33:18.454194    3511 logs.go:130] failed describe nodes: command: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig" /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig": Process exited with status 1
stdout:

stderr:
E0207 03:33:18.442570    4743 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:33:18.443641    4743 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:33:18.445484    4743 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:33:18.446087    4743 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:33:18.448221    4743 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
The connection to the server localhost:8443 was refused - did you specify the right host or port?
 output: 
** stderr ** 
E0207 03:33:18.442570    4743 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:33:18.443641    4743 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:33:18.445484    4743 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:33:18.446087    4743 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:33:18.448221    4743 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
The connection to the server localhost:8443 was refused - did you specify the right host or port?

** /stderr **
I0207 11:33:18.454209    3511 logs.go:123] Gathering logs for kube-apiserver [1b36eac9fea0] ...
I0207 11:33:18.454223    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 1b36eac9fea0"
I0207 11:33:18.516781    3511 logs.go:123] Gathering logs for etcd [5cf1eff8b362] ...
I0207 11:33:18.516809    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 5cf1eff8b362"
I0207 11:33:18.579262    3511 logs.go:123] Gathering logs for coredns [06fb15ac4a32] ...
I0207 11:33:18.579278    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 06fb15ac4a32"
I0207 11:33:18.631257    3511 logs.go:123] Gathering logs for coredns [b94028402ad3] ...
I0207 11:33:18.631271    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 b94028402ad3"
I0207 11:33:18.671979    3511 logs.go:123] Gathering logs for dmesg ...
I0207 11:33:18.671994    3511 ssh_runner.go:195] Run: /bin/bash -c "sudo dmesg -PH -L=never --level warn,err,crit,alert,emerg | tail -n 400"
I0207 11:33:18.720199    3511 logs.go:123] Gathering logs for kube-proxy [407d81ee6871] ...
I0207 11:33:18.720215    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 407d81ee6871"
I0207 11:33:18.758218    3511 logs.go:123] Gathering logs for kube-controller-manager [e1bb8361945e] ...
I0207 11:33:18.758232    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 e1bb8361945e"
I0207 11:33:18.838027    3511 logs.go:123] Gathering logs for controller_ingress [8d50139a5311] ...
I0207 11:33:18.838045    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 8d50139a5311"
I0207 11:33:18.944593    3511 logs.go:123] Gathering logs for Docker ...
I0207 11:33:18.944611    3511 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u docker -u cri-docker -n 400"
I0207 11:33:21.515944    3511 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0207 11:33:21.591422    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-apiserver --format={{.ID}}
I0207 11:33:21.626644    3511 logs.go:284] 2 containers: [1b36eac9fea0 39d1988a2fab]
I0207 11:33:21.626793    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_etcd --format={{.ID}}
I0207 11:33:21.653625    3511 logs.go:284] 2 containers: [1eed8f4e884e 5cf1eff8b362]
I0207 11:33:21.653831    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_coredns --format={{.ID}}
I0207 11:33:21.687754    3511 logs.go:284] 2 containers: [06fb15ac4a32 b94028402ad3]
I0207 11:33:21.687881    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-scheduler --format={{.ID}}
I0207 11:33:21.735914    3511 logs.go:284] 2 containers: [1d1e9abd9836 ee7c2215045a]
I0207 11:33:21.736039    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-proxy --format={{.ID}}
I0207 11:33:21.770326    3511 logs.go:284] 2 containers: [1db52edcdad4 407d81ee6871]
I0207 11:33:21.770482    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-controller-manager --format={{.ID}}
I0207 11:33:21.825266    3511 logs.go:284] 2 containers: [e1bb8361945e 885389110d12]
I0207 11:33:21.825376    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kindnet --format={{.ID}}
I0207 11:33:21.856027    3511 logs.go:284] 0 containers: []
W0207 11:33:21.856043    3511 logs.go:286] No container was found matching "kindnet"
I0207 11:33:21.856124    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kubernetes-dashboard --format={{.ID}}
I0207 11:33:21.888752    3511 logs.go:284] 2 containers: [5631cbc21ffd f4b96dcbc9f2]
I0207 11:33:21.888935    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_controller_ingress --format={{.ID}}
I0207 11:33:21.925651    3511 logs.go:284] 1 containers: [8d50139a5311]
I0207 11:33:21.925837    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_storage-provisioner --format={{.ID}}
I0207 11:33:21.967751    3511 logs.go:284] 2 containers: [ec52b9ed7e21 0b85673627c7]
I0207 11:33:21.967780    3511 logs.go:123] Gathering logs for kubernetes-dashboard [f4b96dcbc9f2] ...
I0207 11:33:21.967826    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 f4b96dcbc9f2"
I0207 11:33:22.045758    3511 logs.go:123] Gathering logs for storage-provisioner [ec52b9ed7e21] ...
I0207 11:33:22.045775    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 ec52b9ed7e21"
I0207 11:33:22.095467    3511 logs.go:123] Gathering logs for kube-apiserver [39d1988a2fab] ...
I0207 11:33:22.095483    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 39d1988a2fab"
I0207 11:33:22.167954    3511 logs.go:123] Gathering logs for etcd [1eed8f4e884e] ...
I0207 11:33:22.167970    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 1eed8f4e884e"
I0207 11:33:22.322668    3511 logs.go:123] Gathering logs for kube-controller-manager [885389110d12] ...
I0207 11:33:22.322683    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 885389110d12"
I0207 11:33:22.394897    3511 logs.go:123] Gathering logs for kube-apiserver [1b36eac9fea0] ...
I0207 11:33:22.394927    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 1b36eac9fea0"
I0207 11:33:22.459118    3511 logs.go:123] Gathering logs for kube-proxy [407d81ee6871] ...
I0207 11:33:22.459134    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 407d81ee6871"
I0207 11:33:22.491541    3511 logs.go:123] Gathering logs for Docker ...
I0207 11:33:22.491560    3511 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u docker -u cri-docker -n 400"
I0207 11:33:22.551564    3511 logs.go:123] Gathering logs for kube-scheduler [ee7c2215045a] ...
I0207 11:33:22.551579    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 ee7c2215045a"
I0207 11:33:22.606502    3511 logs.go:123] Gathering logs for kube-proxy [1db52edcdad4] ...
I0207 11:33:22.606520    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 1db52edcdad4"
I0207 11:33:22.637125    3511 logs.go:123] Gathering logs for kube-controller-manager [e1bb8361945e] ...
I0207 11:33:22.637139    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 e1bb8361945e"
I0207 11:33:22.726291    3511 logs.go:123] Gathering logs for controller_ingress [8d50139a5311] ...
I0207 11:33:22.726307    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 8d50139a5311"
I0207 11:33:22.788406    3511 logs.go:123] Gathering logs for storage-provisioner [0b85673627c7] ...
I0207 11:33:22.788420    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 0b85673627c7"
I0207 11:33:22.859745    3511 logs.go:123] Gathering logs for kubelet ...
I0207 11:33:22.859772    3511 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u kubelet -n 400"
I0207 11:33:22.973426    3511 logs.go:123] Gathering logs for etcd [5cf1eff8b362] ...
I0207 11:33:22.973442    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 5cf1eff8b362"
I0207 11:33:23.052013    3511 logs.go:123] Gathering logs for coredns [06fb15ac4a32] ...
I0207 11:33:23.052035    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 06fb15ac4a32"
I0207 11:33:23.140178    3511 logs.go:123] Gathering logs for container status ...
I0207 11:33:23.140303    3511 ssh_runner.go:195] Run: /bin/bash -c "sudo `which crictl || echo crictl` ps -a || sudo docker ps -a"
I0207 11:33:23.312547    3511 logs.go:123] Gathering logs for kube-scheduler [1d1e9abd9836] ...
I0207 11:33:23.312563    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 1d1e9abd9836"
I0207 11:33:23.377264    3511 logs.go:123] Gathering logs for kubernetes-dashboard [5631cbc21ffd] ...
I0207 11:33:23.377278    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 5631cbc21ffd"
I0207 11:33:23.412140    3511 logs.go:123] Gathering logs for dmesg ...
I0207 11:33:23.412159    3511 ssh_runner.go:195] Run: /bin/bash -c "sudo dmesg -PH -L=never --level warn,err,crit,alert,emerg | tail -n 400"
I0207 11:33:23.467282    3511 logs.go:123] Gathering logs for describe nodes ...
I0207 11:33:23.467297    3511 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig"
W0207 11:33:23.578304    3511 logs.go:130] failed describe nodes: command: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig" /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig": Process exited with status 1
stdout:

stderr:
E0207 03:33:23.566064    5076 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:33:23.567104    5076 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:33:23.567931    5076 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:33:23.569595    5076 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:33:23.570221    5076 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
The connection to the server localhost:8443 was refused - did you specify the right host or port?
 output: 
** stderr ** 
E0207 03:33:23.566064    5076 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:33:23.567104    5076 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:33:23.567931    5076 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:33:23.569595    5076 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:33:23.570221    5076 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
The connection to the server localhost:8443 was refused - did you specify the right host or port?

** /stderr **
I0207 11:33:23.578320    3511 logs.go:123] Gathering logs for coredns [b94028402ad3] ...
I0207 11:33:23.578337    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 b94028402ad3"
I0207 11:33:26.145677    3511 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0207 11:33:26.165205    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-apiserver --format={{.ID}}
I0207 11:33:26.194076    3511 logs.go:284] 2 containers: [1b36eac9fea0 39d1988a2fab]
I0207 11:33:26.194197    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_etcd --format={{.ID}}
I0207 11:33:26.220178    3511 logs.go:284] 2 containers: [1eed8f4e884e 5cf1eff8b362]
I0207 11:33:26.220295    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_coredns --format={{.ID}}
I0207 11:33:26.259679    3511 logs.go:284] 2 containers: [06fb15ac4a32 b94028402ad3]
I0207 11:33:26.259790    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-scheduler --format={{.ID}}
I0207 11:33:26.292751    3511 logs.go:284] 2 containers: [1d1e9abd9836 ee7c2215045a]
I0207 11:33:26.292903    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-proxy --format={{.ID}}
I0207 11:33:26.325561    3511 logs.go:284] 2 containers: [1db52edcdad4 407d81ee6871]
I0207 11:33:26.325703    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-controller-manager --format={{.ID}}
I0207 11:33:26.359150    3511 logs.go:284] 2 containers: [e1bb8361945e 885389110d12]
I0207 11:33:26.359295    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kindnet --format={{.ID}}
I0207 11:33:26.390651    3511 logs.go:284] 0 containers: []
W0207 11:33:26.390671    3511 logs.go:286] No container was found matching "kindnet"
I0207 11:33:26.390768    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kubernetes-dashboard --format={{.ID}}
I0207 11:33:26.435004    3511 logs.go:284] 2 containers: [5631cbc21ffd f4b96dcbc9f2]
I0207 11:33:26.435110    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_controller_ingress --format={{.ID}}
I0207 11:33:26.475418    3511 logs.go:284] 1 containers: [8d50139a5311]
I0207 11:33:26.475522    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_storage-provisioner --format={{.ID}}
I0207 11:33:26.504625    3511 logs.go:284] 2 containers: [ec52b9ed7e21 0b85673627c7]
I0207 11:33:26.504683    3511 logs.go:123] Gathering logs for kubelet ...
I0207 11:33:26.504696    3511 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u kubelet -n 400"
I0207 11:33:26.601099    3511 logs.go:123] Gathering logs for describe nodes ...
I0207 11:33:26.601122    3511 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig"
W0207 11:33:26.713345    3511 logs.go:130] failed describe nodes: command: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig" /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig": Process exited with status 1
stdout:

stderr:
E0207 03:33:26.701385    5213 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:33:26.702645    5213 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:33:26.703979    5213 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:33:26.705144    5213 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:33:26.706606    5213 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
The connection to the server localhost:8443 was refused - did you specify the right host or port?
 output: 
** stderr ** 
E0207 03:33:26.701385    5213 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:33:26.702645    5213 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:33:26.703979    5213 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:33:26.705144    5213 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:33:26.706606    5213 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
The connection to the server localhost:8443 was refused - did you specify the right host or port?

** /stderr **
I0207 11:33:26.713361    3511 logs.go:123] Gathering logs for kube-apiserver [1b36eac9fea0] ...
I0207 11:33:26.713378    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 1b36eac9fea0"
I0207 11:33:26.775732    3511 logs.go:123] Gathering logs for kubernetes-dashboard [f4b96dcbc9f2] ...
I0207 11:33:26.775748    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 f4b96dcbc9f2"
I0207 11:33:26.857922    3511 logs.go:123] Gathering logs for controller_ingress [8d50139a5311] ...
I0207 11:33:26.857941    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 8d50139a5311"
I0207 11:33:26.919226    3511 logs.go:123] Gathering logs for storage-provisioner [ec52b9ed7e21] ...
I0207 11:33:26.919241    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 ec52b9ed7e21"
I0207 11:33:26.963060    3511 logs.go:123] Gathering logs for etcd [1eed8f4e884e] ...
I0207 11:33:26.963076    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 1eed8f4e884e"
I0207 11:33:27.196813    3511 logs.go:123] Gathering logs for coredns [b94028402ad3] ...
I0207 11:33:27.196828    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 b94028402ad3"
I0207 11:33:27.236101    3511 logs.go:123] Gathering logs for kube-scheduler [ee7c2215045a] ...
I0207 11:33:27.236119    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 ee7c2215045a"
I0207 11:33:27.291310    3511 logs.go:123] Gathering logs for storage-provisioner [0b85673627c7] ...
I0207 11:33:27.291325    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 0b85673627c7"
I0207 11:33:27.357385    3511 logs.go:123] Gathering logs for container status ...
I0207 11:33:27.357401    3511 ssh_runner.go:195] Run: /bin/bash -c "sudo `which crictl || echo crictl` ps -a || sudo docker ps -a"
I0207 11:33:27.560453    3511 logs.go:123] Gathering logs for dmesg ...
I0207 11:33:27.560483    3511 ssh_runner.go:195] Run: /bin/bash -c "sudo dmesg -PH -L=never --level warn,err,crit,alert,emerg | tail -n 400"
I0207 11:33:27.627430    3511 logs.go:123] Gathering logs for kube-apiserver [39d1988a2fab] ...
I0207 11:33:27.627445    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 39d1988a2fab"
I0207 11:33:27.683459    3511 logs.go:123] Gathering logs for etcd [5cf1eff8b362] ...
I0207 11:33:27.683473    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 5cf1eff8b362"
I0207 11:33:27.785510    3511 logs.go:123] Gathering logs for coredns [06fb15ac4a32] ...
I0207 11:33:27.785526    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 06fb15ac4a32"
I0207 11:33:27.856018    3511 logs.go:123] Gathering logs for kube-scheduler [1d1e9abd9836] ...
I0207 11:33:27.856034    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 1d1e9abd9836"
I0207 11:33:27.930512    3511 logs.go:123] Gathering logs for Docker ...
I0207 11:33:27.930530    3511 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u docker -u cri-docker -n 400"
I0207 11:33:28.017921    3511 logs.go:123] Gathering logs for kube-proxy [1db52edcdad4] ...
I0207 11:33:28.017939    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 1db52edcdad4"
I0207 11:33:28.083531    3511 logs.go:123] Gathering logs for kube-proxy [407d81ee6871] ...
I0207 11:33:28.083547    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 407d81ee6871"
I0207 11:33:28.147410    3511 logs.go:123] Gathering logs for kube-controller-manager [e1bb8361945e] ...
I0207 11:33:28.147425    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 e1bb8361945e"
I0207 11:33:28.222836    3511 logs.go:123] Gathering logs for kube-controller-manager [885389110d12] ...
I0207 11:33:28.222851    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 885389110d12"
I0207 11:33:28.287319    3511 logs.go:123] Gathering logs for kubernetes-dashboard [5631cbc21ffd] ...
I0207 11:33:28.287333    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 5631cbc21ffd"
I0207 11:33:30.825923    3511 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0207 11:33:30.868601    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-apiserver --format={{.ID}}
I0207 11:33:30.933858    3511 logs.go:284] 2 containers: [1b36eac9fea0 39d1988a2fab]
I0207 11:33:30.933971    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_etcd --format={{.ID}}
I0207 11:33:30.983732    3511 logs.go:284] 2 containers: [1eed8f4e884e 5cf1eff8b362]
I0207 11:33:30.983834    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_coredns --format={{.ID}}
I0207 11:33:31.031026    3511 logs.go:284] 2 containers: [06fb15ac4a32 b94028402ad3]
I0207 11:33:31.031149    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-scheduler --format={{.ID}}
I0207 11:33:31.104551    3511 logs.go:284] 2 containers: [1d1e9abd9836 ee7c2215045a]
I0207 11:33:31.104669    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-proxy --format={{.ID}}
I0207 11:33:31.140699    3511 logs.go:284] 2 containers: [1db52edcdad4 407d81ee6871]
I0207 11:33:31.140815    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-controller-manager --format={{.ID}}
I0207 11:33:31.183699    3511 logs.go:284] 2 containers: [e1bb8361945e 885389110d12]
I0207 11:33:31.183796    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kindnet --format={{.ID}}
I0207 11:33:31.214774    3511 logs.go:284] 0 containers: []
W0207 11:33:31.214792    3511 logs.go:286] No container was found matching "kindnet"
I0207 11:33:31.214873    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kubernetes-dashboard --format={{.ID}}
I0207 11:33:31.244775    3511 logs.go:284] 2 containers: [5631cbc21ffd f4b96dcbc9f2]
I0207 11:33:31.244867    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_controller_ingress --format={{.ID}}
I0207 11:33:31.279526    3511 logs.go:284] 1 containers: [8d50139a5311]
I0207 11:33:31.279622    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_storage-provisioner --format={{.ID}}
I0207 11:33:31.307981    3511 logs.go:284] 2 containers: [ec52b9ed7e21 0b85673627c7]
I0207 11:33:31.308042    3511 logs.go:123] Gathering logs for storage-provisioner [0b85673627c7] ...
I0207 11:33:31.308057    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 0b85673627c7"
I0207 11:33:31.370988    3511 logs.go:123] Gathering logs for Docker ...
I0207 11:33:31.371006    3511 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u docker -u cri-docker -n 400"
I0207 11:33:31.433554    3511 logs.go:123] Gathering logs for dmesg ...
I0207 11:33:31.433571    3511 ssh_runner.go:195] Run: /bin/bash -c "sudo dmesg -PH -L=never --level warn,err,crit,alert,emerg | tail -n 400"
I0207 11:33:31.484043    3511 logs.go:123] Gathering logs for etcd [1eed8f4e884e] ...
I0207 11:33:31.484062    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 1eed8f4e884e"
I0207 11:33:31.656524    3511 logs.go:123] Gathering logs for coredns [06fb15ac4a32] ...
I0207 11:33:31.656541    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 06fb15ac4a32"
I0207 11:33:31.709696    3511 logs.go:123] Gathering logs for kube-proxy [407d81ee6871] ...
I0207 11:33:31.709711    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 407d81ee6871"
I0207 11:33:31.742174    3511 logs.go:123] Gathering logs for kube-controller-manager [e1bb8361945e] ...
I0207 11:33:31.742197    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 e1bb8361945e"
I0207 11:33:31.880629    3511 logs.go:123] Gathering logs for kube-scheduler [1d1e9abd9836] ...
I0207 11:33:31.880649    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 1d1e9abd9836"
I0207 11:33:31.976266    3511 logs.go:123] Gathering logs for kube-proxy [1db52edcdad4] ...
I0207 11:33:31.976287    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 1db52edcdad4"
I0207 11:33:32.020798    3511 logs.go:123] Gathering logs for container status ...
I0207 11:33:32.020813    3511 ssh_runner.go:195] Run: /bin/bash -c "sudo `which crictl || echo crictl` ps -a || sudo docker ps -a"
I0207 11:33:32.191070    3511 logs.go:123] Gathering logs for kubernetes-dashboard [5631cbc21ffd] ...
I0207 11:33:32.191100    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 5631cbc21ffd"
I0207 11:33:32.225387    3511 logs.go:123] Gathering logs for kubernetes-dashboard [f4b96dcbc9f2] ...
I0207 11:33:32.225401    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 f4b96dcbc9f2"
I0207 11:33:32.297054    3511 logs.go:123] Gathering logs for controller_ingress [8d50139a5311] ...
I0207 11:33:32.297072    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 8d50139a5311"
I0207 11:33:32.370661    3511 logs.go:123] Gathering logs for kubelet ...
I0207 11:33:32.370686    3511 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u kubelet -n 400"
I0207 11:33:32.464269    3511 logs.go:123] Gathering logs for kube-apiserver [1b36eac9fea0] ...
I0207 11:33:32.464294    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 1b36eac9fea0"
I0207 11:33:32.517582    3511 logs.go:123] Gathering logs for kube-apiserver [39d1988a2fab] ...
I0207 11:33:32.517597    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 39d1988a2fab"
I0207 11:33:32.565542    3511 logs.go:123] Gathering logs for coredns [b94028402ad3] ...
I0207 11:33:32.565587    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 b94028402ad3"
I0207 11:33:32.608413    3511 logs.go:123] Gathering logs for kube-scheduler [ee7c2215045a] ...
I0207 11:33:32.608429    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 ee7c2215045a"
I0207 11:33:32.660494    3511 logs.go:123] Gathering logs for storage-provisioner [ec52b9ed7e21] ...
I0207 11:33:32.660509    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 ec52b9ed7e21"
I0207 11:33:32.695408    3511 logs.go:123] Gathering logs for describe nodes ...
I0207 11:33:32.695423    3511 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig"
W0207 11:33:32.800681    3511 logs.go:130] failed describe nodes: command: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig" /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig": Process exited with status 1
stdout:

stderr:
E0207 03:33:32.788261    5646 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:33:32.789719    5646 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:33:32.790590    5646 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:33:32.792010    5646 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:33:32.792881    5646 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
The connection to the server localhost:8443 was refused - did you specify the right host or port?
 output: 
** stderr ** 
E0207 03:33:32.788261    5646 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:33:32.789719    5646 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:33:32.790590    5646 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:33:32.792010    5646 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:33:32.792881    5646 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
The connection to the server localhost:8443 was refused - did you specify the right host or port?

** /stderr **
I0207 11:33:32.800711    3511 logs.go:123] Gathering logs for etcd [5cf1eff8b362] ...
I0207 11:33:32.800730    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 5cf1eff8b362"
I0207 11:33:32.869538    3511 logs.go:123] Gathering logs for kube-controller-manager [885389110d12] ...
I0207 11:33:32.869554    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 885389110d12"
I0207 11:33:35.454084    3511 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0207 11:33:35.480498    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-apiserver --format={{.ID}}
I0207 11:33:35.519459    3511 logs.go:284] 2 containers: [1b36eac9fea0 39d1988a2fab]
I0207 11:33:35.519551    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_etcd --format={{.ID}}
I0207 11:33:35.549287    3511 logs.go:284] 2 containers: [1eed8f4e884e 5cf1eff8b362]
I0207 11:33:35.549382    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_coredns --format={{.ID}}
I0207 11:33:35.578431    3511 logs.go:284] 2 containers: [06fb15ac4a32 b94028402ad3]
I0207 11:33:35.578567    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-scheduler --format={{.ID}}
I0207 11:33:35.610323    3511 logs.go:284] 2 containers: [1d1e9abd9836 ee7c2215045a]
I0207 11:33:35.610555    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-proxy --format={{.ID}}
I0207 11:33:35.639766    3511 logs.go:284] 2 containers: [1db52edcdad4 407d81ee6871]
I0207 11:33:35.639873    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-controller-manager --format={{.ID}}
I0207 11:33:35.677559    3511 logs.go:284] 2 containers: [e1bb8361945e 885389110d12]
I0207 11:33:35.677757    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kindnet --format={{.ID}}
I0207 11:33:35.707327    3511 logs.go:284] 0 containers: []
W0207 11:33:35.707341    3511 logs.go:286] No container was found matching "kindnet"
I0207 11:33:35.707420    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_controller_ingress --format={{.ID}}
I0207 11:33:35.742500    3511 logs.go:284] 1 containers: [8d50139a5311]
I0207 11:33:35.742597    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_storage-provisioner --format={{.ID}}
I0207 11:33:35.778997    3511 logs.go:284] 2 containers: [ec52b9ed7e21 0b85673627c7]
I0207 11:33:35.779149    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kubernetes-dashboard --format={{.ID}}
I0207 11:33:35.824840    3511 logs.go:284] 2 containers: [5631cbc21ffd f4b96dcbc9f2]
I0207 11:33:35.824908    3511 logs.go:123] Gathering logs for storage-provisioner [0b85673627c7] ...
I0207 11:33:35.824941    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 0b85673627c7"
I0207 11:33:35.889088    3511 logs.go:123] Gathering logs for kubernetes-dashboard [f4b96dcbc9f2] ...
I0207 11:33:35.889104    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 f4b96dcbc9f2"
I0207 11:33:35.977332    3511 logs.go:123] Gathering logs for kubelet ...
I0207 11:33:35.977349    3511 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u kubelet -n 400"
I0207 11:33:36.082792    3511 logs.go:123] Gathering logs for dmesg ...
I0207 11:33:36.082809    3511 ssh_runner.go:195] Run: /bin/bash -c "sudo dmesg -PH -L=never --level warn,err,crit,alert,emerg | tail -n 400"
I0207 11:33:36.147721    3511 logs.go:123] Gathering logs for kube-apiserver [1b36eac9fea0] ...
I0207 11:33:36.147742    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 1b36eac9fea0"
I0207 11:33:36.211484    3511 logs.go:123] Gathering logs for kube-proxy [407d81ee6871] ...
I0207 11:33:36.211499    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 407d81ee6871"
I0207 11:33:36.248879    3511 logs.go:123] Gathering logs for kube-controller-manager [885389110d12] ...
I0207 11:33:36.248901    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 885389110d12"
I0207 11:33:36.308263    3511 logs.go:123] Gathering logs for controller_ingress [8d50139a5311] ...
I0207 11:33:36.308277    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 8d50139a5311"
I0207 11:33:36.364134    3511 logs.go:123] Gathering logs for kube-apiserver [39d1988a2fab] ...
I0207 11:33:36.364155    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 39d1988a2fab"
I0207 11:33:36.431183    3511 logs.go:123] Gathering logs for etcd [1eed8f4e884e] ...
I0207 11:33:36.431199    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 1eed8f4e884e"
I0207 11:33:36.613629    3511 logs.go:123] Gathering logs for etcd [5cf1eff8b362] ...
I0207 11:33:36.613645    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 5cf1eff8b362"
I0207 11:33:36.674751    3511 logs.go:123] Gathering logs for kube-controller-manager [e1bb8361945e] ...
I0207 11:33:36.674766    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 e1bb8361945e"
I0207 11:33:36.769325    3511 logs.go:123] Gathering logs for Docker ...
I0207 11:33:36.769350    3511 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u docker -u cri-docker -n 400"
I0207 11:33:36.849602    3511 logs.go:123] Gathering logs for describe nodes ...
I0207 11:33:36.849619    3511 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig"
W0207 11:33:37.027089    3511 logs.go:130] failed describe nodes: command: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig" /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig": Process exited with status 1
stdout:

stderr:
E0207 03:33:37.010858    5876 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:33:37.012965    5876 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:33:37.014012    5876 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:33:37.015240    5876 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:33:37.016529    5876 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
The connection to the server localhost:8443 was refused - did you specify the right host or port?
 output: 
** stderr ** 
E0207 03:33:37.010858    5876 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:33:37.012965    5876 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:33:37.014012    5876 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:33:37.015240    5876 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:33:37.016529    5876 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
The connection to the server localhost:8443 was refused - did you specify the right host or port?

** /stderr **
I0207 11:33:37.027106    3511 logs.go:123] Gathering logs for coredns [06fb15ac4a32] ...
I0207 11:33:37.027125    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 06fb15ac4a32"
I0207 11:33:37.074855    3511 logs.go:123] Gathering logs for coredns [b94028402ad3] ...
I0207 11:33:37.074871    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 b94028402ad3"
I0207 11:33:37.128287    3511 logs.go:123] Gathering logs for kube-scheduler [ee7c2215045a] ...
I0207 11:33:37.128306    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 ee7c2215045a"
I0207 11:33:37.237493    3511 logs.go:123] Gathering logs for storage-provisioner [ec52b9ed7e21] ...
I0207 11:33:37.237514    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 ec52b9ed7e21"
I0207 11:33:37.295277    3511 logs.go:123] Gathering logs for container status ...
I0207 11:33:37.295297    3511 ssh_runner.go:195] Run: /bin/bash -c "sudo `which crictl || echo crictl` ps -a || sudo docker ps -a"
I0207 11:33:37.476768    3511 logs.go:123] Gathering logs for kube-scheduler [1d1e9abd9836] ...
I0207 11:33:37.476783    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 1d1e9abd9836"
I0207 11:33:37.536617    3511 logs.go:123] Gathering logs for kube-proxy [1db52edcdad4] ...
I0207 11:33:37.536633    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 1db52edcdad4"
I0207 11:33:37.591955    3511 logs.go:123] Gathering logs for kubernetes-dashboard [5631cbc21ffd] ...
I0207 11:33:37.591971    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 5631cbc21ffd"
I0207 11:33:40.139909    3511 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0207 11:33:40.158136    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-apiserver --format={{.ID}}
I0207 11:33:40.198117    3511 logs.go:284] 2 containers: [1b36eac9fea0 39d1988a2fab]
I0207 11:33:40.198231    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_etcd --format={{.ID}}
I0207 11:33:40.234083    3511 logs.go:284] 2 containers: [1eed8f4e884e 5cf1eff8b362]
I0207 11:33:40.234181    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_coredns --format={{.ID}}
I0207 11:33:40.264206    3511 logs.go:284] 2 containers: [06fb15ac4a32 b94028402ad3]
I0207 11:33:40.264309    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-scheduler --format={{.ID}}
I0207 11:33:40.293587    3511 logs.go:284] 2 containers: [1d1e9abd9836 ee7c2215045a]
I0207 11:33:40.293706    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-proxy --format={{.ID}}
I0207 11:33:40.324797    3511 logs.go:284] 2 containers: [1db52edcdad4 407d81ee6871]
I0207 11:33:40.324940    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-controller-manager --format={{.ID}}
I0207 11:33:40.359421    3511 logs.go:284] 2 containers: [e1bb8361945e 885389110d12]
I0207 11:33:40.359576    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kindnet --format={{.ID}}
I0207 11:33:40.387655    3511 logs.go:284] 0 containers: []
W0207 11:33:40.387672    3511 logs.go:286] No container was found matching "kindnet"
I0207 11:33:40.387747    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_controller_ingress --format={{.ID}}
I0207 11:33:40.426657    3511 logs.go:284] 1 containers: [8d50139a5311]
I0207 11:33:40.426751    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_storage-provisioner --format={{.ID}}
I0207 11:33:40.468265    3511 logs.go:284] 2 containers: [ec52b9ed7e21 0b85673627c7]
I0207 11:33:40.468369    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kubernetes-dashboard --format={{.ID}}
I0207 11:33:40.529047    3511 logs.go:284] 2 containers: [5631cbc21ffd f4b96dcbc9f2]
I0207 11:33:40.529102    3511 logs.go:123] Gathering logs for describe nodes ...
I0207 11:33:40.529118    3511 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig"
W0207 11:33:40.678443    3511 logs.go:130] failed describe nodes: command: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig" /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig": Process exited with status 1
stdout:

stderr:
E0207 03:33:40.665626    6088 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:33:40.666680    6088 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:33:40.667458    6088 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:33:40.669903    6088 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:33:40.670561    6088 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
The connection to the server localhost:8443 was refused - did you specify the right host or port?
 output: 
** stderr ** 
E0207 03:33:40.665626    6088 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:33:40.666680    6088 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:33:40.667458    6088 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:33:40.669903    6088 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:33:40.670561    6088 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
The connection to the server localhost:8443 was refused - did you specify the right host or port?

** /stderr **
I0207 11:33:40.678461    3511 logs.go:123] Gathering logs for storage-provisioner [0b85673627c7] ...
I0207 11:33:40.678475    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 0b85673627c7"
I0207 11:33:40.742999    3511 logs.go:123] Gathering logs for container status ...
I0207 11:33:40.743025    3511 ssh_runner.go:195] Run: /bin/bash -c "sudo `which crictl || echo crictl` ps -a || sudo docker ps -a"
I0207 11:33:40.929567    3511 logs.go:123] Gathering logs for kube-apiserver [39d1988a2fab] ...
I0207 11:33:40.929586    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 39d1988a2fab"
I0207 11:33:40.976729    3511 logs.go:123] Gathering logs for etcd [5cf1eff8b362] ...
I0207 11:33:40.976743    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 5cf1eff8b362"
I0207 11:33:41.051576    3511 logs.go:123] Gathering logs for coredns [b94028402ad3] ...
I0207 11:33:41.051592    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 b94028402ad3"
I0207 11:33:41.101381    3511 logs.go:123] Gathering logs for kube-scheduler [1d1e9abd9836] ...
I0207 11:33:41.101410    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 1d1e9abd9836"
I0207 11:33:41.166579    3511 logs.go:123] Gathering logs for kube-scheduler [ee7c2215045a] ...
I0207 11:33:41.166595    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 ee7c2215045a"
I0207 11:33:41.217128    3511 logs.go:123] Gathering logs for kube-proxy [1db52edcdad4] ...
I0207 11:33:41.217156    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 1db52edcdad4"
I0207 11:33:41.269026    3511 logs.go:123] Gathering logs for kubernetes-dashboard [5631cbc21ffd] ...
I0207 11:33:41.269048    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 5631cbc21ffd"
I0207 11:33:41.301029    3511 logs.go:123] Gathering logs for dmesg ...
I0207 11:33:41.301043    3511 ssh_runner.go:195] Run: /bin/bash -c "sudo dmesg -PH -L=never --level warn,err,crit,alert,emerg | tail -n 400"
I0207 11:33:41.375480    3511 logs.go:123] Gathering logs for kube-apiserver [1b36eac9fea0] ...
I0207 11:33:41.375514    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 1b36eac9fea0"
I0207 11:33:41.450921    3511 logs.go:123] Gathering logs for coredns [06fb15ac4a32] ...
I0207 11:33:41.450942    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 06fb15ac4a32"
I0207 11:33:41.539365    3511 logs.go:123] Gathering logs for kube-proxy [407d81ee6871] ...
I0207 11:33:41.539379    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 407d81ee6871"
I0207 11:33:41.583895    3511 logs.go:123] Gathering logs for controller_ingress [8d50139a5311] ...
I0207 11:33:41.583916    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 8d50139a5311"
I0207 11:33:41.685364    3511 logs.go:123] Gathering logs for kubernetes-dashboard [f4b96dcbc9f2] ...
I0207 11:33:41.685380    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 f4b96dcbc9f2"
I0207 11:33:41.782945    3511 logs.go:123] Gathering logs for Docker ...
I0207 11:33:41.782960    3511 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u docker -u cri-docker -n 400"
I0207 11:33:41.844967    3511 logs.go:123] Gathering logs for kubelet ...
I0207 11:33:41.844996    3511 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u kubelet -n 400"
I0207 11:33:41.953262    3511 logs.go:123] Gathering logs for etcd [1eed8f4e884e] ...
I0207 11:33:41.953278    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 1eed8f4e884e"
I0207 11:33:42.095738    3511 logs.go:123] Gathering logs for kube-controller-manager [e1bb8361945e] ...
I0207 11:33:42.095759    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 e1bb8361945e"
I0207 11:33:42.197205    3511 logs.go:123] Gathering logs for kube-controller-manager [885389110d12] ...
I0207 11:33:42.197238    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 885389110d12"
I0207 11:33:42.270302    3511 logs.go:123] Gathering logs for storage-provisioner [ec52b9ed7e21] ...
I0207 11:33:42.270322    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 ec52b9ed7e21"
I0207 11:33:44.807694    3511 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0207 11:33:44.826182    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-apiserver --format={{.ID}}
I0207 11:33:44.853828    3511 logs.go:284] 2 containers: [1b36eac9fea0 39d1988a2fab]
I0207 11:33:44.853948    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_etcd --format={{.ID}}
I0207 11:33:44.887055    3511 logs.go:284] 2 containers: [1eed8f4e884e 5cf1eff8b362]
I0207 11:33:44.887166    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_coredns --format={{.ID}}
I0207 11:33:44.943467    3511 logs.go:284] 2 containers: [06fb15ac4a32 b94028402ad3]
I0207 11:33:44.943587    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-scheduler --format={{.ID}}
I0207 11:33:44.980720    3511 logs.go:284] 2 containers: [1d1e9abd9836 ee7c2215045a]
I0207 11:33:44.980860    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-proxy --format={{.ID}}
I0207 11:33:45.012642    3511 logs.go:284] 2 containers: [1db52edcdad4 407d81ee6871]
I0207 11:33:45.012787    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-controller-manager --format={{.ID}}
I0207 11:33:45.058949    3511 logs.go:284] 2 containers: [e1bb8361945e 885389110d12]
I0207 11:33:45.059045    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kindnet --format={{.ID}}
I0207 11:33:45.111732    3511 logs.go:284] 0 containers: []
W0207 11:33:45.111748    3511 logs.go:286] No container was found matching "kindnet"
I0207 11:33:45.111821    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_controller_ingress --format={{.ID}}
I0207 11:33:45.156103    3511 logs.go:284] 1 containers: [8d50139a5311]
I0207 11:33:45.156238    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_storage-provisioner --format={{.ID}}
I0207 11:33:45.199259    3511 logs.go:284] 2 containers: [ec52b9ed7e21 0b85673627c7]
I0207 11:33:45.199417    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kubernetes-dashboard --format={{.ID}}
I0207 11:33:45.237286    3511 logs.go:284] 2 containers: [5631cbc21ffd f4b96dcbc9f2]
I0207 11:33:45.237340    3511 logs.go:123] Gathering logs for dmesg ...
I0207 11:33:45.237352    3511 ssh_runner.go:195] Run: /bin/bash -c "sudo dmesg -PH -L=never --level warn,err,crit,alert,emerg | tail -n 400"
I0207 11:33:45.312465    3511 logs.go:123] Gathering logs for coredns [b94028402ad3] ...
I0207 11:33:45.312489    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 b94028402ad3"
I0207 11:33:45.364754    3511 logs.go:123] Gathering logs for storage-provisioner [0b85673627c7] ...
I0207 11:33:45.364776    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 0b85673627c7"
I0207 11:33:45.455172    3511 logs.go:123] Gathering logs for container status ...
I0207 11:33:45.455197    3511 ssh_runner.go:195] Run: /bin/bash -c "sudo `which crictl || echo crictl` ps -a || sudo docker ps -a"
I0207 11:33:45.655921    3511 logs.go:123] Gathering logs for kubernetes-dashboard [5631cbc21ffd] ...
I0207 11:33:45.655950    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 5631cbc21ffd"
I0207 11:33:45.706323    3511 logs.go:123] Gathering logs for kubernetes-dashboard [f4b96dcbc9f2] ...
I0207 11:33:45.706338    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 f4b96dcbc9f2"
I0207 11:33:45.778340    3511 logs.go:123] Gathering logs for Docker ...
I0207 11:33:45.778356    3511 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u docker -u cri-docker -n 400"
I0207 11:33:45.864496    3511 logs.go:123] Gathering logs for describe nodes ...
I0207 11:33:45.864511    3511 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig"
W0207 11:33:46.272789    3511 logs.go:130] failed describe nodes: command: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig" /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig": Process exited with status 1
stdout:

stderr:
E0207 03:33:46.260362    6460 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:33:46.261356    6460 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:33:46.262404    6460 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:33:46.263443    6460 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:33:46.264735    6460 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
The connection to the server localhost:8443 was refused - did you specify the right host or port?
 output: 
** stderr ** 
E0207 03:33:46.260362    6460 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:33:46.261356    6460 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:33:46.262404    6460 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:33:46.263443    6460 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:33:46.264735    6460 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
The connection to the server localhost:8443 was refused - did you specify the right host or port?

** /stderr **
I0207 11:33:46.272806    3511 logs.go:123] Gathering logs for kube-apiserver [1b36eac9fea0] ...
I0207 11:33:46.272821    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 1b36eac9fea0"
I0207 11:33:46.332151    3511 logs.go:123] Gathering logs for kube-apiserver [39d1988a2fab] ...
I0207 11:33:46.332168    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 39d1988a2fab"
I0207 11:33:46.379985    3511 logs.go:123] Gathering logs for coredns [06fb15ac4a32] ...
I0207 11:33:46.380003    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 06fb15ac4a32"
I0207 11:33:46.444690    3511 logs.go:123] Gathering logs for kube-proxy [407d81ee6871] ...
I0207 11:33:46.444714    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 407d81ee6871"
I0207 11:33:46.480871    3511 logs.go:123] Gathering logs for kubelet ...
I0207 11:33:46.480890    3511 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u kubelet -n 400"
I0207 11:33:46.559524    3511 logs.go:123] Gathering logs for etcd [1eed8f4e884e] ...
I0207 11:33:46.559550    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 1eed8f4e884e"
I0207 11:33:46.705924    3511 logs.go:123] Gathering logs for etcd [5cf1eff8b362] ...
I0207 11:33:46.705941    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 5cf1eff8b362"
I0207 11:33:46.797263    3511 logs.go:123] Gathering logs for kube-proxy [1db52edcdad4] ...
I0207 11:33:46.797285    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 1db52edcdad4"
I0207 11:33:46.834326    3511 logs.go:123] Gathering logs for kube-controller-manager [885389110d12] ...
I0207 11:33:46.834343    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 885389110d12"
I0207 11:33:46.904854    3511 logs.go:123] Gathering logs for kube-scheduler [1d1e9abd9836] ...
I0207 11:33:46.904872    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 1d1e9abd9836"
I0207 11:33:46.984535    3511 logs.go:123] Gathering logs for kube-scheduler [ee7c2215045a] ...
I0207 11:33:46.984551    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 ee7c2215045a"
I0207 11:33:47.038777    3511 logs.go:123] Gathering logs for kube-controller-manager [e1bb8361945e] ...
I0207 11:33:47.038797    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 e1bb8361945e"
I0207 11:33:47.195693    3511 logs.go:123] Gathering logs for controller_ingress [8d50139a5311] ...
I0207 11:33:47.195708    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 8d50139a5311"
I0207 11:33:47.255340    3511 logs.go:123] Gathering logs for storage-provisioner [ec52b9ed7e21] ...
I0207 11:33:47.255356    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 ec52b9ed7e21"
I0207 11:33:49.804561    3511 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0207 11:33:49.831528    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-apiserver --format={{.ID}}
I0207 11:33:49.870719    3511 logs.go:284] 2 containers: [1b36eac9fea0 39d1988a2fab]
I0207 11:33:49.870833    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_etcd --format={{.ID}}
I0207 11:33:49.913804    3511 logs.go:284] 2 containers: [1eed8f4e884e 5cf1eff8b362]
I0207 11:33:49.913908    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_coredns --format={{.ID}}
I0207 11:33:49.966148    3511 logs.go:284] 2 containers: [06fb15ac4a32 b94028402ad3]
I0207 11:33:49.966307    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-scheduler --format={{.ID}}
I0207 11:33:50.012765    3511 logs.go:284] 2 containers: [1d1e9abd9836 ee7c2215045a]
I0207 11:33:50.012867    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-proxy --format={{.ID}}
I0207 11:33:50.077095    3511 logs.go:284] 2 containers: [1db52edcdad4 407d81ee6871]
I0207 11:33:50.077194    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-controller-manager --format={{.ID}}
I0207 11:33:50.141544    3511 logs.go:284] 2 containers: [e1bb8361945e 885389110d12]
I0207 11:33:50.141656    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kindnet --format={{.ID}}
I0207 11:33:50.208151    3511 logs.go:284] 0 containers: []
W0207 11:33:50.208175    3511 logs.go:286] No container was found matching "kindnet"
I0207 11:33:50.208260    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kubernetes-dashboard --format={{.ID}}
I0207 11:33:50.255216    3511 logs.go:284] 2 containers: [5631cbc21ffd f4b96dcbc9f2]
I0207 11:33:50.255366    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_controller_ingress --format={{.ID}}
I0207 11:33:50.289292    3511 logs.go:284] 1 containers: [8d50139a5311]
I0207 11:33:50.289397    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_storage-provisioner --format={{.ID}}
I0207 11:33:50.324546    3511 logs.go:284] 2 containers: [ec52b9ed7e21 0b85673627c7]
I0207 11:33:50.324574    3511 logs.go:123] Gathering logs for storage-provisioner [ec52b9ed7e21] ...
I0207 11:33:50.324587    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 ec52b9ed7e21"
I0207 11:33:50.368970    3511 logs.go:123] Gathering logs for describe nodes ...
I0207 11:33:50.368985    3511 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig"
W0207 11:33:50.479670    3511 logs.go:130] failed describe nodes: command: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig" /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig": Process exited with status 1
stdout:

stderr:
E0207 03:33:50.466987    6710 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:33:50.468206    6710 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:33:50.469827    6710 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:33:50.471382    6710 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:33:50.472331    6710 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
The connection to the server localhost:8443 was refused - did you specify the right host or port?
 output: 
** stderr ** 
E0207 03:33:50.466987    6710 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:33:50.468206    6710 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:33:50.469827    6710 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:33:50.471382    6710 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:33:50.472331    6710 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
The connection to the server localhost:8443 was refused - did you specify the right host or port?

** /stderr **
I0207 11:33:50.479682    3511 logs.go:123] Gathering logs for kube-apiserver [1b36eac9fea0] ...
I0207 11:33:50.479697    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 1b36eac9fea0"
I0207 11:33:50.539092    3511 logs.go:123] Gathering logs for etcd [5cf1eff8b362] ...
I0207 11:33:50.539107    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 5cf1eff8b362"
I0207 11:33:50.624408    3511 logs.go:123] Gathering logs for kube-scheduler [ee7c2215045a] ...
I0207 11:33:50.624422    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 ee7c2215045a"
I0207 11:33:50.687438    3511 logs.go:123] Gathering logs for kube-proxy [407d81ee6871] ...
I0207 11:33:50.687458    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 407d81ee6871"
I0207 11:33:50.741947    3511 logs.go:123] Gathering logs for kubernetes-dashboard [5631cbc21ffd] ...
I0207 11:33:50.741963    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 5631cbc21ffd"
I0207 11:33:50.791163    3511 logs.go:123] Gathering logs for kubernetes-dashboard [f4b96dcbc9f2] ...
I0207 11:33:50.791180    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 f4b96dcbc9f2"
I0207 11:33:50.926386    3511 logs.go:123] Gathering logs for storage-provisioner [0b85673627c7] ...
I0207 11:33:50.926403    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 0b85673627c7"
I0207 11:33:50.998819    3511 logs.go:123] Gathering logs for Docker ...
I0207 11:33:50.998838    3511 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u docker -u cri-docker -n 400"
I0207 11:33:51.062974    3511 logs.go:123] Gathering logs for kube-apiserver [39d1988a2fab] ...
I0207 11:33:51.062989    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 39d1988a2fab"
I0207 11:33:51.119105    3511 logs.go:123] Gathering logs for kube-proxy [1db52edcdad4] ...
I0207 11:33:51.119122    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 1db52edcdad4"
I0207 11:33:51.164737    3511 logs.go:123] Gathering logs for kube-controller-manager [885389110d12] ...
I0207 11:33:51.164752    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 885389110d12"
I0207 11:33:51.230187    3511 logs.go:123] Gathering logs for controller_ingress [8d50139a5311] ...
I0207 11:33:51.230205    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 8d50139a5311"
I0207 11:33:51.301634    3511 logs.go:123] Gathering logs for container status ...
I0207 11:33:51.301650    3511 ssh_runner.go:195] Run: /bin/bash -c "sudo `which crictl || echo crictl` ps -a || sudo docker ps -a"
I0207 11:33:51.445604    3511 logs.go:123] Gathering logs for kubelet ...
I0207 11:33:51.445619    3511 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u kubelet -n 400"
I0207 11:33:51.533547    3511 logs.go:123] Gathering logs for coredns [06fb15ac4a32] ...
I0207 11:33:51.533564    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 06fb15ac4a32"
I0207 11:33:51.589227    3511 logs.go:123] Gathering logs for kube-scheduler [1d1e9abd9836] ...
I0207 11:33:51.589242    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 1d1e9abd9836"
I0207 11:33:51.656096    3511 logs.go:123] Gathering logs for kube-controller-manager [e1bb8361945e] ...
I0207 11:33:51.656113    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 e1bb8361945e"
I0207 11:33:51.809497    3511 logs.go:123] Gathering logs for dmesg ...
I0207 11:33:51.809513    3511 ssh_runner.go:195] Run: /bin/bash -c "sudo dmesg -PH -L=never --level warn,err,crit,alert,emerg | tail -n 400"
I0207 11:33:51.913624    3511 logs.go:123] Gathering logs for etcd [1eed8f4e884e] ...
I0207 11:33:51.913646    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 1eed8f4e884e"
I0207 11:33:52.088796    3511 logs.go:123] Gathering logs for coredns [b94028402ad3] ...
I0207 11:33:52.088812    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 b94028402ad3"
I0207 11:33:54.646271    3511 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0207 11:33:54.671284    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-apiserver --format={{.ID}}
I0207 11:33:54.702135    3511 logs.go:284] 2 containers: [1b36eac9fea0 39d1988a2fab]
I0207 11:33:54.702250    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_etcd --format={{.ID}}
I0207 11:33:54.743489    3511 logs.go:284] 2 containers: [1eed8f4e884e 5cf1eff8b362]
I0207 11:33:54.743587    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_coredns --format={{.ID}}
I0207 11:33:54.800466    3511 logs.go:284] 2 containers: [06fb15ac4a32 b94028402ad3]
I0207 11:33:54.800589    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-scheduler --format={{.ID}}
I0207 11:33:54.848515    3511 logs.go:284] 2 containers: [1d1e9abd9836 ee7c2215045a]
I0207 11:33:54.848659    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-proxy --format={{.ID}}
I0207 11:33:54.892908    3511 logs.go:284] 2 containers: [1db52edcdad4 407d81ee6871]
I0207 11:33:54.893056    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-controller-manager --format={{.ID}}
I0207 11:33:54.924106    3511 logs.go:284] 2 containers: [e1bb8361945e 885389110d12]
I0207 11:33:54.924202    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kindnet --format={{.ID}}
I0207 11:33:54.955602    3511 logs.go:284] 0 containers: []
W0207 11:33:54.955618    3511 logs.go:286] No container was found matching "kindnet"
I0207 11:33:54.955696    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_storage-provisioner --format={{.ID}}
I0207 11:33:54.986181    3511 logs.go:284] 2 containers: [ec52b9ed7e21 0b85673627c7]
I0207 11:33:54.986277    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kubernetes-dashboard --format={{.ID}}
I0207 11:33:55.028152    3511 logs.go:284] 2 containers: [5631cbc21ffd f4b96dcbc9f2]
I0207 11:33:55.028259    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_controller_ingress --format={{.ID}}
I0207 11:33:55.058618    3511 logs.go:284] 1 containers: [8d50139a5311]
I0207 11:33:55.058647    3511 logs.go:123] Gathering logs for etcd [5cf1eff8b362] ...
I0207 11:33:55.058661    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 5cf1eff8b362"
I0207 11:33:55.133082    3511 logs.go:123] Gathering logs for kube-scheduler [ee7c2215045a] ...
I0207 11:33:55.133098    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 ee7c2215045a"
I0207 11:33:55.192711    3511 logs.go:123] Gathering logs for kube-proxy [407d81ee6871] ...
I0207 11:33:55.192726    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 407d81ee6871"
I0207 11:33:55.225356    3511 logs.go:123] Gathering logs for kube-controller-manager [885389110d12] ...
I0207 11:33:55.225371    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 885389110d12"
I0207 11:33:55.291142    3511 logs.go:123] Gathering logs for storage-provisioner [ec52b9ed7e21] ...
I0207 11:33:55.291162    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 ec52b9ed7e21"
I0207 11:33:55.330826    3511 logs.go:123] Gathering logs for storage-provisioner [0b85673627c7] ...
I0207 11:33:55.330845    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 0b85673627c7"
I0207 11:33:55.403990    3511 logs.go:123] Gathering logs for container status ...
I0207 11:33:55.404005    3511 ssh_runner.go:195] Run: /bin/bash -c "sudo `which crictl || echo crictl` ps -a || sudo docker ps -a"
I0207 11:33:55.590852    3511 logs.go:123] Gathering logs for dmesg ...
I0207 11:33:55.590868    3511 ssh_runner.go:195] Run: /bin/bash -c "sudo dmesg -PH -L=never --level warn,err,crit,alert,emerg | tail -n 400"
I0207 11:33:55.658552    3511 logs.go:123] Gathering logs for etcd [1eed8f4e884e] ...
I0207 11:33:55.658580    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 1eed8f4e884e"
I0207 11:33:55.833663    3511 logs.go:123] Gathering logs for coredns [b94028402ad3] ...
I0207 11:33:55.833681    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 b94028402ad3"
I0207 11:33:55.879995    3511 logs.go:123] Gathering logs for Docker ...
I0207 11:33:55.880034    3511 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u docker -u cri-docker -n 400"
I0207 11:33:55.940040    3511 logs.go:123] Gathering logs for describe nodes ...
I0207 11:33:55.940056    3511 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig"
W0207 11:33:56.047668    3511 logs.go:130] failed describe nodes: command: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig" /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig": Process exited with status 1
stdout:

stderr:
E0207 03:33:56.029472    7100 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:33:56.032459    7100 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:33:56.036589    7100 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:33:56.037933    7100 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:33:56.038592    7100 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
The connection to the server localhost:8443 was refused - did you specify the right host or port?
 output: 
** stderr ** 
E0207 03:33:56.029472    7100 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:33:56.032459    7100 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:33:56.036589    7100 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:33:56.037933    7100 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:33:56.038592    7100 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
The connection to the server localhost:8443 was refused - did you specify the right host or port?

** /stderr **
I0207 11:33:56.047682    3511 logs.go:123] Gathering logs for kube-apiserver [1b36eac9fea0] ...
I0207 11:33:56.047697    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 1b36eac9fea0"
I0207 11:33:56.130702    3511 logs.go:123] Gathering logs for kube-apiserver [39d1988a2fab] ...
I0207 11:33:56.130716    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 39d1988a2fab"
I0207 11:33:56.191946    3511 logs.go:123] Gathering logs for kube-scheduler [1d1e9abd9836] ...
I0207 11:33:56.191961    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 1d1e9abd9836"
I0207 11:33:56.254532    3511 logs.go:123] Gathering logs for kube-proxy [1db52edcdad4] ...
I0207 11:33:56.254548    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 1db52edcdad4"
I0207 11:33:56.289289    3511 logs.go:123] Gathering logs for kube-controller-manager [e1bb8361945e] ...
I0207 11:33:56.289303    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 e1bb8361945e"
I0207 11:33:56.380490    3511 logs.go:123] Gathering logs for kubernetes-dashboard [f4b96dcbc9f2] ...
I0207 11:33:56.380509    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 f4b96dcbc9f2"
I0207 11:33:56.461171    3511 logs.go:123] Gathering logs for kubelet ...
I0207 11:33:56.461189    3511 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u kubelet -n 400"
I0207 11:33:56.559232    3511 logs.go:123] Gathering logs for coredns [06fb15ac4a32] ...
I0207 11:33:56.559251    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 06fb15ac4a32"
I0207 11:33:56.644767    3511 logs.go:123] Gathering logs for kubernetes-dashboard [5631cbc21ffd] ...
I0207 11:33:56.644783    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 5631cbc21ffd"
I0207 11:33:56.679982    3511 logs.go:123] Gathering logs for controller_ingress [8d50139a5311] ...
I0207 11:33:56.679997    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 8d50139a5311"
I0207 11:33:59.247273    3511 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0207 11:33:59.279162    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-apiserver --format={{.ID}}
I0207 11:33:59.310089    3511 logs.go:284] 2 containers: [1b36eac9fea0 39d1988a2fab]
I0207 11:33:59.310164    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_etcd --format={{.ID}}
I0207 11:33:59.343239    3511 logs.go:284] 2 containers: [1eed8f4e884e 5cf1eff8b362]
I0207 11:33:59.343362    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_coredns --format={{.ID}}
I0207 11:33:59.403398    3511 logs.go:284] 2 containers: [06fb15ac4a32 b94028402ad3]
I0207 11:33:59.403541    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-scheduler --format={{.ID}}
I0207 11:33:59.454664    3511 logs.go:284] 2 containers: [1d1e9abd9836 ee7c2215045a]
I0207 11:33:59.454761    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-proxy --format={{.ID}}
I0207 11:33:59.500329    3511 logs.go:284] 2 containers: [1db52edcdad4 407d81ee6871]
I0207 11:33:59.500462    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-controller-manager --format={{.ID}}
I0207 11:33:59.533765    3511 logs.go:284] 2 containers: [e1bb8361945e 885389110d12]
I0207 11:33:59.533868    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kindnet --format={{.ID}}
I0207 11:33:59.561267    3511 logs.go:284] 0 containers: []
W0207 11:33:59.561283    3511 logs.go:286] No container was found matching "kindnet"
I0207 11:33:59.561378    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_storage-provisioner --format={{.ID}}
I0207 11:33:59.589034    3511 logs.go:284] 2 containers: [ec52b9ed7e21 0b85673627c7]
I0207 11:33:59.589130    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kubernetes-dashboard --format={{.ID}}
I0207 11:33:59.619195    3511 logs.go:284] 2 containers: [5631cbc21ffd f4b96dcbc9f2]
I0207 11:33:59.619294    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_controller_ingress --format={{.ID}}
I0207 11:33:59.649287    3511 logs.go:284] 1 containers: [8d50139a5311]
I0207 11:33:59.649322    3511 logs.go:123] Gathering logs for kube-apiserver [39d1988a2fab] ...
I0207 11:33:59.649335    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 39d1988a2fab"
I0207 11:33:59.701319    3511 logs.go:123] Gathering logs for etcd [5cf1eff8b362] ...
I0207 11:33:59.701345    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 5cf1eff8b362"
I0207 11:33:59.768582    3511 logs.go:123] Gathering logs for Docker ...
I0207 11:33:59.768597    3511 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u docker -u cri-docker -n 400"
I0207 11:33:59.826501    3511 logs.go:123] Gathering logs for dmesg ...
I0207 11:33:59.826526    3511 ssh_runner.go:195] Run: /bin/bash -c "sudo dmesg -PH -L=never --level warn,err,crit,alert,emerg | tail -n 400"
I0207 11:33:59.889940    3511 logs.go:123] Gathering logs for kube-proxy [407d81ee6871] ...
I0207 11:33:59.889955    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 407d81ee6871"
I0207 11:33:59.923747    3511 logs.go:123] Gathering logs for kubernetes-dashboard [5631cbc21ffd] ...
I0207 11:33:59.923766    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 5631cbc21ffd"
I0207 11:33:59.969210    3511 logs.go:123] Gathering logs for controller_ingress [8d50139a5311] ...
I0207 11:33:59.969226    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 8d50139a5311"
I0207 11:34:00.036304    3511 logs.go:123] Gathering logs for kube-controller-manager [885389110d12] ...
I0207 11:34:00.036322    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 885389110d12"
I0207 11:34:00.096626    3511 logs.go:123] Gathering logs for storage-provisioner [0b85673627c7] ...
I0207 11:34:00.096651    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 0b85673627c7"
I0207 11:34:00.188246    3511 logs.go:123] Gathering logs for describe nodes ...
I0207 11:34:00.188263    3511 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig"
W0207 11:34:00.341335    3511 logs.go:130] failed describe nodes: command: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig" /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig": Process exited with status 1
stdout:

stderr:
E0207 03:34:00.329101    7346 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:34:00.330262    7346 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:34:00.330949    7346 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:34:00.332589    7346 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:34:00.333422    7346 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
The connection to the server localhost:8443 was refused - did you specify the right host or port?
 output: 
** stderr ** 
E0207 03:34:00.329101    7346 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:34:00.330262    7346 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:34:00.330949    7346 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:34:00.332589    7346 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:34:00.333422    7346 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
The connection to the server localhost:8443 was refused - did you specify the right host or port?

** /stderr **
I0207 11:34:00.341350    3511 logs.go:123] Gathering logs for coredns [b94028402ad3] ...
I0207 11:34:00.341366    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 b94028402ad3"
I0207 11:34:00.409782    3511 logs.go:123] Gathering logs for kube-scheduler [1d1e9abd9836] ...
I0207 11:34:00.409801    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 1d1e9abd9836"
I0207 11:34:00.488184    3511 logs.go:123] Gathering logs for kube-scheduler [ee7c2215045a] ...
I0207 11:34:00.488201    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 ee7c2215045a"
I0207 11:34:00.543903    3511 logs.go:123] Gathering logs for kube-proxy [1db52edcdad4] ...
I0207 11:34:00.543917    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 1db52edcdad4"
I0207 11:34:00.582542    3511 logs.go:123] Gathering logs for kube-controller-manager [e1bb8361945e] ...
I0207 11:34:00.582570    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 e1bb8361945e"
I0207 11:34:00.670406    3511 logs.go:123] Gathering logs for container status ...
I0207 11:34:00.670423    3511 ssh_runner.go:195] Run: /bin/bash -c "sudo `which crictl || echo crictl` ps -a || sudo docker ps -a"
I0207 11:34:00.829179    3511 logs.go:123] Gathering logs for kubelet ...
I0207 11:34:00.829202    3511 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u kubelet -n 400"
I0207 11:34:00.940776    3511 logs.go:123] Gathering logs for kube-apiserver [1b36eac9fea0] ...
I0207 11:34:00.940791    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 1b36eac9fea0"
I0207 11:34:01.000881    3511 logs.go:123] Gathering logs for etcd [1eed8f4e884e] ...
I0207 11:34:01.000896    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 1eed8f4e884e"
I0207 11:34:01.140451    3511 logs.go:123] Gathering logs for coredns [06fb15ac4a32] ...
I0207 11:34:01.140471    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 06fb15ac4a32"
I0207 11:34:01.229742    3511 logs.go:123] Gathering logs for storage-provisioner [ec52b9ed7e21] ...
I0207 11:34:01.229770    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 ec52b9ed7e21"
I0207 11:34:01.281047    3511 logs.go:123] Gathering logs for kubernetes-dashboard [f4b96dcbc9f2] ...
I0207 11:34:01.281093    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 f4b96dcbc9f2"
I0207 11:34:03.862650    3511 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0207 11:34:03.918781    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-apiserver --format={{.ID}}
I0207 11:34:03.954529    3511 logs.go:284] 2 containers: [1b36eac9fea0 39d1988a2fab]
I0207 11:34:03.954632    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_etcd --format={{.ID}}
I0207 11:34:03.983214    3511 logs.go:284] 2 containers: [1eed8f4e884e 5cf1eff8b362]
I0207 11:34:03.983348    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_coredns --format={{.ID}}
I0207 11:34:04.013408    3511 logs.go:284] 2 containers: [06fb15ac4a32 b94028402ad3]
I0207 11:34:04.013513    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-scheduler --format={{.ID}}
I0207 11:34:04.048321    3511 logs.go:284] 2 containers: [1d1e9abd9836 ee7c2215045a]
I0207 11:34:04.048418    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-proxy --format={{.ID}}
I0207 11:34:04.077912    3511 logs.go:284] 2 containers: [1db52edcdad4 407d81ee6871]
I0207 11:34:04.078061    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-controller-manager --format={{.ID}}
I0207 11:34:04.114632    3511 logs.go:284] 2 containers: [e1bb8361945e 885389110d12]
I0207 11:34:04.114719    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kindnet --format={{.ID}}
I0207 11:34:04.142668    3511 logs.go:284] 0 containers: []
W0207 11:34:04.142704    3511 logs.go:286] No container was found matching "kindnet"
I0207 11:34:04.142813    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kubernetes-dashboard --format={{.ID}}
I0207 11:34:04.201009    3511 logs.go:284] 2 containers: [5631cbc21ffd f4b96dcbc9f2]
I0207 11:34:04.201103    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_controller_ingress --format={{.ID}}
I0207 11:34:04.261916    3511 logs.go:284] 1 containers: [8d50139a5311]
I0207 11:34:04.262049    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_storage-provisioner --format={{.ID}}
I0207 11:34:04.315508    3511 logs.go:284] 2 containers: [ec52b9ed7e21 0b85673627c7]
I0207 11:34:04.315555    3511 logs.go:123] Gathering logs for kube-controller-manager [e1bb8361945e] ...
I0207 11:34:04.315571    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 e1bb8361945e"
I0207 11:34:04.448837    3511 logs.go:123] Gathering logs for kubernetes-dashboard [5631cbc21ffd] ...
I0207 11:34:04.448853    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 5631cbc21ffd"
I0207 11:34:04.489456    3511 logs.go:123] Gathering logs for controller_ingress [8d50139a5311] ...
I0207 11:34:04.489472    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 8d50139a5311"
I0207 11:34:04.547765    3511 logs.go:123] Gathering logs for storage-provisioner [0b85673627c7] ...
I0207 11:34:04.547779    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 0b85673627c7"
I0207 11:34:04.624239    3511 logs.go:123] Gathering logs for dmesg ...
I0207 11:34:04.624254    3511 ssh_runner.go:195] Run: /bin/bash -c "sudo dmesg -PH -L=never --level warn,err,crit,alert,emerg | tail -n 400"
I0207 11:34:04.680727    3511 logs.go:123] Gathering logs for describe nodes ...
I0207 11:34:04.680742    3511 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig"
W0207 11:34:04.788856    3511 logs.go:130] failed describe nodes: command: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig" /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig": Process exited with status 1
stdout:

stderr:
E0207 03:34:04.776736    7615 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:34:04.777652    7615 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:34:04.779658    7615 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:34:04.780655    7615 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:34:04.781227    7615 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
The connection to the server localhost:8443 was refused - did you specify the right host or port?
 output: 
** stderr ** 
E0207 03:34:04.776736    7615 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:34:04.777652    7615 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:34:04.779658    7615 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:34:04.780655    7615 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:34:04.781227    7615 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
The connection to the server localhost:8443 was refused - did you specify the right host or port?

** /stderr **
I0207 11:34:04.788873    3511 logs.go:123] Gathering logs for kube-apiserver [1b36eac9fea0] ...
I0207 11:34:04.788887    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 1b36eac9fea0"
I0207 11:34:04.842916    3511 logs.go:123] Gathering logs for coredns [06fb15ac4a32] ...
I0207 11:34:04.842931    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 06fb15ac4a32"
I0207 11:34:04.897796    3511 logs.go:123] Gathering logs for kube-proxy [407d81ee6871] ...
I0207 11:34:04.897812    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 407d81ee6871"
I0207 11:34:04.933422    3511 logs.go:123] Gathering logs for storage-provisioner [ec52b9ed7e21] ...
I0207 11:34:04.933437    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 ec52b9ed7e21"
I0207 11:34:04.967656    3511 logs.go:123] Gathering logs for kube-apiserver [39d1988a2fab] ...
I0207 11:34:04.967671    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 39d1988a2fab"
I0207 11:34:05.028734    3511 logs.go:123] Gathering logs for etcd [1eed8f4e884e] ...
I0207 11:34:05.028754    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 1eed8f4e884e"
I0207 11:34:05.235920    3511 logs.go:123] Gathering logs for kube-scheduler [1d1e9abd9836] ...
I0207 11:34:05.235938    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 1d1e9abd9836"
I0207 11:34:05.323910    3511 logs.go:123] Gathering logs for kube-proxy [1db52edcdad4] ...
I0207 11:34:05.323928    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 1db52edcdad4"
I0207 11:34:05.372765    3511 logs.go:123] Gathering logs for kube-controller-manager [885389110d12] ...
I0207 11:34:05.372788    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 885389110d12"
I0207 11:34:05.470332    3511 logs.go:123] Gathering logs for kubernetes-dashboard [f4b96dcbc9f2] ...
I0207 11:34:05.470349    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 f4b96dcbc9f2"
I0207 11:34:05.545919    3511 logs.go:123] Gathering logs for Docker ...
I0207 11:34:05.545933    3511 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u docker -u cri-docker -n 400"
I0207 11:34:05.604401    3511 logs.go:123] Gathering logs for container status ...
I0207 11:34:05.604418    3511 ssh_runner.go:195] Run: /bin/bash -c "sudo `which crictl || echo crictl` ps -a || sudo docker ps -a"
I0207 11:34:05.787207    3511 logs.go:123] Gathering logs for kubelet ...
I0207 11:34:05.787226    3511 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u kubelet -n 400"
I0207 11:34:05.918927    3511 logs.go:123] Gathering logs for etcd [5cf1eff8b362] ...
I0207 11:34:05.918943    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 5cf1eff8b362"
I0207 11:34:05.992704    3511 logs.go:123] Gathering logs for coredns [b94028402ad3] ...
I0207 11:34:05.992722    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 b94028402ad3"
I0207 11:34:06.048566    3511 logs.go:123] Gathering logs for kube-scheduler [ee7c2215045a] ...
I0207 11:34:06.048581    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 ee7c2215045a"
I0207 11:34:08.619897    3511 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0207 11:34:08.640359    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-apiserver --format={{.ID}}
I0207 11:34:08.673397    3511 logs.go:284] 2 containers: [1b36eac9fea0 39d1988a2fab]
I0207 11:34:08.673489    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_etcd --format={{.ID}}
I0207 11:34:08.709913    3511 logs.go:284] 2 containers: [1eed8f4e884e 5cf1eff8b362]
I0207 11:34:08.710024    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_coredns --format={{.ID}}
I0207 11:34:08.746286    3511 logs.go:284] 2 containers: [06fb15ac4a32 b94028402ad3]
I0207 11:34:08.746389    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-scheduler --format={{.ID}}
I0207 11:34:08.777821    3511 logs.go:284] 2 containers: [1d1e9abd9836 ee7c2215045a]
I0207 11:34:08.777918    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-proxy --format={{.ID}}
I0207 11:34:08.806994    3511 logs.go:284] 2 containers: [1db52edcdad4 407d81ee6871]
I0207 11:34:08.807090    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-controller-manager --format={{.ID}}
I0207 11:34:08.837595    3511 logs.go:284] 2 containers: [e1bb8361945e 885389110d12]
I0207 11:34:08.837692    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kindnet --format={{.ID}}
I0207 11:34:08.875435    3511 logs.go:284] 0 containers: []
W0207 11:34:08.875457    3511 logs.go:286] No container was found matching "kindnet"
I0207 11:34:08.875549    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kubernetes-dashboard --format={{.ID}}
I0207 11:34:08.909753    3511 logs.go:284] 2 containers: [5631cbc21ffd f4b96dcbc9f2]
I0207 11:34:08.909844    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_controller_ingress --format={{.ID}}
I0207 11:34:08.944833    3511 logs.go:284] 1 containers: [8d50139a5311]
I0207 11:34:08.944978    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_storage-provisioner --format={{.ID}}
I0207 11:34:08.978026    3511 logs.go:284] 2 containers: [ec52b9ed7e21 0b85673627c7]
I0207 11:34:08.978053    3511 logs.go:123] Gathering logs for storage-provisioner [ec52b9ed7e21] ...
I0207 11:34:08.978065    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 ec52b9ed7e21"
I0207 11:34:09.009569    3511 logs.go:123] Gathering logs for storage-provisioner [0b85673627c7] ...
I0207 11:34:09.009584    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 0b85673627c7"
I0207 11:34:09.090871    3511 logs.go:123] Gathering logs for describe nodes ...
I0207 11:34:09.090886    3511 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig"
W0207 11:34:09.218828    3511 logs.go:130] failed describe nodes: command: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig" /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig": Process exited with status 1
stdout:

stderr:
E0207 03:34:09.205120    7878 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:34:09.206141    7878 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:34:09.208337    7878 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:34:09.209402    7878 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:34:09.211380    7878 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
The connection to the server localhost:8443 was refused - did you specify the right host or port?
 output: 
** stderr ** 
E0207 03:34:09.205120    7878 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:34:09.206141    7878 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:34:09.208337    7878 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:34:09.209402    7878 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:34:09.211380    7878 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
The connection to the server localhost:8443 was refused - did you specify the right host or port?

** /stderr **
I0207 11:34:09.218843    3511 logs.go:123] Gathering logs for etcd [1eed8f4e884e] ...
I0207 11:34:09.218860    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 1eed8f4e884e"
I0207 11:34:09.377369    3511 logs.go:123] Gathering logs for etcd [5cf1eff8b362] ...
I0207 11:34:09.377385    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 5cf1eff8b362"
I0207 11:34:09.459341    3511 logs.go:123] Gathering logs for coredns [b94028402ad3] ...
I0207 11:34:09.459358    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 b94028402ad3"
I0207 11:34:09.525610    3511 logs.go:123] Gathering logs for kube-controller-manager [e1bb8361945e] ...
I0207 11:34:09.525627    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 e1bb8361945e"
I0207 11:34:09.667469    3511 logs.go:123] Gathering logs for container status ...
I0207 11:34:09.667486    3511 ssh_runner.go:195] Run: /bin/bash -c "sudo `which crictl || echo crictl` ps -a || sudo docker ps -a"
I0207 11:34:09.829608    3511 logs.go:123] Gathering logs for dmesg ...
I0207 11:34:09.829624    3511 ssh_runner.go:195] Run: /bin/bash -c "sudo dmesg -PH -L=never --level warn,err,crit,alert,emerg | tail -n 400"
I0207 11:34:09.893253    3511 logs.go:123] Gathering logs for coredns [06fb15ac4a32] ...
I0207 11:34:09.893268    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 06fb15ac4a32"
I0207 11:34:09.939292    3511 logs.go:123] Gathering logs for kube-scheduler [1d1e9abd9836] ...
I0207 11:34:09.939307    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 1d1e9abd9836"
I0207 11:34:10.003762    3511 logs.go:123] Gathering logs for kube-scheduler [ee7c2215045a] ...
I0207 11:34:10.003791    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 ee7c2215045a"
I0207 11:34:10.053996    3511 logs.go:123] Gathering logs for kubernetes-dashboard [f4b96dcbc9f2] ...
I0207 11:34:10.054026    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 f4b96dcbc9f2"
I0207 11:34:10.120199    3511 logs.go:123] Gathering logs for Docker ...
I0207 11:34:10.120215    3511 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u docker -u cri-docker -n 400"
I0207 11:34:10.181652    3511 logs.go:123] Gathering logs for kubelet ...
I0207 11:34:10.181674    3511 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u kubelet -n 400"
I0207 11:34:10.272079    3511 logs.go:123] Gathering logs for kube-apiserver [39d1988a2fab] ...
I0207 11:34:10.272105    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 39d1988a2fab"
I0207 11:34:10.332834    3511 logs.go:123] Gathering logs for kube-controller-manager [885389110d12] ...
I0207 11:34:10.332849    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 885389110d12"
I0207 11:34:10.400660    3511 logs.go:123] Gathering logs for kubernetes-dashboard [5631cbc21ffd] ...
I0207 11:34:10.400675    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 5631cbc21ffd"
I0207 11:34:10.440992    3511 logs.go:123] Gathering logs for controller_ingress [8d50139a5311] ...
I0207 11:34:10.441007    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 8d50139a5311"
I0207 11:34:10.533717    3511 logs.go:123] Gathering logs for kube-apiserver [1b36eac9fea0] ...
I0207 11:34:10.533742    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 1b36eac9fea0"
I0207 11:34:10.628094    3511 logs.go:123] Gathering logs for kube-proxy [1db52edcdad4] ...
I0207 11:34:10.628111    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 1db52edcdad4"
I0207 11:34:10.675309    3511 logs.go:123] Gathering logs for kube-proxy [407d81ee6871] ...
I0207 11:34:10.675326    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 407d81ee6871"
I0207 11:34:13.215050    3511 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0207 11:34:13.241282    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-apiserver --format={{.ID}}
I0207 11:34:13.288264    3511 logs.go:284] 2 containers: [1b36eac9fea0 39d1988a2fab]
I0207 11:34:13.288361    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_etcd --format={{.ID}}
I0207 11:34:13.330619    3511 logs.go:284] 2 containers: [1eed8f4e884e 5cf1eff8b362]
I0207 11:34:13.330719    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_coredns --format={{.ID}}
I0207 11:34:13.383307    3511 logs.go:284] 2 containers: [06fb15ac4a32 b94028402ad3]
I0207 11:34:13.383415    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-scheduler --format={{.ID}}
I0207 11:34:13.433911    3511 logs.go:284] 2 containers: [1d1e9abd9836 ee7c2215045a]
I0207 11:34:13.434038    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-proxy --format={{.ID}}
I0207 11:34:13.474119    3511 logs.go:284] 2 containers: [1db52edcdad4 407d81ee6871]
I0207 11:34:13.474243    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-controller-manager --format={{.ID}}
I0207 11:34:13.509735    3511 logs.go:284] 2 containers: [e1bb8361945e 885389110d12]
I0207 11:34:13.509845    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kindnet --format={{.ID}}
I0207 11:34:13.559533    3511 logs.go:284] 0 containers: []
W0207 11:34:13.559617    3511 logs.go:286] No container was found matching "kindnet"
I0207 11:34:13.559709    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kubernetes-dashboard --format={{.ID}}
I0207 11:34:13.609477    3511 logs.go:284] 2 containers: [5631cbc21ffd f4b96dcbc9f2]
I0207 11:34:13.609576    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_controller_ingress --format={{.ID}}
I0207 11:34:13.642159    3511 logs.go:284] 1 containers: [8d50139a5311]
I0207 11:34:13.642273    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_storage-provisioner --format={{.ID}}
I0207 11:34:13.676737    3511 logs.go:284] 2 containers: [ec52b9ed7e21 0b85673627c7]
I0207 11:34:13.676768    3511 logs.go:123] Gathering logs for kube-apiserver [39d1988a2fab] ...
I0207 11:34:13.676809    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 39d1988a2fab"
I0207 11:34:13.729505    3511 logs.go:123] Gathering logs for coredns [06fb15ac4a32] ...
I0207 11:34:13.729521    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 06fb15ac4a32"
I0207 11:34:13.780496    3511 logs.go:123] Gathering logs for container status ...
I0207 11:34:13.780514    3511 ssh_runner.go:195] Run: /bin/bash -c "sudo `which crictl || echo crictl` ps -a || sudo docker ps -a"
I0207 11:34:13.946089    3511 logs.go:123] Gathering logs for kubelet ...
I0207 11:34:13.946104    3511 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u kubelet -n 400"
I0207 11:34:14.023147    3511 logs.go:123] Gathering logs for kube-controller-manager [e1bb8361945e] ...
I0207 11:34:14.023163    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 e1bb8361945e"
I0207 11:34:14.135916    3511 logs.go:123] Gathering logs for storage-provisioner [ec52b9ed7e21] ...
I0207 11:34:14.135931    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 ec52b9ed7e21"
I0207 11:34:14.205364    3511 logs.go:123] Gathering logs for storage-provisioner [0b85673627c7] ...
I0207 11:34:14.205389    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 0b85673627c7"
I0207 11:34:14.307329    3511 logs.go:123] Gathering logs for Docker ...
I0207 11:34:14.307346    3511 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u docker -u cri-docker -n 400"
I0207 11:34:14.395247    3511 logs.go:123] Gathering logs for describe nodes ...
I0207 11:34:14.395263    3511 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig"
W0207 11:34:14.516037    3511 logs.go:130] failed describe nodes: command: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig" /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig": Process exited with status 1
stdout:

stderr:
E0207 03:34:14.500081    8244 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:34:14.501725    8244 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:34:14.502180    8244 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:34:14.505120    8244 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:34:14.505468    8244 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
The connection to the server localhost:8443 was refused - did you specify the right host or port?
 output: 
** stderr ** 
E0207 03:34:14.500081    8244 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:34:14.501725    8244 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:34:14.502180    8244 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:34:14.505120    8244 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:34:14.505468    8244 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
The connection to the server localhost:8443 was refused - did you specify the right host or port?

** /stderr **
I0207 11:34:14.516052    3511 logs.go:123] Gathering logs for kube-apiserver [1b36eac9fea0] ...
I0207 11:34:14.516066    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 1b36eac9fea0"
I0207 11:34:14.595931    3511 logs.go:123] Gathering logs for kube-controller-manager [885389110d12] ...
I0207 11:34:14.595946    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 885389110d12"
I0207 11:34:14.666096    3511 logs.go:123] Gathering logs for controller_ingress [8d50139a5311] ...
I0207 11:34:14.666117    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 8d50139a5311"
I0207 11:34:14.734901    3511 logs.go:123] Gathering logs for kube-scheduler [ee7c2215045a] ...
I0207 11:34:14.734918    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 ee7c2215045a"
I0207 11:34:14.789474    3511 logs.go:123] Gathering logs for kube-proxy [1db52edcdad4] ...
I0207 11:34:14.789491    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 1db52edcdad4"
I0207 11:34:14.821925    3511 logs.go:123] Gathering logs for kube-proxy [407d81ee6871] ...
I0207 11:34:14.821938    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 407d81ee6871"
I0207 11:34:14.858565    3511 logs.go:123] Gathering logs for dmesg ...
I0207 11:34:14.858584    3511 ssh_runner.go:195] Run: /bin/bash -c "sudo dmesg -PH -L=never --level warn,err,crit,alert,emerg | tail -n 400"
I0207 11:34:14.928867    3511 logs.go:123] Gathering logs for etcd [1eed8f4e884e] ...
I0207 11:34:14.928882    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 1eed8f4e884e"
I0207 11:34:15.073643    3511 logs.go:123] Gathering logs for etcd [5cf1eff8b362] ...
I0207 11:34:15.073658    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 5cf1eff8b362"
I0207 11:34:15.138234    3511 logs.go:123] Gathering logs for coredns [b94028402ad3] ...
I0207 11:34:15.138249    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 b94028402ad3"
I0207 11:34:15.199281    3511 logs.go:123] Gathering logs for kube-scheduler [1d1e9abd9836] ...
I0207 11:34:15.199298    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 1d1e9abd9836"
I0207 11:34:15.270966    3511 logs.go:123] Gathering logs for kubernetes-dashboard [5631cbc21ffd] ...
I0207 11:34:15.270984    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 5631cbc21ffd"
I0207 11:34:15.326679    3511 logs.go:123] Gathering logs for kubernetes-dashboard [f4b96dcbc9f2] ...
I0207 11:34:15.326698    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 f4b96dcbc9f2"
I0207 11:34:17.938973    3511 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0207 11:34:17.964393    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-apiserver --format={{.ID}}
I0207 11:34:18.007267    3511 logs.go:284] 2 containers: [1b36eac9fea0 39d1988a2fab]
I0207 11:34:18.007379    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_etcd --format={{.ID}}
I0207 11:34:18.050360    3511 logs.go:284] 2 containers: [1eed8f4e884e 5cf1eff8b362]
I0207 11:34:18.050506    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_coredns --format={{.ID}}
I0207 11:34:18.090909    3511 logs.go:284] 2 containers: [06fb15ac4a32 b94028402ad3]
I0207 11:34:18.091063    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-scheduler --format={{.ID}}
I0207 11:34:18.141989    3511 logs.go:284] 2 containers: [1d1e9abd9836 ee7c2215045a]
I0207 11:34:18.142095    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-proxy --format={{.ID}}
I0207 11:34:18.178044    3511 logs.go:284] 2 containers: [1db52edcdad4 407d81ee6871]
I0207 11:34:18.178142    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-controller-manager --format={{.ID}}
I0207 11:34:18.206272    3511 logs.go:284] 2 containers: [e1bb8361945e 885389110d12]
I0207 11:34:18.206357    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kindnet --format={{.ID}}
I0207 11:34:18.232174    3511 logs.go:284] 0 containers: []
W0207 11:34:18.232190    3511 logs.go:286] No container was found matching "kindnet"
I0207 11:34:18.232267    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kubernetes-dashboard --format={{.ID}}
I0207 11:34:18.263252    3511 logs.go:284] 2 containers: [5631cbc21ffd f4b96dcbc9f2]
I0207 11:34:18.263343    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_controller_ingress --format={{.ID}}
I0207 11:34:18.294003    3511 logs.go:284] 1 containers: [8d50139a5311]
I0207 11:34:18.294092    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_storage-provisioner --format={{.ID}}
I0207 11:34:18.334566    3511 logs.go:284] 2 containers: [ec52b9ed7e21 0b85673627c7]
I0207 11:34:18.334593    3511 logs.go:123] Gathering logs for coredns [b94028402ad3] ...
I0207 11:34:18.334603    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 b94028402ad3"
I0207 11:34:18.385193    3511 logs.go:123] Gathering logs for kube-scheduler [1d1e9abd9836] ...
I0207 11:34:18.385209    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 1d1e9abd9836"
I0207 11:34:18.460205    3511 logs.go:123] Gathering logs for kube-controller-manager [e1bb8361945e] ...
I0207 11:34:18.460222    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 e1bb8361945e"
I0207 11:34:18.565934    3511 logs.go:123] Gathering logs for kubelet ...
I0207 11:34:18.565950    3511 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u kubelet -n 400"
I0207 11:34:18.655780    3511 logs.go:123] Gathering logs for etcd [1eed8f4e884e] ...
I0207 11:34:18.655797    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 1eed8f4e884e"
I0207 11:34:18.824486    3511 logs.go:123] Gathering logs for kubernetes-dashboard [5631cbc21ffd] ...
I0207 11:34:18.824503    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 5631cbc21ffd"
I0207 11:34:18.859250    3511 logs.go:123] Gathering logs for kubernetes-dashboard [f4b96dcbc9f2] ...
I0207 11:34:18.859264    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 f4b96dcbc9f2"
I0207 11:34:18.971745    3511 logs.go:123] Gathering logs for controller_ingress [8d50139a5311] ...
I0207 11:34:18.971762    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 8d50139a5311"
I0207 11:34:19.043095    3511 logs.go:123] Gathering logs for storage-provisioner [ec52b9ed7e21] ...
I0207 11:34:19.043111    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 ec52b9ed7e21"
I0207 11:34:19.078243    3511 logs.go:123] Gathering logs for storage-provisioner [0b85673627c7] ...
I0207 11:34:19.078271    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 0b85673627c7"
I0207 11:34:19.172178    3511 logs.go:123] Gathering logs for Docker ...
I0207 11:34:19.172193    3511 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u docker -u cri-docker -n 400"
I0207 11:34:19.229822    3511 logs.go:123] Gathering logs for dmesg ...
I0207 11:34:19.229847    3511 ssh_runner.go:195] Run: /bin/bash -c "sudo dmesg -PH -L=never --level warn,err,crit,alert,emerg | tail -n 400"
I0207 11:34:19.289920    3511 logs.go:123] Gathering logs for kube-apiserver [1b36eac9fea0] ...
I0207 11:34:19.289935    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 1b36eac9fea0"
I0207 11:34:19.364743    3511 logs.go:123] Gathering logs for coredns [06fb15ac4a32] ...
I0207 11:34:19.364766    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 06fb15ac4a32"
I0207 11:34:19.418942    3511 logs.go:123] Gathering logs for kube-proxy [1db52edcdad4] ...
I0207 11:34:19.418957    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 1db52edcdad4"
I0207 11:34:19.452943    3511 logs.go:123] Gathering logs for kube-controller-manager [885389110d12] ...
I0207 11:34:19.452958    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 885389110d12"
I0207 11:34:19.515919    3511 logs.go:123] Gathering logs for container status ...
I0207 11:34:19.515934    3511 ssh_runner.go:195] Run: /bin/bash -c "sudo `which crictl || echo crictl` ps -a || sudo docker ps -a"
I0207 11:34:19.702529    3511 logs.go:123] Gathering logs for describe nodes ...
I0207 11:34:19.702554    3511 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig"
W0207 11:34:19.823627    3511 logs.go:130] failed describe nodes: command: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig" /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig": Process exited with status 1
stdout:

stderr:
E0207 03:34:19.807833    8607 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:34:19.809357    8607 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:34:19.811213    8607 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:34:19.814079    8607 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:34:19.816348    8607 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
The connection to the server localhost:8443 was refused - did you specify the right host or port?
 output: 
** stderr ** 
E0207 03:34:19.807833    8607 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:34:19.809357    8607 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:34:19.811213    8607 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:34:19.814079    8607 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:34:19.816348    8607 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
The connection to the server localhost:8443 was refused - did you specify the right host or port?

** /stderr **
I0207 11:34:19.823645    3511 logs.go:123] Gathering logs for kube-apiserver [39d1988a2fab] ...
I0207 11:34:19.823662    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 39d1988a2fab"
I0207 11:34:19.873323    3511 logs.go:123] Gathering logs for etcd [5cf1eff8b362] ...
I0207 11:34:19.873342    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 5cf1eff8b362"
I0207 11:34:19.945623    3511 logs.go:123] Gathering logs for kube-scheduler [ee7c2215045a] ...
I0207 11:34:19.945648    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 ee7c2215045a"
I0207 11:34:20.036441    3511 logs.go:123] Gathering logs for kube-proxy [407d81ee6871] ...
I0207 11:34:20.036460    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 407d81ee6871"
I0207 11:34:22.575931    3511 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0207 11:34:22.597853    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-apiserver --format={{.ID}}
I0207 11:34:22.625723    3511 logs.go:284] 2 containers: [1b36eac9fea0 39d1988a2fab]
I0207 11:34:22.625823    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_etcd --format={{.ID}}
I0207 11:34:22.657996    3511 logs.go:284] 2 containers: [1eed8f4e884e 5cf1eff8b362]
I0207 11:34:22.658086    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_coredns --format={{.ID}}
I0207 11:34:22.687088    3511 logs.go:284] 2 containers: [06fb15ac4a32 b94028402ad3]
I0207 11:34:22.687246    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-scheduler --format={{.ID}}
I0207 11:34:22.721262    3511 logs.go:284] 2 containers: [1d1e9abd9836 ee7c2215045a]
I0207 11:34:22.721403    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-proxy --format={{.ID}}
I0207 11:34:22.753171    3511 logs.go:284] 2 containers: [1db52edcdad4 407d81ee6871]
I0207 11:34:22.753289    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-controller-manager --format={{.ID}}
I0207 11:34:22.788291    3511 logs.go:284] 2 containers: [e1bb8361945e 885389110d12]
I0207 11:34:22.788392    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kindnet --format={{.ID}}
I0207 11:34:22.820449    3511 logs.go:284] 0 containers: []
W0207 11:34:22.820465    3511 logs.go:286] No container was found matching "kindnet"
I0207 11:34:22.820570    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kubernetes-dashboard --format={{.ID}}
I0207 11:34:22.851679    3511 logs.go:284] 2 containers: [5631cbc21ffd f4b96dcbc9f2]
I0207 11:34:22.851785    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_controller_ingress --format={{.ID}}
I0207 11:34:22.893293    3511 logs.go:284] 1 containers: [8d50139a5311]
I0207 11:34:22.893419    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_storage-provisioner --format={{.ID}}
I0207 11:34:22.931702    3511 logs.go:284] 2 containers: [ec52b9ed7e21 0b85673627c7]
I0207 11:34:22.931729    3511 logs.go:123] Gathering logs for kube-apiserver [1b36eac9fea0] ...
I0207 11:34:22.931740    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 1b36eac9fea0"
I0207 11:34:23.000065    3511 logs.go:123] Gathering logs for kubernetes-dashboard [f4b96dcbc9f2] ...
I0207 11:34:23.000082    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 f4b96dcbc9f2"
I0207 11:34:23.100052    3511 logs.go:123] Gathering logs for kubelet ...
I0207 11:34:23.100070    3511 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u kubelet -n 400"
I0207 11:34:23.214680    3511 logs.go:123] Gathering logs for kube-apiserver [39d1988a2fab] ...
I0207 11:34:23.214695    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 39d1988a2fab"
I0207 11:34:23.278711    3511 logs.go:123] Gathering logs for kube-proxy [407d81ee6871] ...
I0207 11:34:23.278727    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 407d81ee6871"
I0207 11:34:23.312427    3511 logs.go:123] Gathering logs for Docker ...
I0207 11:34:23.312442    3511 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u docker -u cri-docker -n 400"
I0207 11:34:23.374538    3511 logs.go:123] Gathering logs for container status ...
I0207 11:34:23.374556    3511 ssh_runner.go:195] Run: /bin/bash -c "sudo `which crictl || echo crictl` ps -a || sudo docker ps -a"
I0207 11:34:23.530839    3511 logs.go:123] Gathering logs for controller_ingress [8d50139a5311] ...
I0207 11:34:23.530858    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 8d50139a5311"
I0207 11:34:23.623909    3511 logs.go:123] Gathering logs for dmesg ...
I0207 11:34:23.623928    3511 ssh_runner.go:195] Run: /bin/bash -c "sudo dmesg -PH -L=never --level warn,err,crit,alert,emerg | tail -n 400"
I0207 11:34:23.686794    3511 logs.go:123] Gathering logs for describe nodes ...
I0207 11:34:23.686808    3511 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig"
W0207 11:34:23.795336    3511 logs.go:130] failed describe nodes: command: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig" /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig": Process exited with status 1
stdout:

stderr:
E0207 03:34:23.781169    8841 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:34:23.781938    8841 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:34:23.783692    8841 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:34:23.784474    8841 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:34:23.788128    8841 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
The connection to the server localhost:8443 was refused - did you specify the right host or port?
 output: 
** stderr ** 
E0207 03:34:23.781169    8841 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:34:23.781938    8841 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:34:23.783692    8841 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:34:23.784474    8841 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:34:23.788128    8841 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
The connection to the server localhost:8443 was refused - did you specify the right host or port?

** /stderr **
I0207 11:34:23.795350    3511 logs.go:123] Gathering logs for etcd [1eed8f4e884e] ...
I0207 11:34:23.795364    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 1eed8f4e884e"
I0207 11:34:23.986638    3511 logs.go:123] Gathering logs for coredns [06fb15ac4a32] ...
I0207 11:34:23.986660    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 06fb15ac4a32"
I0207 11:34:24.047728    3511 logs.go:123] Gathering logs for coredns [b94028402ad3] ...
I0207 11:34:24.047746    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 b94028402ad3"
I0207 11:34:24.122227    3511 logs.go:123] Gathering logs for kube-controller-manager [e1bb8361945e] ...
I0207 11:34:24.122243    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 e1bb8361945e"
I0207 11:34:24.208309    3511 logs.go:123] Gathering logs for kubernetes-dashboard [5631cbc21ffd] ...
I0207 11:34:24.208326    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 5631cbc21ffd"
I0207 11:34:24.240805    3511 logs.go:123] Gathering logs for storage-provisioner [0b85673627c7] ...
I0207 11:34:24.240820    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 0b85673627c7"
I0207 11:34:24.301718    3511 logs.go:123] Gathering logs for etcd [5cf1eff8b362] ...
I0207 11:34:24.301732    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 5cf1eff8b362"
I0207 11:34:24.367376    3511 logs.go:123] Gathering logs for kube-scheduler [1d1e9abd9836] ...
I0207 11:34:24.367391    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 1d1e9abd9836"
I0207 11:34:24.456841    3511 logs.go:123] Gathering logs for kube-scheduler [ee7c2215045a] ...
I0207 11:34:24.456863    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 ee7c2215045a"
I0207 11:34:24.511083    3511 logs.go:123] Gathering logs for kube-proxy [1db52edcdad4] ...
I0207 11:34:24.511098    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 1db52edcdad4"
I0207 11:34:24.545266    3511 logs.go:123] Gathering logs for kube-controller-manager [885389110d12] ...
I0207 11:34:24.545281    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 885389110d12"
I0207 11:34:24.618353    3511 logs.go:123] Gathering logs for storage-provisioner [ec52b9ed7e21] ...
I0207 11:34:24.618369    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 ec52b9ed7e21"
I0207 11:34:27.158301    3511 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0207 11:34:27.180753    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-apiserver --format={{.ID}}
I0207 11:34:27.211527    3511 logs.go:284] 2 containers: [1b36eac9fea0 39d1988a2fab]
I0207 11:34:27.211632    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_etcd --format={{.ID}}
I0207 11:34:27.239035    3511 logs.go:284] 2 containers: [1eed8f4e884e 5cf1eff8b362]
I0207 11:34:27.239127    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_coredns --format={{.ID}}
I0207 11:34:27.272111    3511 logs.go:284] 2 containers: [06fb15ac4a32 b94028402ad3]
I0207 11:34:27.272220    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-scheduler --format={{.ID}}
I0207 11:34:27.306861    3511 logs.go:284] 2 containers: [1d1e9abd9836 ee7c2215045a]
I0207 11:34:27.306953    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-proxy --format={{.ID}}
I0207 11:34:27.336329    3511 logs.go:284] 2 containers: [1db52edcdad4 407d81ee6871]
I0207 11:34:27.336436    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-controller-manager --format={{.ID}}
I0207 11:34:27.368531    3511 logs.go:284] 2 containers: [e1bb8361945e 885389110d12]
I0207 11:34:27.368624    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kindnet --format={{.ID}}
I0207 11:34:27.398367    3511 logs.go:284] 0 containers: []
W0207 11:34:27.398383    3511 logs.go:286] No container was found matching "kindnet"
I0207 11:34:27.398464    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kubernetes-dashboard --format={{.ID}}
I0207 11:34:27.437614    3511 logs.go:284] 2 containers: [5631cbc21ffd f4b96dcbc9f2]
I0207 11:34:27.437710    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_controller_ingress --format={{.ID}}
I0207 11:34:27.475150    3511 logs.go:284] 1 containers: [8d50139a5311]
I0207 11:34:27.475287    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_storage-provisioner --format={{.ID}}
I0207 11:34:27.512473    3511 logs.go:284] 2 containers: [ec52b9ed7e21 0b85673627c7]
I0207 11:34:27.512543    3511 logs.go:123] Gathering logs for container status ...
I0207 11:34:27.512557    3511 ssh_runner.go:195] Run: /bin/bash -c "sudo `which crictl || echo crictl` ps -a || sudo docker ps -a"
I0207 11:34:27.682712    3511 logs.go:123] Gathering logs for kube-apiserver [1b36eac9fea0] ...
I0207 11:34:27.682739    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 1b36eac9fea0"
I0207 11:34:27.766775    3511 logs.go:123] Gathering logs for etcd [1eed8f4e884e] ...
I0207 11:34:27.766790    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 1eed8f4e884e"
I0207 11:34:27.916324    3511 logs.go:123] Gathering logs for coredns [06fb15ac4a32] ...
I0207 11:34:27.916344    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 06fb15ac4a32"
I0207 11:34:27.989576    3511 logs.go:123] Gathering logs for coredns [b94028402ad3] ...
I0207 11:34:27.989598    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 b94028402ad3"
I0207 11:34:28.040175    3511 logs.go:123] Gathering logs for kubernetes-dashboard [f4b96dcbc9f2] ...
I0207 11:34:28.040192    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 f4b96dcbc9f2"
I0207 11:34:28.167461    3511 logs.go:123] Gathering logs for kube-controller-manager [e1bb8361945e] ...
I0207 11:34:28.167477    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 e1bb8361945e"
I0207 11:34:28.262919    3511 logs.go:123] Gathering logs for kube-controller-manager [885389110d12] ...
I0207 11:34:28.262940    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 885389110d12"
I0207 11:34:28.337411    3511 logs.go:123] Gathering logs for storage-provisioner [ec52b9ed7e21] ...
I0207 11:34:28.337427    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 ec52b9ed7e21"
I0207 11:34:28.385948    3511 logs.go:123] Gathering logs for kube-apiserver [39d1988a2fab] ...
I0207 11:34:28.385967    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 39d1988a2fab"
I0207 11:34:28.449730    3511 logs.go:123] Gathering logs for kube-scheduler [1d1e9abd9836] ...
I0207 11:34:28.449753    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 1d1e9abd9836"
I0207 11:34:28.521405    3511 logs.go:123] Gathering logs for kube-scheduler [ee7c2215045a] ...
I0207 11:34:28.521419    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 ee7c2215045a"
I0207 11:34:28.571890    3511 logs.go:123] Gathering logs for kube-proxy [1db52edcdad4] ...
I0207 11:34:28.571906    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 1db52edcdad4"
I0207 11:34:28.612672    3511 logs.go:123] Gathering logs for kube-proxy [407d81ee6871] ...
I0207 11:34:28.612688    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 407d81ee6871"
I0207 11:34:28.643530    3511 logs.go:123] Gathering logs for describe nodes ...
I0207 11:34:28.643545    3511 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig"
W0207 11:34:28.750960    3511 logs.go:130] failed describe nodes: command: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig" /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig": Process exited with status 1
stdout:

stderr:
E0207 03:34:28.738524    9178 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:34:28.739523    9178 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:34:28.741757    9178 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:34:28.742428    9178 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:34:28.743480    9178 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
The connection to the server localhost:8443 was refused - did you specify the right host or port?
 output: 
** stderr ** 
E0207 03:34:28.738524    9178 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:34:28.739523    9178 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:34:28.741757    9178 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:34:28.742428    9178 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:34:28.743480    9178 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
The connection to the server localhost:8443 was refused - did you specify the right host or port?

** /stderr **
I0207 11:34:28.750974    3511 logs.go:123] Gathering logs for kubernetes-dashboard [5631cbc21ffd] ...
I0207 11:34:28.750991    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 5631cbc21ffd"
I0207 11:34:28.787130    3511 logs.go:123] Gathering logs for Docker ...
I0207 11:34:28.787145    3511 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u docker -u cri-docker -n 400"
I0207 11:34:28.885048    3511 logs.go:123] Gathering logs for kubelet ...
I0207 11:34:28.885109    3511 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u kubelet -n 400"
I0207 11:34:29.012103    3511 logs.go:123] Gathering logs for dmesg ...
I0207 11:34:29.012122    3511 ssh_runner.go:195] Run: /bin/bash -c "sudo dmesg -PH -L=never --level warn,err,crit,alert,emerg | tail -n 400"
I0207 11:34:29.093357    3511 logs.go:123] Gathering logs for etcd [5cf1eff8b362] ...
I0207 11:34:29.093372    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 5cf1eff8b362"
I0207 11:34:29.159550    3511 logs.go:123] Gathering logs for controller_ingress [8d50139a5311] ...
I0207 11:34:29.159566    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 8d50139a5311"
I0207 11:34:29.227362    3511 logs.go:123] Gathering logs for storage-provisioner [0b85673627c7] ...
I0207 11:34:29.227376    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 0b85673627c7"
I0207 11:34:31.785588    3511 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0207 11:34:31.850610    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-apiserver --format={{.ID}}
I0207 11:34:31.894548    3511 logs.go:284] 2 containers: [1b36eac9fea0 39d1988a2fab]
I0207 11:34:31.894645    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_etcd --format={{.ID}}
I0207 11:34:31.933222    3511 logs.go:284] 2 containers: [1eed8f4e884e 5cf1eff8b362]
I0207 11:34:31.933344    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_coredns --format={{.ID}}
I0207 11:34:31.979836    3511 logs.go:284] 2 containers: [06fb15ac4a32 b94028402ad3]
I0207 11:34:31.980117    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-scheduler --format={{.ID}}
I0207 11:34:32.051002    3511 logs.go:284] 2 containers: [1d1e9abd9836 ee7c2215045a]
I0207 11:34:32.051109    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-proxy --format={{.ID}}
I0207 11:34:32.111364    3511 logs.go:284] 2 containers: [1db52edcdad4 407d81ee6871]
I0207 11:34:32.111449    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-controller-manager --format={{.ID}}
I0207 11:34:32.138946    3511 logs.go:284] 2 containers: [e1bb8361945e 885389110d12]
I0207 11:34:32.139069    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kindnet --format={{.ID}}
I0207 11:34:32.164759    3511 logs.go:284] 0 containers: []
W0207 11:34:32.164777    3511 logs.go:286] No container was found matching "kindnet"
I0207 11:34:32.164864    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_controller_ingress --format={{.ID}}
I0207 11:34:32.192235    3511 logs.go:284] 1 containers: [8d50139a5311]
I0207 11:34:32.192389    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_storage-provisioner --format={{.ID}}
I0207 11:34:32.222568    3511 logs.go:284] 2 containers: [ec52b9ed7e21 0b85673627c7]
I0207 11:34:32.222675    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kubernetes-dashboard --format={{.ID}}
I0207 11:34:32.259719    3511 logs.go:284] 2 containers: [5631cbc21ffd f4b96dcbc9f2]
I0207 11:34:32.259748    3511 logs.go:123] Gathering logs for kubernetes-dashboard [f4b96dcbc9f2] ...
I0207 11:34:32.259762    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 f4b96dcbc9f2"
I0207 11:34:32.340509    3511 logs.go:123] Gathering logs for etcd [1eed8f4e884e] ...
I0207 11:34:32.340524    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 1eed8f4e884e"
I0207 11:34:32.514061    3511 logs.go:123] Gathering logs for coredns [06fb15ac4a32] ...
I0207 11:34:32.514078    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 06fb15ac4a32"
I0207 11:34:32.581921    3511 logs.go:123] Gathering logs for coredns [b94028402ad3] ...
I0207 11:34:32.581946    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 b94028402ad3"
I0207 11:34:32.634789    3511 logs.go:123] Gathering logs for kube-scheduler [ee7c2215045a] ...
I0207 11:34:32.634816    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 ee7c2215045a"
I0207 11:34:32.694724    3511 logs.go:123] Gathering logs for kube-controller-manager [e1bb8361945e] ...
I0207 11:34:32.694738    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 e1bb8361945e"
I0207 11:34:32.785417    3511 logs.go:123] Gathering logs for kube-controller-manager [885389110d12] ...
I0207 11:34:32.785432    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 885389110d12"
I0207 11:34:32.843786    3511 logs.go:123] Gathering logs for storage-provisioner [ec52b9ed7e21] ...
I0207 11:34:32.843804    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 ec52b9ed7e21"
I0207 11:34:32.891242    3511 logs.go:123] Gathering logs for kube-apiserver [39d1988a2fab] ...
I0207 11:34:32.891257    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 39d1988a2fab"
I0207 11:34:32.963049    3511 logs.go:123] Gathering logs for Docker ...
I0207 11:34:32.963068    3511 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u docker -u cri-docker -n 400"
I0207 11:34:33.046514    3511 logs.go:123] Gathering logs for kubelet ...
I0207 11:34:33.046532    3511 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u kubelet -n 400"
I0207 11:34:33.164742    3511 logs.go:123] Gathering logs for etcd [5cf1eff8b362] ...
I0207 11:34:33.164766    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 5cf1eff8b362"
I0207 11:34:33.231872    3511 logs.go:123] Gathering logs for kube-scheduler [1d1e9abd9836] ...
I0207 11:34:33.231888    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 1d1e9abd9836"
I0207 11:34:33.287292    3511 logs.go:123] Gathering logs for kube-proxy [1db52edcdad4] ...
I0207 11:34:33.287307    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 1db52edcdad4"
I0207 11:34:33.320798    3511 logs.go:123] Gathering logs for container status ...
I0207 11:34:33.320818    3511 ssh_runner.go:195] Run: /bin/bash -c "sudo `which crictl || echo crictl` ps -a || sudo docker ps -a"
I0207 11:34:33.476557    3511 logs.go:123] Gathering logs for dmesg ...
I0207 11:34:33.476571    3511 ssh_runner.go:195] Run: /bin/bash -c "sudo dmesg -PH -L=never --level warn,err,crit,alert,emerg | tail -n 400"
I0207 11:34:33.542025    3511 logs.go:123] Gathering logs for describe nodes ...
I0207 11:34:33.542041    3511 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig"
W0207 11:34:33.653533    3511 logs.go:130] failed describe nodes: command: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig" /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig": Process exited with status 1
stdout:

stderr:
E0207 03:34:33.638366    9479 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:34:33.639444    9479 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:34:33.641370    9479 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:34:33.643679    9479 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:34:33.644563    9479 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
The connection to the server localhost:8443 was refused - did you specify the right host or port?
 output: 
** stderr ** 
E0207 03:34:33.638366    9479 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:34:33.639444    9479 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:34:33.641370    9479 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:34:33.643679    9479 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:34:33.644563    9479 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
The connection to the server localhost:8443 was refused - did you specify the right host or port?

** /stderr **
I0207 11:34:33.653547    3511 logs.go:123] Gathering logs for kube-apiserver [1b36eac9fea0] ...
I0207 11:34:33.653560    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 1b36eac9fea0"
I0207 11:34:33.728408    3511 logs.go:123] Gathering logs for kube-proxy [407d81ee6871] ...
I0207 11:34:33.728423    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 407d81ee6871"
I0207 11:34:33.761573    3511 logs.go:123] Gathering logs for controller_ingress [8d50139a5311] ...
I0207 11:34:33.761616    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 8d50139a5311"
I0207 11:34:33.824792    3511 logs.go:123] Gathering logs for storage-provisioner [0b85673627c7] ...
I0207 11:34:33.824808    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 0b85673627c7"
I0207 11:34:33.888878    3511 logs.go:123] Gathering logs for kubernetes-dashboard [5631cbc21ffd] ...
I0207 11:34:33.888894    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 5631cbc21ffd"
I0207 11:34:36.465273    3511 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0207 11:34:36.495970    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-apiserver --format={{.ID}}
I0207 11:34:36.536309    3511 logs.go:284] 2 containers: [1b36eac9fea0 39d1988a2fab]
I0207 11:34:36.536424    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_etcd --format={{.ID}}
I0207 11:34:36.568099    3511 logs.go:284] 2 containers: [1eed8f4e884e 5cf1eff8b362]
I0207 11:34:36.568204    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_coredns --format={{.ID}}
I0207 11:34:36.598695    3511 logs.go:284] 2 containers: [06fb15ac4a32 b94028402ad3]
I0207 11:34:36.598819    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-scheduler --format={{.ID}}
I0207 11:34:36.630676    3511 logs.go:284] 2 containers: [1d1e9abd9836 ee7c2215045a]
I0207 11:34:36.630795    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-proxy --format={{.ID}}
I0207 11:34:36.657652    3511 logs.go:284] 2 containers: [1db52edcdad4 407d81ee6871]
I0207 11:34:36.657759    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-controller-manager --format={{.ID}}
I0207 11:34:36.688636    3511 logs.go:284] 2 containers: [e1bb8361945e 885389110d12]
I0207 11:34:36.688728    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kindnet --format={{.ID}}
I0207 11:34:36.723839    3511 logs.go:284] 0 containers: []
W0207 11:34:36.723857    3511 logs.go:286] No container was found matching "kindnet"
I0207 11:34:36.723971    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_controller_ingress --format={{.ID}}
I0207 11:34:36.758674    3511 logs.go:284] 1 containers: [8d50139a5311]
I0207 11:34:36.758791    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_storage-provisioner --format={{.ID}}
I0207 11:34:36.793458    3511 logs.go:284] 2 containers: [ec52b9ed7e21 0b85673627c7]
I0207 11:34:36.793551    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kubernetes-dashboard --format={{.ID}}
I0207 11:34:36.831263    3511 logs.go:284] 2 containers: [5631cbc21ffd f4b96dcbc9f2]
I0207 11:34:36.831290    3511 logs.go:123] Gathering logs for describe nodes ...
I0207 11:34:36.831299    3511 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig"
W0207 11:34:36.982601    3511 logs.go:130] failed describe nodes: command: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig" /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig": Process exited with status 1
stdout:

stderr:
E0207 03:34:36.970105    9632 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:34:36.970955    9632 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:34:36.973034    9632 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:34:36.974052    9632 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:34:36.975021    9632 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
The connection to the server localhost:8443 was refused - did you specify the right host or port?
 output: 
** stderr ** 
E0207 03:34:36.970105    9632 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:34:36.970955    9632 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:34:36.973034    9632 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:34:36.974052    9632 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:34:36.975021    9632 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
The connection to the server localhost:8443 was refused - did you specify the right host or port?

** /stderr **
I0207 11:34:36.982635    3511 logs.go:123] Gathering logs for etcd [1eed8f4e884e] ...
I0207 11:34:36.982668    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 1eed8f4e884e"
I0207 11:34:37.162532    3511 logs.go:123] Gathering logs for kube-scheduler [1d1e9abd9836] ...
I0207 11:34:37.162558    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 1d1e9abd9836"
I0207 11:34:37.235605    3511 logs.go:123] Gathering logs for kube-proxy [1db52edcdad4] ...
I0207 11:34:37.235620    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 1db52edcdad4"
I0207 11:34:37.283423    3511 logs.go:123] Gathering logs for kube-controller-manager [885389110d12] ...
I0207 11:34:37.283445    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 885389110d12"
I0207 11:34:37.341751    3511 logs.go:123] Gathering logs for storage-provisioner [ec52b9ed7e21] ...
I0207 11:34:37.341766    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 ec52b9ed7e21"
I0207 11:34:37.374762    3511 logs.go:123] Gathering logs for kube-apiserver [1b36eac9fea0] ...
I0207 11:34:37.374785    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 1b36eac9fea0"
I0207 11:34:37.449108    3511 logs.go:123] Gathering logs for kube-apiserver [39d1988a2fab] ...
I0207 11:34:37.449125    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 39d1988a2fab"
I0207 11:34:37.502856    3511 logs.go:123] Gathering logs for etcd [5cf1eff8b362] ...
I0207 11:34:37.502871    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 5cf1eff8b362"
I0207 11:34:37.581216    3511 logs.go:123] Gathering logs for coredns [06fb15ac4a32] ...
I0207 11:34:37.581232    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 06fb15ac4a32"
I0207 11:34:37.644646    3511 logs.go:123] Gathering logs for coredns [b94028402ad3] ...
I0207 11:34:37.644665    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 b94028402ad3"
I0207 11:34:37.719001    3511 logs.go:123] Gathering logs for kube-controller-manager [e1bb8361945e] ...
I0207 11:34:37.719023    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 e1bb8361945e"
I0207 11:34:37.898360    3511 logs.go:123] Gathering logs for storage-provisioner [0b85673627c7] ...
I0207 11:34:37.898382    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 0b85673627c7"
I0207 11:34:38.007007    3511 logs.go:123] Gathering logs for kubelet ...
I0207 11:34:38.007030    3511 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u kubelet -n 400"
I0207 11:34:38.092083    3511 logs.go:123] Gathering logs for dmesg ...
I0207 11:34:38.092106    3511 ssh_runner.go:195] Run: /bin/bash -c "sudo dmesg -PH -L=never --level warn,err,crit,alert,emerg | tail -n 400"
I0207 11:34:38.159276    3511 logs.go:123] Gathering logs for kube-scheduler [ee7c2215045a] ...
I0207 11:34:38.159291    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 ee7c2215045a"
I0207 11:34:38.210905    3511 logs.go:123] Gathering logs for kube-proxy [407d81ee6871] ...
I0207 11:34:38.210967    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 407d81ee6871"
I0207 11:34:38.245721    3511 logs.go:123] Gathering logs for controller_ingress [8d50139a5311] ...
I0207 11:34:38.245737    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 8d50139a5311"
I0207 11:34:38.308457    3511 logs.go:123] Gathering logs for kubernetes-dashboard [5631cbc21ffd] ...
I0207 11:34:38.308488    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 5631cbc21ffd"
I0207 11:34:38.346690    3511 logs.go:123] Gathering logs for Docker ...
I0207 11:34:38.346707    3511 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u docker -u cri-docker -n 400"
I0207 11:34:38.404145    3511 logs.go:123] Gathering logs for kubernetes-dashboard [f4b96dcbc9f2] ...
I0207 11:34:38.404161    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 f4b96dcbc9f2"
I0207 11:34:38.508167    3511 logs.go:123] Gathering logs for container status ...
I0207 11:34:38.508188    3511 ssh_runner.go:195] Run: /bin/bash -c "sudo `which crictl || echo crictl` ps -a || sudo docker ps -a"
I0207 11:34:41.209881    3511 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0207 11:34:41.229076    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-apiserver --format={{.ID}}
I0207 11:34:41.261472    3511 logs.go:284] 2 containers: [1b36eac9fea0 39d1988a2fab]
I0207 11:34:41.261595    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_etcd --format={{.ID}}
I0207 11:34:41.293851    3511 logs.go:284] 2 containers: [1eed8f4e884e 5cf1eff8b362]
I0207 11:34:41.294037    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_coredns --format={{.ID}}
I0207 11:34:41.324990    3511 logs.go:284] 2 containers: [06fb15ac4a32 b94028402ad3]
I0207 11:34:41.325113    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-scheduler --format={{.ID}}
I0207 11:34:41.360524    3511 logs.go:284] 2 containers: [1d1e9abd9836 ee7c2215045a]
I0207 11:34:41.360658    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-proxy --format={{.ID}}
I0207 11:34:41.396963    3511 logs.go:284] 2 containers: [1db52edcdad4 407d81ee6871]
I0207 11:34:41.397133    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-controller-manager --format={{.ID}}
I0207 11:34:41.427424    3511 logs.go:284] 2 containers: [e1bb8361945e 885389110d12]
I0207 11:34:41.427555    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kindnet --format={{.ID}}
I0207 11:34:41.457039    3511 logs.go:284] 0 containers: []
W0207 11:34:41.457057    3511 logs.go:286] No container was found matching "kindnet"
I0207 11:34:41.457136    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_controller_ingress --format={{.ID}}
I0207 11:34:41.488575    3511 logs.go:284] 1 containers: [8d50139a5311]
I0207 11:34:41.488674    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_storage-provisioner --format={{.ID}}
I0207 11:34:41.519070    3511 logs.go:284] 2 containers: [ec52b9ed7e21 0b85673627c7]
I0207 11:34:41.519180    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kubernetes-dashboard --format={{.ID}}
I0207 11:34:41.552679    3511 logs.go:284] 2 containers: [5631cbc21ffd f4b96dcbc9f2]
I0207 11:34:41.552717    3511 logs.go:123] Gathering logs for kube-apiserver [1b36eac9fea0] ...
I0207 11:34:41.552732    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 1b36eac9fea0"
I0207 11:34:41.614974    3511 logs.go:123] Gathering logs for Docker ...
I0207 11:34:41.615001    3511 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u docker -u cri-docker -n 400"
I0207 11:34:41.693124    3511 logs.go:123] Gathering logs for kubelet ...
I0207 11:34:41.693188    3511 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u kubelet -n 400"
I0207 11:34:41.794622    3511 logs.go:123] Gathering logs for dmesg ...
I0207 11:34:41.794642    3511 ssh_runner.go:195] Run: /bin/bash -c "sudo dmesg -PH -L=never --level warn,err,crit,alert,emerg | tail -n 400"
I0207 11:34:41.877672    3511 logs.go:123] Gathering logs for coredns [b94028402ad3] ...
I0207 11:34:41.877691    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 b94028402ad3"
I0207 11:34:41.922132    3511 logs.go:123] Gathering logs for kube-proxy [407d81ee6871] ...
I0207 11:34:41.922164    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 407d81ee6871"
I0207 11:34:41.959949    3511 logs.go:123] Gathering logs for kube-controller-manager [885389110d12] ...
I0207 11:34:41.959970    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 885389110d12"
I0207 11:34:42.040572    3511 logs.go:123] Gathering logs for storage-provisioner [ec52b9ed7e21] ...
I0207 11:34:42.040598    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 ec52b9ed7e21"
I0207 11:34:42.085035    3511 logs.go:123] Gathering logs for storage-provisioner [0b85673627c7] ...
I0207 11:34:42.085056    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 0b85673627c7"
I0207 11:34:42.186625    3511 logs.go:123] Gathering logs for container status ...
I0207 11:34:42.186643    3511 ssh_runner.go:195] Run: /bin/bash -c "sudo `which crictl || echo crictl` ps -a || sudo docker ps -a"
I0207 11:34:42.392374    3511 logs.go:123] Gathering logs for etcd [5cf1eff8b362] ...
I0207 11:34:42.392391    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 5cf1eff8b362"
I0207 11:34:42.457325    3511 logs.go:123] Gathering logs for coredns [06fb15ac4a32] ...
I0207 11:34:42.457341    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 06fb15ac4a32"
I0207 11:34:42.514383    3511 logs.go:123] Gathering logs for kube-proxy [1db52edcdad4] ...
I0207 11:34:42.514398    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 1db52edcdad4"
I0207 11:34:42.562635    3511 logs.go:123] Gathering logs for kube-controller-manager [e1bb8361945e] ...
I0207 11:34:42.562653    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 e1bb8361945e"
I0207 11:34:42.647772    3511 logs.go:123] Gathering logs for kubernetes-dashboard [f4b96dcbc9f2] ...
I0207 11:34:42.647787    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 f4b96dcbc9f2"
I0207 11:34:42.766999    3511 logs.go:123] Gathering logs for describe nodes ...
I0207 11:34:42.767016    3511 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig"
W0207 11:34:42.922082    3511 logs.go:130] failed describe nodes: command: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig" /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig": Process exited with status 1
stdout:

stderr:
E0207 03:34:42.909766   10055 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:34:42.910971   10055 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:34:42.911415   10055 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:34:42.913835   10055 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:34:42.914895   10055 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
The connection to the server localhost:8443 was refused - did you specify the right host or port?
 output: 
** stderr ** 
E0207 03:34:42.909766   10055 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:34:42.910971   10055 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:34:42.911415   10055 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:34:42.913835   10055 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:34:42.914895   10055 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
The connection to the server localhost:8443 was refused - did you specify the right host or port?

** /stderr **
I0207 11:34:42.922095    3511 logs.go:123] Gathering logs for kube-scheduler [1d1e9abd9836] ...
I0207 11:34:42.922109    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 1d1e9abd9836"
I0207 11:34:42.987758    3511 logs.go:123] Gathering logs for kube-scheduler [ee7c2215045a] ...
I0207 11:34:42.987773    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 ee7c2215045a"
I0207 11:34:43.042228    3511 logs.go:123] Gathering logs for controller_ingress [8d50139a5311] ...
I0207 11:34:43.042252    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 8d50139a5311"
I0207 11:34:43.111678    3511 logs.go:123] Gathering logs for kubernetes-dashboard [5631cbc21ffd] ...
I0207 11:34:43.111694    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 5631cbc21ffd"
I0207 11:34:43.146886    3511 logs.go:123] Gathering logs for kube-apiserver [39d1988a2fab] ...
I0207 11:34:43.146903    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 39d1988a2fab"
I0207 11:34:43.218757    3511 logs.go:123] Gathering logs for etcd [1eed8f4e884e] ...
I0207 11:34:43.218772    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 1eed8f4e884e"
I0207 11:34:45.868049    3511 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0207 11:34:45.892020    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-apiserver --format={{.ID}}
I0207 11:34:45.922464    3511 logs.go:284] 2 containers: [1b36eac9fea0 39d1988a2fab]
I0207 11:34:45.922593    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_etcd --format={{.ID}}
I0207 11:34:45.953389    3511 logs.go:284] 2 containers: [1eed8f4e884e 5cf1eff8b362]
I0207 11:34:45.953485    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_coredns --format={{.ID}}
I0207 11:34:45.987213    3511 logs.go:284] 2 containers: [06fb15ac4a32 b94028402ad3]
I0207 11:34:45.987312    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-scheduler --format={{.ID}}
I0207 11:34:46.022195    3511 logs.go:284] 2 containers: [1d1e9abd9836 ee7c2215045a]
I0207 11:34:46.022324    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-proxy --format={{.ID}}
I0207 11:34:46.062582    3511 logs.go:284] 2 containers: [1db52edcdad4 407d81ee6871]
I0207 11:34:46.062675    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-controller-manager --format={{.ID}}
I0207 11:34:46.103906    3511 logs.go:284] 2 containers: [e1bb8361945e 885389110d12]
I0207 11:34:46.104057    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kindnet --format={{.ID}}
I0207 11:34:46.140956    3511 logs.go:284] 0 containers: []
W0207 11:34:46.140972    3511 logs.go:286] No container was found matching "kindnet"
I0207 11:34:46.141056    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kubernetes-dashboard --format={{.ID}}
I0207 11:34:46.171782    3511 logs.go:284] 2 containers: [5631cbc21ffd f4b96dcbc9f2]
I0207 11:34:46.171862    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_controller_ingress --format={{.ID}}
I0207 11:34:46.202889    3511 logs.go:284] 1 containers: [8d50139a5311]
I0207 11:34:46.203054    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_storage-provisioner --format={{.ID}}
I0207 11:34:46.237535    3511 logs.go:284] 2 containers: [ec52b9ed7e21 0b85673627c7]
I0207 11:34:46.237564    3511 logs.go:123] Gathering logs for coredns [b94028402ad3] ...
I0207 11:34:46.237636    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 b94028402ad3"
I0207 11:34:46.306612    3511 logs.go:123] Gathering logs for kube-scheduler [1d1e9abd9836] ...
I0207 11:34:46.306629    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 1d1e9abd9836"
I0207 11:34:46.381793    3511 logs.go:123] Gathering logs for kube-scheduler [ee7c2215045a] ...
I0207 11:34:46.381818    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 ee7c2215045a"
I0207 11:34:46.431143    3511 logs.go:123] Gathering logs for controller_ingress [8d50139a5311] ...
I0207 11:34:46.431172    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 8d50139a5311"
I0207 11:34:46.493487    3511 logs.go:123] Gathering logs for storage-provisioner [0b85673627c7] ...
I0207 11:34:46.493502    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 0b85673627c7"
I0207 11:34:46.569385    3511 logs.go:123] Gathering logs for kubelet ...
I0207 11:34:46.569410    3511 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u kubelet -n 400"
I0207 11:34:46.681847    3511 logs.go:123] Gathering logs for etcd [5cf1eff8b362] ...
I0207 11:34:46.681867    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 5cf1eff8b362"
I0207 11:34:46.775715    3511 logs.go:123] Gathering logs for kube-apiserver [1b36eac9fea0] ...
I0207 11:34:46.775742    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 1b36eac9fea0"
I0207 11:34:46.857299    3511 logs.go:123] Gathering logs for coredns [06fb15ac4a32] ...
I0207 11:34:46.857328    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 06fb15ac4a32"
I0207 11:34:46.921567    3511 logs.go:123] Gathering logs for kube-controller-manager [e1bb8361945e] ...
I0207 11:34:46.921583    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 e1bb8361945e"
I0207 11:34:46.999641    3511 logs.go:123] Gathering logs for kubernetes-dashboard [5631cbc21ffd] ...
I0207 11:34:46.999655    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 5631cbc21ffd"
I0207 11:34:47.044782    3511 logs.go:123] Gathering logs for Docker ...
I0207 11:34:47.044797    3511 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u docker -u cri-docker -n 400"
I0207 11:34:47.107292    3511 logs.go:123] Gathering logs for dmesg ...
I0207 11:34:47.107308    3511 ssh_runner.go:195] Run: /bin/bash -c "sudo dmesg -PH -L=never --level warn,err,crit,alert,emerg | tail -n 400"
I0207 11:34:47.180451    3511 logs.go:123] Gathering logs for describe nodes ...
I0207 11:34:47.180473    3511 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig"
W0207 11:34:47.307249    3511 logs.go:130] failed describe nodes: command: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig" /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig": Process exited with status 1
stdout:

stderr:
E0207 03:34:47.289640   10316 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:34:47.290572   10316 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:34:47.293352   10316 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:34:47.295360   10316 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:34:47.297208   10316 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
The connection to the server localhost:8443 was refused - did you specify the right host or port?
 output: 
** stderr ** 
E0207 03:34:47.289640   10316 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:34:47.290572   10316 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:34:47.293352   10316 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:34:47.295360   10316 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:34:47.297208   10316 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
The connection to the server localhost:8443 was refused - did you specify the right host or port?

** /stderr **
I0207 11:34:47.307260    3511 logs.go:123] Gathering logs for kube-controller-manager [885389110d12] ...
I0207 11:34:47.307269    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 885389110d12"
I0207 11:34:47.399670    3511 logs.go:123] Gathering logs for kube-apiserver [39d1988a2fab] ...
I0207 11:34:47.399699    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 39d1988a2fab"
I0207 11:34:47.449833    3511 logs.go:123] Gathering logs for kube-proxy [1db52edcdad4] ...
I0207 11:34:47.449849    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 1db52edcdad4"
I0207 11:34:47.486506    3511 logs.go:123] Gathering logs for kubernetes-dashboard [f4b96dcbc9f2] ...
I0207 11:34:47.486522    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 f4b96dcbc9f2"
I0207 11:34:47.557278    3511 logs.go:123] Gathering logs for storage-provisioner [ec52b9ed7e21] ...
I0207 11:34:47.557304    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 ec52b9ed7e21"
I0207 11:34:47.595481    3511 logs.go:123] Gathering logs for container status ...
I0207 11:34:47.595496    3511 ssh_runner.go:195] Run: /bin/bash -c "sudo `which crictl || echo crictl` ps -a || sudo docker ps -a"
I0207 11:34:47.889116    3511 logs.go:123] Gathering logs for etcd [1eed8f4e884e] ...
I0207 11:34:47.889144    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 1eed8f4e884e"
I0207 11:34:48.088999    3511 logs.go:123] Gathering logs for kube-proxy [407d81ee6871] ...
I0207 11:34:48.089018    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 407d81ee6871"
I0207 11:34:50.625350    3511 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0207 11:34:50.660319    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-apiserver --format={{.ID}}
I0207 11:34:50.701271    3511 logs.go:284] 2 containers: [1b36eac9fea0 39d1988a2fab]
I0207 11:34:50.701389    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_etcd --format={{.ID}}
I0207 11:34:50.738077    3511 logs.go:284] 2 containers: [1eed8f4e884e 5cf1eff8b362]
I0207 11:34:50.738167    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_coredns --format={{.ID}}
I0207 11:34:50.785646    3511 logs.go:284] 2 containers: [06fb15ac4a32 b94028402ad3]
I0207 11:34:50.785747    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-scheduler --format={{.ID}}
I0207 11:34:50.829428    3511 logs.go:284] 2 containers: [1d1e9abd9836 ee7c2215045a]
I0207 11:34:50.829542    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-proxy --format={{.ID}}
I0207 11:34:50.892245    3511 logs.go:284] 2 containers: [1db52edcdad4 407d81ee6871]
I0207 11:34:50.892346    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-controller-manager --format={{.ID}}
I0207 11:34:50.923521    3511 logs.go:284] 2 containers: [e1bb8361945e 885389110d12]
I0207 11:34:50.923655    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kindnet --format={{.ID}}
I0207 11:34:50.951820    3511 logs.go:284] 0 containers: []
W0207 11:34:50.951838    3511 logs.go:286] No container was found matching "kindnet"
I0207 11:34:50.951923    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kubernetes-dashboard --format={{.ID}}
I0207 11:34:50.982067    3511 logs.go:284] 2 containers: [5631cbc21ffd f4b96dcbc9f2]
I0207 11:34:50.982157    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_controller_ingress --format={{.ID}}
I0207 11:34:51.017498    3511 logs.go:284] 1 containers: [8d50139a5311]
I0207 11:34:51.017625    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_storage-provisioner --format={{.ID}}
I0207 11:34:51.057723    3511 logs.go:284] 2 containers: [ec52b9ed7e21 0b85673627c7]
I0207 11:34:51.057750    3511 logs.go:123] Gathering logs for storage-provisioner [0b85673627c7] ...
I0207 11:34:51.057763    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 0b85673627c7"
I0207 11:34:51.124569    3511 logs.go:123] Gathering logs for Docker ...
I0207 11:34:51.124585    3511 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u docker -u cri-docker -n 400"
I0207 11:34:51.192918    3511 logs.go:123] Gathering logs for dmesg ...
I0207 11:34:51.192938    3511 ssh_runner.go:195] Run: /bin/bash -c "sudo dmesg -PH -L=never --level warn,err,crit,alert,emerg | tail -n 400"
I0207 11:34:51.264925    3511 logs.go:123] Gathering logs for describe nodes ...
I0207 11:34:51.264951    3511 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig"
W0207 11:34:51.403999    3511 logs.go:130] failed describe nodes: command: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig" /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig": Process exited with status 1
stdout:

stderr:
E0207 03:34:51.386677   10542 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:34:51.388242   10542 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:34:51.390007   10542 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:34:51.391446   10542 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:34:51.393817   10542 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
The connection to the server localhost:8443 was refused - did you specify the right host or port?
 output: 
** stderr ** 
E0207 03:34:51.386677   10542 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:34:51.388242   10542 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:34:51.390007   10542 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:34:51.391446   10542 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:34:51.393817   10542 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
The connection to the server localhost:8443 was refused - did you specify the right host or port?

** /stderr **
I0207 11:34:51.404037    3511 logs.go:123] Gathering logs for kube-apiserver [1b36eac9fea0] ...
I0207 11:34:51.404059    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 1b36eac9fea0"
I0207 11:34:51.482766    3511 logs.go:123] Gathering logs for kube-controller-manager [e1bb8361945e] ...
I0207 11:34:51.482783    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 e1bb8361945e"
I0207 11:34:51.578821    3511 logs.go:123] Gathering logs for kubernetes-dashboard [5631cbc21ffd] ...
I0207 11:34:51.578836    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 5631cbc21ffd"
I0207 11:34:51.624211    3511 logs.go:123] Gathering logs for coredns [b94028402ad3] ...
I0207 11:34:51.624236    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 b94028402ad3"
I0207 11:34:51.682993    3511 logs.go:123] Gathering logs for kube-controller-manager [885389110d12] ...
I0207 11:34:51.683012    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 885389110d12"
I0207 11:34:51.764588    3511 logs.go:123] Gathering logs for kubernetes-dashboard [f4b96dcbc9f2] ...
I0207 11:34:51.764605    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 f4b96dcbc9f2"
I0207 11:34:51.869320    3511 logs.go:123] Gathering logs for kubelet ...
I0207 11:34:51.869336    3511 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u kubelet -n 400"
I0207 11:34:51.975751    3511 logs.go:123] Gathering logs for etcd [1eed8f4e884e] ...
I0207 11:34:51.975768    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 1eed8f4e884e"
I0207 11:34:52.155969    3511 logs.go:123] Gathering logs for etcd [5cf1eff8b362] ...
I0207 11:34:52.155984    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 5cf1eff8b362"
I0207 11:34:52.235502    3511 logs.go:123] Gathering logs for kube-scheduler [1d1e9abd9836] ...
I0207 11:34:52.235529    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 1d1e9abd9836"
I0207 11:34:52.301012    3511 logs.go:123] Gathering logs for kube-scheduler [ee7c2215045a] ...
I0207 11:34:52.301030    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 ee7c2215045a"
I0207 11:34:52.354193    3511 logs.go:123] Gathering logs for storage-provisioner [ec52b9ed7e21] ...
I0207 11:34:52.354221    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 ec52b9ed7e21"
I0207 11:34:52.389970    3511 logs.go:123] Gathering logs for container status ...
I0207 11:34:52.389990    3511 ssh_runner.go:195] Run: /bin/bash -c "sudo `which crictl || echo crictl` ps -a || sudo docker ps -a"
I0207 11:34:52.560731    3511 logs.go:123] Gathering logs for kube-apiserver [39d1988a2fab] ...
I0207 11:34:52.560746    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 39d1988a2fab"
I0207 11:34:52.607544    3511 logs.go:123] Gathering logs for coredns [06fb15ac4a32] ...
I0207 11:34:52.607572    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 06fb15ac4a32"
I0207 11:34:52.689265    3511 logs.go:123] Gathering logs for kube-proxy [1db52edcdad4] ...
I0207 11:34:52.689285    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 1db52edcdad4"
I0207 11:34:52.751613    3511 logs.go:123] Gathering logs for kube-proxy [407d81ee6871] ...
I0207 11:34:52.751632    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 407d81ee6871"
I0207 11:34:52.808096    3511 logs.go:123] Gathering logs for controller_ingress [8d50139a5311] ...
I0207 11:34:52.808115    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 8d50139a5311"
I0207 11:34:55.416770    3511 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0207 11:34:55.438894    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-apiserver --format={{.ID}}
I0207 11:34:55.468671    3511 logs.go:284] 2 containers: [1b36eac9fea0 39d1988a2fab]
I0207 11:34:55.468772    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_etcd --format={{.ID}}
I0207 11:34:55.500572    3511 logs.go:284] 2 containers: [1eed8f4e884e 5cf1eff8b362]
I0207 11:34:55.500700    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_coredns --format={{.ID}}
I0207 11:34:55.532983    3511 logs.go:284] 2 containers: [06fb15ac4a32 b94028402ad3]
I0207 11:34:55.533120    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-scheduler --format={{.ID}}
I0207 11:34:55.561960    3511 logs.go:284] 2 containers: [1d1e9abd9836 ee7c2215045a]
I0207 11:34:55.562088    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-proxy --format={{.ID}}
I0207 11:34:55.597547    3511 logs.go:284] 2 containers: [1db52edcdad4 407d81ee6871]
I0207 11:34:55.597701    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-controller-manager --format={{.ID}}
I0207 11:34:55.636322    3511 logs.go:284] 2 containers: [e1bb8361945e 885389110d12]
I0207 11:34:55.636417    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kindnet --format={{.ID}}
I0207 11:34:55.666619    3511 logs.go:284] 0 containers: []
W0207 11:34:55.666631    3511 logs.go:286] No container was found matching "kindnet"
I0207 11:34:55.666719    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_storage-provisioner --format={{.ID}}
I0207 11:34:55.706635    3511 logs.go:284] 2 containers: [ec52b9ed7e21 0b85673627c7]
I0207 11:34:55.706748    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kubernetes-dashboard --format={{.ID}}
I0207 11:34:55.746443    3511 logs.go:284] 2 containers: [5631cbc21ffd f4b96dcbc9f2]
I0207 11:34:55.746560    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_controller_ingress --format={{.ID}}
I0207 11:34:55.785283    3511 logs.go:284] 1 containers: [8d50139a5311]
I0207 11:34:55.785338    3511 logs.go:123] Gathering logs for etcd [1eed8f4e884e] ...
I0207 11:34:55.785351    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 1eed8f4e884e"
I0207 11:34:55.934823    3511 logs.go:123] Gathering logs for coredns [b94028402ad3] ...
I0207 11:34:55.934838    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 b94028402ad3"
I0207 11:34:55.979895    3511 logs.go:123] Gathering logs for kube-scheduler [1d1e9abd9836] ...
I0207 11:34:55.979920    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 1d1e9abd9836"
I0207 11:34:56.040975    3511 logs.go:123] Gathering logs for storage-provisioner [ec52b9ed7e21] ...
I0207 11:34:56.040991    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 ec52b9ed7e21"
I0207 11:34:56.073379    3511 logs.go:123] Gathering logs for kubelet ...
I0207 11:34:56.073425    3511 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u kubelet -n 400"
I0207 11:34:56.177138    3511 logs.go:123] Gathering logs for kube-apiserver [1b36eac9fea0] ...
I0207 11:34:56.177157    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 1b36eac9fea0"
I0207 11:34:56.242176    3511 logs.go:123] Gathering logs for kube-proxy [1db52edcdad4] ...
I0207 11:34:56.242194    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 1db52edcdad4"
I0207 11:34:56.315792    3511 logs.go:123] Gathering logs for coredns [06fb15ac4a32] ...
I0207 11:34:56.315808    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 06fb15ac4a32"
I0207 11:34:56.377990    3511 logs.go:123] Gathering logs for kube-scheduler [ee7c2215045a] ...
I0207 11:34:56.378007    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 ee7c2215045a"
I0207 11:34:56.444085    3511 logs.go:123] Gathering logs for kube-controller-manager [e1bb8361945e] ...
I0207 11:34:56.444101    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 e1bb8361945e"
I0207 11:34:56.537290    3511 logs.go:123] Gathering logs for kube-controller-manager [885389110d12] ...
I0207 11:34:56.537320    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 885389110d12"
I0207 11:34:56.609564    3511 logs.go:123] Gathering logs for kubernetes-dashboard [5631cbc21ffd] ...
I0207 11:34:56.609581    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 5631cbc21ffd"
I0207 11:34:56.644657    3511 logs.go:123] Gathering logs for controller_ingress [8d50139a5311] ...
I0207 11:34:56.644672    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 8d50139a5311"
I0207 11:34:56.724364    3511 logs.go:123] Gathering logs for dmesg ...
I0207 11:34:56.724391    3511 ssh_runner.go:195] Run: /bin/bash -c "sudo dmesg -PH -L=never --level warn,err,crit,alert,emerg | tail -n 400"
I0207 11:34:56.794653    3511 logs.go:123] Gathering logs for describe nodes ...
I0207 11:34:56.794668    3511 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig"
W0207 11:34:56.904390    3511 logs.go:130] failed describe nodes: command: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig" /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig": Process exited with status 1
stdout:

stderr:
E0207 03:34:56.890226   10917 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:34:56.891676   10917 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:34:56.893339   10917 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:34:56.893950   10917 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:34:56.895708   10917 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
The connection to the server localhost:8443 was refused - did you specify the right host or port?
 output: 
** stderr ** 
E0207 03:34:56.890226   10917 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:34:56.891676   10917 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:34:56.893339   10917 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:34:56.893950   10917 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:34:56.895708   10917 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
The connection to the server localhost:8443 was refused - did you specify the right host or port?

** /stderr **
I0207 11:34:56.904441    3511 logs.go:123] Gathering logs for Docker ...
I0207 11:34:56.904459    3511 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u docker -u cri-docker -n 400"
I0207 11:34:56.995423    3511 logs.go:123] Gathering logs for kube-proxy [407d81ee6871] ...
I0207 11:34:56.995440    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 407d81ee6871"
I0207 11:34:57.049053    3511 logs.go:123] Gathering logs for storage-provisioner [0b85673627c7] ...
I0207 11:34:57.049072    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 0b85673627c7"
I0207 11:34:57.151197    3511 logs.go:123] Gathering logs for kubernetes-dashboard [f4b96dcbc9f2] ...
I0207 11:34:57.151219    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 f4b96dcbc9f2"
I0207 11:34:57.225127    3511 logs.go:123] Gathering logs for container status ...
I0207 11:34:57.225142    3511 ssh_runner.go:195] Run: /bin/bash -c "sudo `which crictl || echo crictl` ps -a || sudo docker ps -a"
I0207 11:34:57.387337    3511 logs.go:123] Gathering logs for kube-apiserver [39d1988a2fab] ...
I0207 11:34:57.387354    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 39d1988a2fab"
I0207 11:34:57.453541    3511 logs.go:123] Gathering logs for etcd [5cf1eff8b362] ...
I0207 11:34:57.453563    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 5cf1eff8b362"
I0207 11:35:00.023049    3511 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0207 11:35:00.058301    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-apiserver --format={{.ID}}
I0207 11:35:00.123479    3511 logs.go:284] 2 containers: [1b36eac9fea0 39d1988a2fab]
I0207 11:35:00.123573    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_etcd --format={{.ID}}
I0207 11:35:00.154979    3511 logs.go:284] 2 containers: [1eed8f4e884e 5cf1eff8b362]
I0207 11:35:00.155096    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_coredns --format={{.ID}}
I0207 11:35:00.185544    3511 logs.go:284] 2 containers: [06fb15ac4a32 b94028402ad3]
I0207 11:35:00.185656    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-scheduler --format={{.ID}}
I0207 11:35:00.220210    3511 logs.go:284] 2 containers: [1d1e9abd9836 ee7c2215045a]
I0207 11:35:00.220316    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-proxy --format={{.ID}}
I0207 11:35:00.253561    3511 logs.go:284] 2 containers: [1db52edcdad4 407d81ee6871]
I0207 11:35:00.253752    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-controller-manager --format={{.ID}}
I0207 11:35:00.287770    3511 logs.go:284] 2 containers: [e1bb8361945e 885389110d12]
I0207 11:35:00.287862    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kindnet --format={{.ID}}
I0207 11:35:00.315721    3511 logs.go:284] 0 containers: []
W0207 11:35:00.315733    3511 logs.go:286] No container was found matching "kindnet"
I0207 11:35:00.315805    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kubernetes-dashboard --format={{.ID}}
I0207 11:35:00.344413    3511 logs.go:284] 2 containers: [5631cbc21ffd f4b96dcbc9f2]
I0207 11:35:00.344510    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_controller_ingress --format={{.ID}}
I0207 11:35:00.376867    3511 logs.go:284] 1 containers: [8d50139a5311]
I0207 11:35:00.376960    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_storage-provisioner --format={{.ID}}
I0207 11:35:00.407473    3511 logs.go:284] 2 containers: [ec52b9ed7e21 0b85673627c7]
I0207 11:35:00.407505    3511 logs.go:123] Gathering logs for storage-provisioner [ec52b9ed7e21] ...
I0207 11:35:00.407517    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 ec52b9ed7e21"
I0207 11:35:00.445813    3511 logs.go:123] Gathering logs for kubelet ...
I0207 11:35:00.445845    3511 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u kubelet -n 400"
I0207 11:35:00.540950    3511 logs.go:123] Gathering logs for etcd [1eed8f4e884e] ...
I0207 11:35:00.540970    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 1eed8f4e884e"
I0207 11:35:00.687737    3511 logs.go:123] Gathering logs for kube-scheduler [1d1e9abd9836] ...
I0207 11:35:00.687763    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 1d1e9abd9836"
I0207 11:35:00.767988    3511 logs.go:123] Gathering logs for kube-scheduler [ee7c2215045a] ...
I0207 11:35:00.768013    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 ee7c2215045a"
I0207 11:35:00.824488    3511 logs.go:123] Gathering logs for kubernetes-dashboard [f4b96dcbc9f2] ...
I0207 11:35:00.824508    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 f4b96dcbc9f2"
I0207 11:35:00.897939    3511 logs.go:123] Gathering logs for coredns [b94028402ad3] ...
I0207 11:35:00.897954    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 b94028402ad3"
I0207 11:35:00.969455    3511 logs.go:123] Gathering logs for kube-proxy [1db52edcdad4] ...
I0207 11:35:00.969484    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 1db52edcdad4"
I0207 11:35:01.018514    3511 logs.go:123] Gathering logs for kube-controller-manager [e1bb8361945e] ...
I0207 11:35:01.018530    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 e1bb8361945e"
I0207 11:35:01.135594    3511 logs.go:123] Gathering logs for controller_ingress [8d50139a5311] ...
I0207 11:35:01.135612    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 8d50139a5311"
I0207 11:35:01.218302    3511 logs.go:123] Gathering logs for Docker ...
I0207 11:35:01.218319    3511 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u docker -u cri-docker -n 400"
I0207 11:35:01.277283    3511 logs.go:123] Gathering logs for kubernetes-dashboard [5631cbc21ffd] ...
I0207 11:35:01.277299    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 5631cbc21ffd"
I0207 11:35:01.310580    3511 logs.go:123] Gathering logs for describe nodes ...
I0207 11:35:01.310595    3511 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig"
W0207 11:35:01.433871    3511 logs.go:130] failed describe nodes: command: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig" /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig": Process exited with status 1
stdout:

stderr:
E0207 03:35:01.421345   11205 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:35:01.422407   11205 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:35:01.423485   11205 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:35:01.425199   11205 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:35:01.426332   11205 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
The connection to the server localhost:8443 was refused - did you specify the right host or port?
 output: 
** stderr ** 
E0207 03:35:01.421345   11205 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:35:01.422407   11205 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:35:01.423485   11205 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:35:01.425199   11205 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:35:01.426332   11205 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
The connection to the server localhost:8443 was refused - did you specify the right host or port?

** /stderr **
I0207 11:35:01.433881    3511 logs.go:123] Gathering logs for kube-apiserver [39d1988a2fab] ...
I0207 11:35:01.433891    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 39d1988a2fab"
I0207 11:35:01.486702    3511 logs.go:123] Gathering logs for etcd [5cf1eff8b362] ...
I0207 11:35:01.486725    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 5cf1eff8b362"
I0207 11:35:01.579132    3511 logs.go:123] Gathering logs for coredns [06fb15ac4a32] ...
I0207 11:35:01.579148    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 06fb15ac4a32"
I0207 11:35:01.648486    3511 logs.go:123] Gathering logs for kube-controller-manager [885389110d12] ...
I0207 11:35:01.648510    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 885389110d12"
I0207 11:35:01.713694    3511 logs.go:123] Gathering logs for dmesg ...
I0207 11:35:01.713714    3511 ssh_runner.go:195] Run: /bin/bash -c "sudo dmesg -PH -L=never --level warn,err,crit,alert,emerg | tail -n 400"
I0207 11:35:01.783416    3511 logs.go:123] Gathering logs for kube-apiserver [1b36eac9fea0] ...
I0207 11:35:01.783434    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 1b36eac9fea0"
I0207 11:35:01.847955    3511 logs.go:123] Gathering logs for kube-proxy [407d81ee6871] ...
I0207 11:35:01.847980    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 407d81ee6871"
I0207 11:35:01.900160    3511 logs.go:123] Gathering logs for storage-provisioner [0b85673627c7] ...
I0207 11:35:01.900174    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 0b85673627c7"
I0207 11:35:01.973042    3511 logs.go:123] Gathering logs for container status ...
I0207 11:35:01.973060    3511 ssh_runner.go:195] Run: /bin/bash -c "sudo `which crictl || echo crictl` ps -a || sudo docker ps -a"
I0207 11:35:04.659646    3511 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0207 11:35:04.686383    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-apiserver --format={{.ID}}
I0207 11:35:04.727886    3511 logs.go:284] 2 containers: [1b36eac9fea0 39d1988a2fab]
I0207 11:35:04.727991    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_etcd --format={{.ID}}
I0207 11:35:04.770867    3511 logs.go:284] 2 containers: [1eed8f4e884e 5cf1eff8b362]
I0207 11:35:04.771011    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_coredns --format={{.ID}}
I0207 11:35:04.817782    3511 logs.go:284] 2 containers: [06fb15ac4a32 b94028402ad3]
I0207 11:35:04.817936    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-scheduler --format={{.ID}}
I0207 11:35:04.869528    3511 logs.go:284] 2 containers: [1d1e9abd9836 ee7c2215045a]
I0207 11:35:04.869630    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-proxy --format={{.ID}}
I0207 11:35:04.938136    3511 logs.go:284] 2 containers: [1db52edcdad4 407d81ee6871]
I0207 11:35:04.938234    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-controller-manager --format={{.ID}}
I0207 11:35:04.970841    3511 logs.go:284] 2 containers: [e1bb8361945e 885389110d12]
I0207 11:35:04.970978    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kindnet --format={{.ID}}
I0207 11:35:04.999494    3511 logs.go:284] 0 containers: []
W0207 11:35:04.999511    3511 logs.go:286] No container was found matching "kindnet"
I0207 11:35:04.999593    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kubernetes-dashboard --format={{.ID}}
I0207 11:35:05.029073    3511 logs.go:284] 2 containers: [5631cbc21ffd f4b96dcbc9f2]
I0207 11:35:05.029162    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_controller_ingress --format={{.ID}}
I0207 11:35:05.063779    3511 logs.go:284] 1 containers: [8d50139a5311]
I0207 11:35:05.063872    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_storage-provisioner --format={{.ID}}
I0207 11:35:05.100398    3511 logs.go:284] 2 containers: [ec52b9ed7e21 0b85673627c7]
I0207 11:35:05.100428    3511 logs.go:123] Gathering logs for Docker ...
I0207 11:35:05.100441    3511 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u docker -u cri-docker -n 400"
I0207 11:35:05.166590    3511 logs.go:123] Gathering logs for describe nodes ...
I0207 11:35:05.166606    3511 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig"
W0207 11:35:05.293429    3511 logs.go:130] failed describe nodes: command: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig" /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig": Process exited with status 1
stdout:

stderr:
E0207 03:35:05.279492   11429 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:35:05.281115   11429 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:35:05.283080   11429 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:35:05.283647   11429 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:35:05.285527   11429 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
The connection to the server localhost:8443 was refused - did you specify the right host or port?
 output: 
** stderr ** 
E0207 03:35:05.279492   11429 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:35:05.281115   11429 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:35:05.283080   11429 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:35:05.283647   11429 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:35:05.285527   11429 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
The connection to the server localhost:8443 was refused - did you specify the right host or port?

** /stderr **
I0207 11:35:05.293444    3511 logs.go:123] Gathering logs for coredns [06fb15ac4a32] ...
I0207 11:35:05.293461    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 06fb15ac4a32"
I0207 11:35:05.352479    3511 logs.go:123] Gathering logs for coredns [b94028402ad3] ...
I0207 11:35:05.352496    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 b94028402ad3"
I0207 11:35:05.395707    3511 logs.go:123] Gathering logs for kube-scheduler [ee7c2215045a] ...
I0207 11:35:05.395723    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 ee7c2215045a"
I0207 11:35:05.449472    3511 logs.go:123] Gathering logs for kube-controller-manager [e1bb8361945e] ...
I0207 11:35:05.449487    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 e1bb8361945e"
I0207 11:35:05.531547    3511 logs.go:123] Gathering logs for kubernetes-dashboard [f4b96dcbc9f2] ...
I0207 11:35:05.531572    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 f4b96dcbc9f2"
I0207 11:35:05.626938    3511 logs.go:123] Gathering logs for controller_ingress [8d50139a5311] ...
I0207 11:35:05.626954    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 8d50139a5311"
I0207 11:35:05.721085    3511 logs.go:123] Gathering logs for container status ...
I0207 11:35:05.721106    3511 ssh_runner.go:195] Run: /bin/bash -c "sudo `which crictl || echo crictl` ps -a || sudo docker ps -a"
I0207 11:35:05.969919    3511 logs.go:123] Gathering logs for kubelet ...
I0207 11:35:05.969937    3511 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u kubelet -n 400"
I0207 11:35:06.053168    3511 logs.go:123] Gathering logs for kube-apiserver [1b36eac9fea0] ...
I0207 11:35:06.053196    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 1b36eac9fea0"
I0207 11:35:06.120143    3511 logs.go:123] Gathering logs for etcd [1eed8f4e884e] ...
I0207 11:35:06.120168    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 1eed8f4e884e"
I0207 11:35:06.288818    3511 logs.go:123] Gathering logs for kube-controller-manager [885389110d12] ...
I0207 11:35:06.288835    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 885389110d12"
I0207 11:35:06.353361    3511 logs.go:123] Gathering logs for dmesg ...
I0207 11:35:06.353384    3511 ssh_runner.go:195] Run: /bin/bash -c "sudo dmesg -PH -L=never --level warn,err,crit,alert,emerg | tail -n 400"
I0207 11:35:06.421011    3511 logs.go:123] Gathering logs for kube-apiserver [39d1988a2fab] ...
I0207 11:35:06.421027    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 39d1988a2fab"
I0207 11:35:06.475028    3511 logs.go:123] Gathering logs for kube-scheduler [1d1e9abd9836] ...
I0207 11:35:06.475042    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 1d1e9abd9836"
I0207 11:35:06.550415    3511 logs.go:123] Gathering logs for kube-proxy [1db52edcdad4] ...
I0207 11:35:06.550432    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 1db52edcdad4"
I0207 11:35:06.597973    3511 logs.go:123] Gathering logs for kube-proxy [407d81ee6871] ...
I0207 11:35:06.597987    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 407d81ee6871"
I0207 11:35:06.636300    3511 logs.go:123] Gathering logs for etcd [5cf1eff8b362] ...
I0207 11:35:06.636314    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 5cf1eff8b362"
I0207 11:35:06.720144    3511 logs.go:123] Gathering logs for kubernetes-dashboard [5631cbc21ffd] ...
I0207 11:35:06.720161    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 5631cbc21ffd"
I0207 11:35:06.782516    3511 logs.go:123] Gathering logs for storage-provisioner [ec52b9ed7e21] ...
I0207 11:35:06.782531    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 ec52b9ed7e21"
I0207 11:35:06.843798    3511 logs.go:123] Gathering logs for storage-provisioner [0b85673627c7] ...
I0207 11:35:06.843816    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 0b85673627c7"
I0207 11:35:09.434794    3511 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0207 11:35:09.458565    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-apiserver --format={{.ID}}
I0207 11:35:09.501362    3511 logs.go:284] 2 containers: [1b36eac9fea0 39d1988a2fab]
I0207 11:35:09.501476    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_etcd --format={{.ID}}
I0207 11:35:09.544844    3511 logs.go:284] 2 containers: [1eed8f4e884e 5cf1eff8b362]
I0207 11:35:09.544944    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_coredns --format={{.ID}}
I0207 11:35:09.604707    3511 logs.go:284] 2 containers: [06fb15ac4a32 b94028402ad3]
I0207 11:35:09.604818    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-scheduler --format={{.ID}}
I0207 11:35:09.669001    3511 logs.go:284] 2 containers: [1d1e9abd9836 ee7c2215045a]
I0207 11:35:09.669110    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-proxy --format={{.ID}}
I0207 11:35:09.717445    3511 logs.go:284] 2 containers: [1db52edcdad4 407d81ee6871]
I0207 11:35:09.717595    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-controller-manager --format={{.ID}}
I0207 11:35:09.748897    3511 logs.go:284] 2 containers: [e1bb8361945e 885389110d12]
I0207 11:35:09.748990    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kindnet --format={{.ID}}
I0207 11:35:09.785109    3511 logs.go:284] 0 containers: []
W0207 11:35:09.785126    3511 logs.go:286] No container was found matching "kindnet"
I0207 11:35:09.785200    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kubernetes-dashboard --format={{.ID}}
I0207 11:35:09.819310    3511 logs.go:284] 2 containers: [5631cbc21ffd f4b96dcbc9f2]
I0207 11:35:09.819409    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_controller_ingress --format={{.ID}}
I0207 11:35:09.853742    3511 logs.go:284] 1 containers: [8d50139a5311]
I0207 11:35:09.853840    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_storage-provisioner --format={{.ID}}
I0207 11:35:09.893133    3511 logs.go:284] 2 containers: [ec52b9ed7e21 0b85673627c7]
I0207 11:35:09.893169    3511 logs.go:123] Gathering logs for kube-apiserver [1b36eac9fea0] ...
I0207 11:35:09.893182    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 1b36eac9fea0"
I0207 11:35:09.945593    3511 logs.go:123] Gathering logs for coredns [b94028402ad3] ...
I0207 11:35:09.945608    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 b94028402ad3"
I0207 11:35:09.998859    3511 logs.go:123] Gathering logs for kube-proxy [407d81ee6871] ...
I0207 11:35:09.998877    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 407d81ee6871"
I0207 11:35:10.037931    3511 logs.go:123] Gathering logs for kube-controller-manager [885389110d12] ...
I0207 11:35:10.037946    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 885389110d12"
I0207 11:35:10.097900    3511 logs.go:123] Gathering logs for storage-provisioner [0b85673627c7] ...
I0207 11:35:10.097917    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 0b85673627c7"
I0207 11:35:10.169872    3511 logs.go:123] Gathering logs for kube-controller-manager [e1bb8361945e] ...
I0207 11:35:10.169887    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 e1bb8361945e"
I0207 11:35:10.278554    3511 logs.go:123] Gathering logs for Docker ...
I0207 11:35:10.278573    3511 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u docker -u cri-docker -n 400"
I0207 11:35:10.357769    3511 logs.go:123] Gathering logs for container status ...
I0207 11:35:10.357800    3511 ssh_runner.go:195] Run: /bin/bash -c "sudo `which crictl || echo crictl` ps -a || sudo docker ps -a"
I0207 11:35:10.578053    3511 logs.go:123] Gathering logs for kubernetes-dashboard [f4b96dcbc9f2] ...
I0207 11:35:10.578071    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 f4b96dcbc9f2"
I0207 11:35:10.644267    3511 logs.go:123] Gathering logs for dmesg ...
I0207 11:35:10.644282    3511 ssh_runner.go:195] Run: /bin/bash -c "sudo dmesg -PH -L=never --level warn,err,crit,alert,emerg | tail -n 400"
I0207 11:35:10.723396    3511 logs.go:123] Gathering logs for describe nodes ...
I0207 11:35:10.723427    3511 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig"
W0207 11:35:10.852380    3511 logs.go:130] failed describe nodes: command: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig" /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig": Process exited with status 1
stdout:

stderr:
E0207 03:35:10.836994   11819 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:35:10.838171   11819 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:35:10.840195   11819 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:35:10.840878   11819 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:35:10.842856   11819 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
The connection to the server localhost:8443 was refused - did you specify the right host or port?
 output: 
** stderr ** 
E0207 03:35:10.836994   11819 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:35:10.838171   11819 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:35:10.840195   11819 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:35:10.840878   11819 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:35:10.842856   11819 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
The connection to the server localhost:8443 was refused - did you specify the right host or port?

** /stderr **
I0207 11:35:10.852413    3511 logs.go:123] Gathering logs for etcd [1eed8f4e884e] ...
I0207 11:35:10.852430    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 1eed8f4e884e"
I0207 11:35:11.027598    3511 logs.go:123] Gathering logs for etcd [5cf1eff8b362] ...
I0207 11:35:11.027628    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 5cf1eff8b362"
I0207 11:35:11.105293    3511 logs.go:123] Gathering logs for coredns [06fb15ac4a32] ...
I0207 11:35:11.105309    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 06fb15ac4a32"
I0207 11:35:11.185015    3511 logs.go:123] Gathering logs for kube-proxy [1db52edcdad4] ...
I0207 11:35:11.185053    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 1db52edcdad4"
I0207 11:35:11.230845    3511 logs.go:123] Gathering logs for kubernetes-dashboard [5631cbc21ffd] ...
I0207 11:35:11.230860    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 5631cbc21ffd"
I0207 11:35:11.271710    3511 logs.go:123] Gathering logs for controller_ingress [8d50139a5311] ...
I0207 11:35:11.271731    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 8d50139a5311"
I0207 11:35:11.352095    3511 logs.go:123] Gathering logs for kubelet ...
I0207 11:35:11.352116    3511 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u kubelet -n 400"
I0207 11:35:11.452280    3511 logs.go:123] Gathering logs for kube-apiserver [39d1988a2fab] ...
I0207 11:35:11.452301    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 39d1988a2fab"
I0207 11:35:11.522654    3511 logs.go:123] Gathering logs for kube-scheduler [1d1e9abd9836] ...
I0207 11:35:11.522677    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 1d1e9abd9836"
I0207 11:35:11.606923    3511 logs.go:123] Gathering logs for kube-scheduler [ee7c2215045a] ...
I0207 11:35:11.606945    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 ee7c2215045a"
I0207 11:35:11.660003    3511 logs.go:123] Gathering logs for storage-provisioner [ec52b9ed7e21] ...
I0207 11:35:11.660019    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 ec52b9ed7e21"
I0207 11:35:14.209676    3511 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0207 11:35:14.235261    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-apiserver --format={{.ID}}
I0207 11:35:14.281927    3511 logs.go:284] 2 containers: [1b36eac9fea0 39d1988a2fab]
I0207 11:35:14.282046    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_etcd --format={{.ID}}
I0207 11:35:14.328276    3511 logs.go:284] 2 containers: [1eed8f4e884e 5cf1eff8b362]
I0207 11:35:14.328365    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_coredns --format={{.ID}}
I0207 11:35:14.365269    3511 logs.go:284] 2 containers: [06fb15ac4a32 b94028402ad3]
I0207 11:35:14.365370    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-scheduler --format={{.ID}}
I0207 11:35:14.396050    3511 logs.go:284] 2 containers: [1d1e9abd9836 ee7c2215045a]
I0207 11:35:14.396134    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-proxy --format={{.ID}}
I0207 11:35:14.425898    3511 logs.go:284] 2 containers: [1db52edcdad4 407d81ee6871]
I0207 11:35:14.426016    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-controller-manager --format={{.ID}}
I0207 11:35:14.458367    3511 logs.go:284] 2 containers: [e1bb8361945e 885389110d12]
I0207 11:35:14.458497    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kindnet --format={{.ID}}
I0207 11:35:14.485299    3511 logs.go:284] 0 containers: []
W0207 11:35:14.485332    3511 logs.go:286] No container was found matching "kindnet"
I0207 11:35:14.485438    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kubernetes-dashboard --format={{.ID}}
I0207 11:35:14.516359    3511 logs.go:284] 2 containers: [5631cbc21ffd f4b96dcbc9f2]
I0207 11:35:14.516454    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_controller_ingress --format={{.ID}}
I0207 11:35:14.558242    3511 logs.go:284] 1 containers: [8d50139a5311]
I0207 11:35:14.558346    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_storage-provisioner --format={{.ID}}
I0207 11:35:14.605253    3511 logs.go:284] 2 containers: [ec52b9ed7e21 0b85673627c7]
I0207 11:35:14.605285    3511 logs.go:123] Gathering logs for coredns [06fb15ac4a32] ...
I0207 11:35:14.605320    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 06fb15ac4a32"
I0207 11:35:14.657868    3511 logs.go:123] Gathering logs for coredns [b94028402ad3] ...
I0207 11:35:14.657884    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 b94028402ad3"
I0207 11:35:14.701850    3511 logs.go:123] Gathering logs for kube-proxy [1db52edcdad4] ...
I0207 11:35:14.701878    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 1db52edcdad4"
I0207 11:35:14.745428    3511 logs.go:123] Gathering logs for kube-proxy [407d81ee6871] ...
I0207 11:35:14.745488    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 407d81ee6871"
I0207 11:35:14.779976    3511 logs.go:123] Gathering logs for kube-controller-manager [885389110d12] ...
I0207 11:35:14.779991    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 885389110d12"
I0207 11:35:14.839177    3511 logs.go:123] Gathering logs for kubelet ...
I0207 11:35:14.839193    3511 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u kubelet -n 400"
I0207 11:35:14.932259    3511 logs.go:123] Gathering logs for describe nodes ...
I0207 11:35:14.932275    3511 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig"
W0207 11:35:15.078217    3511 logs.go:130] failed describe nodes: command: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig" /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig": Process exited with status 1
stdout:

stderr:
E0207 03:35:15.061230   12056 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:35:15.061980   12056 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:35:15.064115   12056 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:35:15.066941   12056 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:35:15.068200   12056 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
The connection to the server localhost:8443 was refused - did you specify the right host or port?
 output: 
** stderr ** 
E0207 03:35:15.061230   12056 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:35:15.061980   12056 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:35:15.064115   12056 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:35:15.066941   12056 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:35:15.068200   12056 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
The connection to the server localhost:8443 was refused - did you specify the right host or port?

** /stderr **
I0207 11:35:15.078272    3511 logs.go:123] Gathering logs for etcd [5cf1eff8b362] ...
I0207 11:35:15.078314    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 5cf1eff8b362"
I0207 11:35:15.173853    3511 logs.go:123] Gathering logs for storage-provisioner [0b85673627c7] ...
I0207 11:35:15.173868    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 0b85673627c7"
I0207 11:35:15.237897    3511 logs.go:123] Gathering logs for Docker ...
I0207 11:35:15.237912    3511 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u docker -u cri-docker -n 400"
I0207 11:35:15.300260    3511 logs.go:123] Gathering logs for kubernetes-dashboard [5631cbc21ffd] ...
I0207 11:35:15.300275    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 5631cbc21ffd"
I0207 11:35:15.335112    3511 logs.go:123] Gathering logs for kubernetes-dashboard [f4b96dcbc9f2] ...
I0207 11:35:15.335129    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 f4b96dcbc9f2"
I0207 11:35:15.420067    3511 logs.go:123] Gathering logs for controller_ingress [8d50139a5311] ...
I0207 11:35:15.420082    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 8d50139a5311"
I0207 11:35:15.487322    3511 logs.go:123] Gathering logs for dmesg ...
I0207 11:35:15.487337    3511 ssh_runner.go:195] Run: /bin/bash -c "sudo dmesg -PH -L=never --level warn,err,crit,alert,emerg | tail -n 400"
I0207 11:35:15.562111    3511 logs.go:123] Gathering logs for kube-apiserver [1b36eac9fea0] ...
I0207 11:35:15.562127    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 1b36eac9fea0"
I0207 11:35:15.620816    3511 logs.go:123] Gathering logs for kube-scheduler [1d1e9abd9836] ...
I0207 11:35:15.620836    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 1d1e9abd9836"
I0207 11:35:15.698052    3511 logs.go:123] Gathering logs for kube-controller-manager [e1bb8361945e] ...
I0207 11:35:15.698071    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 e1bb8361945e"
I0207 11:35:15.805464    3511 logs.go:123] Gathering logs for kube-apiserver [39d1988a2fab] ...
I0207 11:35:15.805478    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 39d1988a2fab"
I0207 11:35:15.861097    3511 logs.go:123] Gathering logs for etcd [1eed8f4e884e] ...
I0207 11:35:15.861112    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 1eed8f4e884e"
I0207 11:35:16.048188    3511 logs.go:123] Gathering logs for kube-scheduler [ee7c2215045a] ...
I0207 11:35:16.048212    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 ee7c2215045a"
I0207 11:35:16.129190    3511 logs.go:123] Gathering logs for storage-provisioner [ec52b9ed7e21] ...
I0207 11:35:16.129212    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 ec52b9ed7e21"
I0207 11:35:16.172927    3511 logs.go:123] Gathering logs for container status ...
I0207 11:35:16.172942    3511 ssh_runner.go:195] Run: /bin/bash -c "sudo `which crictl || echo crictl` ps -a || sudo docker ps -a"
I0207 11:35:18.837729    3511 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0207 11:35:18.881156    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-apiserver --format={{.ID}}
I0207 11:35:18.945047    3511 logs.go:284] 2 containers: [1b36eac9fea0 39d1988a2fab]
I0207 11:35:18.945152    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_etcd --format={{.ID}}
I0207 11:35:18.982505    3511 logs.go:284] 2 containers: [1eed8f4e884e 5cf1eff8b362]
I0207 11:35:18.982606    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_coredns --format={{.ID}}
I0207 11:35:19.013302    3511 logs.go:284] 2 containers: [06fb15ac4a32 b94028402ad3]
I0207 11:35:19.013397    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-scheduler --format={{.ID}}
I0207 11:35:19.069992    3511 logs.go:284] 2 containers: [1d1e9abd9836 ee7c2215045a]
I0207 11:35:19.070118    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-proxy --format={{.ID}}
I0207 11:35:19.122705    3511 logs.go:284] 2 containers: [1db52edcdad4 407d81ee6871]
I0207 11:35:19.122880    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-controller-manager --format={{.ID}}
I0207 11:35:19.166489    3511 logs.go:284] 2 containers: [e1bb8361945e 885389110d12]
I0207 11:35:19.166623    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kindnet --format={{.ID}}
I0207 11:35:19.195584    3511 logs.go:284] 0 containers: []
W0207 11:35:19.195630    3511 logs.go:286] No container was found matching "kindnet"
I0207 11:35:19.195713    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kubernetes-dashboard --format={{.ID}}
I0207 11:35:19.226881    3511 logs.go:284] 2 containers: [5631cbc21ffd f4b96dcbc9f2]
I0207 11:35:19.226974    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_controller_ingress --format={{.ID}}
I0207 11:35:19.277728    3511 logs.go:284] 1 containers: [8d50139a5311]
I0207 11:35:19.277826    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_storage-provisioner --format={{.ID}}
I0207 11:35:19.315839    3511 logs.go:284] 2 containers: [ec52b9ed7e21 0b85673627c7]
I0207 11:35:19.315865    3511 logs.go:123] Gathering logs for etcd [5cf1eff8b362] ...
I0207 11:35:19.315885    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 5cf1eff8b362"
I0207 11:35:19.377699    3511 logs.go:123] Gathering logs for coredns [b94028402ad3] ...
I0207 11:35:19.377715    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 b94028402ad3"
I0207 11:35:19.423061    3511 logs.go:123] Gathering logs for kube-scheduler [ee7c2215045a] ...
I0207 11:35:19.423077    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 ee7c2215045a"
I0207 11:35:19.491814    3511 logs.go:123] Gathering logs for Docker ...
I0207 11:35:19.491835    3511 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u docker -u cri-docker -n 400"
I0207 11:35:19.556143    3511 logs.go:123] Gathering logs for kubelet ...
I0207 11:35:19.556163    3511 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u kubelet -n 400"
I0207 11:35:19.640150    3511 logs.go:123] Gathering logs for dmesg ...
I0207 11:35:19.640169    3511 ssh_runner.go:195] Run: /bin/bash -c "sudo dmesg -PH -L=never --level warn,err,crit,alert,emerg | tail -n 400"
I0207 11:35:19.714500    3511 logs.go:123] Gathering logs for kube-controller-manager [e1bb8361945e] ...
I0207 11:35:19.714517    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 e1bb8361945e"
I0207 11:35:19.792973    3511 logs.go:123] Gathering logs for storage-provisioner [ec52b9ed7e21] ...
I0207 11:35:19.792995    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 ec52b9ed7e21"
I0207 11:35:19.833342    3511 logs.go:123] Gathering logs for kubernetes-dashboard [f4b96dcbc9f2] ...
I0207 11:35:19.833359    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 f4b96dcbc9f2"
I0207 11:35:19.909458    3511 logs.go:123] Gathering logs for container status ...
I0207 11:35:19.909489    3511 ssh_runner.go:195] Run: /bin/bash -c "sudo `which crictl || echo crictl` ps -a || sudo docker ps -a"
I0207 11:35:20.105714    3511 logs.go:123] Gathering logs for kube-apiserver [1b36eac9fea0] ...
I0207 11:35:20.105739    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 1b36eac9fea0"
I0207 11:35:20.209531    3511 logs.go:123] Gathering logs for kube-apiserver [39d1988a2fab] ...
I0207 11:35:20.209556    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 39d1988a2fab"
I0207 11:35:20.279978    3511 logs.go:123] Gathering logs for coredns [06fb15ac4a32] ...
I0207 11:35:20.279992    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 06fb15ac4a32"
I0207 11:35:20.355822    3511 logs.go:123] Gathering logs for kube-scheduler [1d1e9abd9836] ...
I0207 11:35:20.355842    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 1d1e9abd9836"
I0207 11:35:20.458525    3511 logs.go:123] Gathering logs for kube-proxy [1db52edcdad4] ...
I0207 11:35:20.458544    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 1db52edcdad4"
I0207 11:35:20.507552    3511 logs.go:123] Gathering logs for kubernetes-dashboard [5631cbc21ffd] ...
I0207 11:35:20.507567    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 5631cbc21ffd"
I0207 11:35:20.562750    3511 logs.go:123] Gathering logs for describe nodes ...
I0207 11:35:20.562786    3511 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig"
W0207 11:35:20.696030    3511 logs.go:130] failed describe nodes: command: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig" /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig": Process exited with status 1
stdout:

stderr:
E0207 03:35:20.679888   12441 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:35:20.680821   12441 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:35:20.682803   12441 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:35:20.683538   12441 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:35:20.685571   12441 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
The connection to the server localhost:8443 was refused - did you specify the right host or port?
 output: 
** stderr ** 
E0207 03:35:20.679888   12441 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:35:20.680821   12441 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:35:20.682803   12441 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:35:20.683538   12441 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:35:20.685571   12441 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
The connection to the server localhost:8443 was refused - did you specify the right host or port?

** /stderr **
I0207 11:35:20.696045    3511 logs.go:123] Gathering logs for etcd [1eed8f4e884e] ...
I0207 11:35:20.696059    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 1eed8f4e884e"
I0207 11:35:20.977511    3511 logs.go:123] Gathering logs for kube-proxy [407d81ee6871] ...
I0207 11:35:20.977541    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 407d81ee6871"
I0207 11:35:21.032149    3511 logs.go:123] Gathering logs for kube-controller-manager [885389110d12] ...
I0207 11:35:21.032177    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 885389110d12"
I0207 11:35:21.121476    3511 logs.go:123] Gathering logs for controller_ingress [8d50139a5311] ...
I0207 11:35:21.121495    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 8d50139a5311"
I0207 11:35:21.215466    3511 logs.go:123] Gathering logs for storage-provisioner [0b85673627c7] ...
I0207 11:35:21.215482    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 0b85673627c7"
I0207 11:35:23.786952    3511 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0207 11:35:23.818868    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-apiserver --format={{.ID}}
I0207 11:35:23.865214    3511 logs.go:284] 2 containers: [1b36eac9fea0 39d1988a2fab]
I0207 11:35:23.865317    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_etcd --format={{.ID}}
I0207 11:35:23.938645    3511 logs.go:284] 2 containers: [1eed8f4e884e 5cf1eff8b362]
I0207 11:35:23.938770    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_coredns --format={{.ID}}
I0207 11:35:23.985847    3511 logs.go:284] 2 containers: [06fb15ac4a32 b94028402ad3]
I0207 11:35:23.985942    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-scheduler --format={{.ID}}
I0207 11:35:24.041345    3511 logs.go:284] 2 containers: [1d1e9abd9836 ee7c2215045a]
I0207 11:35:24.041448    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-proxy --format={{.ID}}
I0207 11:35:24.071543    3511 logs.go:284] 2 containers: [1db52edcdad4 407d81ee6871]
I0207 11:35:24.071681    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-controller-manager --format={{.ID}}
I0207 11:35:24.101104    3511 logs.go:284] 2 containers: [e1bb8361945e 885389110d12]
I0207 11:35:24.101193    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kindnet --format={{.ID}}
I0207 11:35:24.132602    3511 logs.go:284] 0 containers: []
W0207 11:35:24.132619    3511 logs.go:286] No container was found matching "kindnet"
I0207 11:35:24.132699    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kubernetes-dashboard --format={{.ID}}
I0207 11:35:24.165388    3511 logs.go:284] 2 containers: [5631cbc21ffd f4b96dcbc9f2]
I0207 11:35:24.165516    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_controller_ingress --format={{.ID}}
I0207 11:35:24.198126    3511 logs.go:284] 1 containers: [8d50139a5311]
I0207 11:35:24.198225    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_storage-provisioner --format={{.ID}}
I0207 11:35:24.227741    3511 logs.go:284] 2 containers: [ec52b9ed7e21 0b85673627c7]
I0207 11:35:24.227787    3511 logs.go:123] Gathering logs for describe nodes ...
I0207 11:35:24.227830    3511 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig"
W0207 11:35:24.348117    3511 logs.go:130] failed describe nodes: command: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig" /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig": Process exited with status 1
stdout:

stderr:
E0207 03:35:24.331164   12625 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:35:24.332629   12625 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:35:24.335435   12625 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:35:24.336952   12625 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:35:24.337746   12625 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
The connection to the server localhost:8443 was refused - did you specify the right host or port?
 output: 
** stderr ** 
E0207 03:35:24.331164   12625 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:35:24.332629   12625 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:35:24.335435   12625 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:35:24.336952   12625 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:35:24.337746   12625 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
The connection to the server localhost:8443 was refused - did you specify the right host or port?

** /stderr **
I0207 11:35:24.348134    3511 logs.go:123] Gathering logs for kube-apiserver [1b36eac9fea0] ...
I0207 11:35:24.348150    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 1b36eac9fea0"
I0207 11:35:24.408302    3511 logs.go:123] Gathering logs for etcd [1eed8f4e884e] ...
I0207 11:35:24.408327    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 1eed8f4e884e"
I0207 11:35:24.585392    3511 logs.go:123] Gathering logs for kubernetes-dashboard [f4b96dcbc9f2] ...
I0207 11:35:24.585408    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 f4b96dcbc9f2"
I0207 11:35:24.684997    3511 logs.go:123] Gathering logs for coredns [06fb15ac4a32] ...
I0207 11:35:24.685014    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 06fb15ac4a32"
I0207 11:35:24.779999    3511 logs.go:123] Gathering logs for coredns [b94028402ad3] ...
I0207 11:35:24.780031    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 b94028402ad3"
I0207 11:35:24.844099    3511 logs.go:123] Gathering logs for kube-scheduler [1d1e9abd9836] ...
I0207 11:35:24.844117    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 1d1e9abd9836"
I0207 11:35:24.950749    3511 logs.go:123] Gathering logs for kube-proxy [1db52edcdad4] ...
I0207 11:35:24.950764    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 1db52edcdad4"
I0207 11:35:24.999686    3511 logs.go:123] Gathering logs for kube-controller-manager [e1bb8361945e] ...
I0207 11:35:24.999702    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 e1bb8361945e"
I0207 11:35:25.087091    3511 logs.go:123] Gathering logs for kube-controller-manager [885389110d12] ...
I0207 11:35:25.087106    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 885389110d12"
I0207 11:35:25.148834    3511 logs.go:123] Gathering logs for storage-provisioner [0b85673627c7] ...
I0207 11:35:25.148861    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 0b85673627c7"
I0207 11:35:25.224290    3511 logs.go:123] Gathering logs for dmesg ...
I0207 11:35:25.224306    3511 ssh_runner.go:195] Run: /bin/bash -c "sudo dmesg -PH -L=never --level warn,err,crit,alert,emerg | tail -n 400"
I0207 11:35:25.296447    3511 logs.go:123] Gathering logs for kube-apiserver [39d1988a2fab] ...
I0207 11:35:25.296463    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 39d1988a2fab"
I0207 11:35:25.348103    3511 logs.go:123] Gathering logs for etcd [5cf1eff8b362] ...
I0207 11:35:25.348126    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 5cf1eff8b362"
I0207 11:35:25.434179    3511 logs.go:123] Gathering logs for kubernetes-dashboard [5631cbc21ffd] ...
I0207 11:35:25.434204    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 5631cbc21ffd"
I0207 11:35:25.473372    3511 logs.go:123] Gathering logs for controller_ingress [8d50139a5311] ...
I0207 11:35:25.473389    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 8d50139a5311"
I0207 11:35:25.545399    3511 logs.go:123] Gathering logs for Docker ...
I0207 11:35:25.545415    3511 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u docker -u cri-docker -n 400"
I0207 11:35:25.633473    3511 logs.go:123] Gathering logs for kubelet ...
I0207 11:35:25.633490    3511 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u kubelet -n 400"
I0207 11:35:25.754165    3511 logs.go:123] Gathering logs for kube-scheduler [ee7c2215045a] ...
I0207 11:35:25.754192    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 ee7c2215045a"
I0207 11:35:25.853903    3511 logs.go:123] Gathering logs for kube-proxy [407d81ee6871] ...
I0207 11:35:25.853920    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 407d81ee6871"
I0207 11:35:25.895327    3511 logs.go:123] Gathering logs for storage-provisioner [ec52b9ed7e21] ...
I0207 11:35:25.895342    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 ec52b9ed7e21"
I0207 11:35:25.937024    3511 logs.go:123] Gathering logs for container status ...
I0207 11:35:25.937058    3511 ssh_runner.go:195] Run: /bin/bash -c "sudo `which crictl || echo crictl` ps -a || sudo docker ps -a"
I0207 11:35:28.596259    3511 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0207 11:35:28.625707    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-apiserver --format={{.ID}}
I0207 11:35:28.677195    3511 logs.go:284] 2 containers: [1b36eac9fea0 39d1988a2fab]
I0207 11:35:28.677289    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_etcd --format={{.ID}}
I0207 11:35:28.721585    3511 logs.go:284] 2 containers: [1eed8f4e884e 5cf1eff8b362]
I0207 11:35:28.721686    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_coredns --format={{.ID}}
I0207 11:35:28.776933    3511 logs.go:284] 2 containers: [06fb15ac4a32 b94028402ad3]
I0207 11:35:28.777056    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-scheduler --format={{.ID}}
I0207 11:35:28.831473    3511 logs.go:284] 2 containers: [1d1e9abd9836 ee7c2215045a]
I0207 11:35:28.831584    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-proxy --format={{.ID}}
I0207 11:35:28.912701    3511 logs.go:284] 2 containers: [1db52edcdad4 407d81ee6871]
I0207 11:35:28.912806    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-controller-manager --format={{.ID}}
I0207 11:35:28.946849    3511 logs.go:284] 2 containers: [e1bb8361945e 885389110d12]
I0207 11:35:28.946951    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kindnet --format={{.ID}}
I0207 11:35:29.003845    3511 logs.go:284] 0 containers: []
W0207 11:35:29.003878    3511 logs.go:286] No container was found matching "kindnet"
I0207 11:35:29.004006    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_storage-provisioner --format={{.ID}}
I0207 11:35:29.051810    3511 logs.go:284] 2 containers: [ec52b9ed7e21 0b85673627c7]
I0207 11:35:29.051903    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kubernetes-dashboard --format={{.ID}}
I0207 11:35:29.091053    3511 logs.go:284] 2 containers: [5631cbc21ffd f4b96dcbc9f2]
I0207 11:35:29.091145    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_controller_ingress --format={{.ID}}
I0207 11:35:29.130666    3511 logs.go:284] 1 containers: [8d50139a5311]
I0207 11:35:29.130717    3511 logs.go:123] Gathering logs for etcd [1eed8f4e884e] ...
I0207 11:35:29.130730    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 1eed8f4e884e"
I0207 11:35:29.300921    3511 logs.go:123] Gathering logs for coredns [06fb15ac4a32] ...
I0207 11:35:29.300957    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 06fb15ac4a32"
I0207 11:35:29.355443    3511 logs.go:123] Gathering logs for kube-controller-manager [e1bb8361945e] ...
I0207 11:35:29.355456    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 e1bb8361945e"
I0207 11:35:29.449431    3511 logs.go:123] Gathering logs for kubernetes-dashboard [f4b96dcbc9f2] ...
I0207 11:35:29.449447    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 f4b96dcbc9f2"
I0207 11:35:29.523594    3511 logs.go:123] Gathering logs for describe nodes ...
I0207 11:35:29.523614    3511 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig"
W0207 11:35:29.699545    3511 logs.go:130] failed describe nodes: command: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig" /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig": Process exited with status 1
stdout:

stderr:
E0207 03:35:29.676368   12939 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:35:29.677682   12939 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:35:29.678601   12939 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:35:29.680448   12939 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:35:29.685649   12939 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
The connection to the server localhost:8443 was refused - did you specify the right host or port?
 output: 
** stderr ** 
E0207 03:35:29.676368   12939 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:35:29.677682   12939 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:35:29.678601   12939 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:35:29.680448   12939 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:35:29.685649   12939 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
The connection to the server localhost:8443 was refused - did you specify the right host or port?

** /stderr **
I0207 11:35:29.699563    3511 logs.go:123] Gathering logs for kube-apiserver [1b36eac9fea0] ...
I0207 11:35:29.699578    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 1b36eac9fea0"
I0207 11:35:29.770179    3511 logs.go:123] Gathering logs for etcd [5cf1eff8b362] ...
I0207 11:35:29.770195    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 5cf1eff8b362"
I0207 11:35:29.833662    3511 logs.go:123] Gathering logs for coredns [b94028402ad3] ...
I0207 11:35:29.833678    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 b94028402ad3"
I0207 11:35:29.876912    3511 logs.go:123] Gathering logs for kube-proxy [407d81ee6871] ...
I0207 11:35:29.876934    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 407d81ee6871"
I0207 11:35:29.920597    3511 logs.go:123] Gathering logs for kube-controller-manager [885389110d12] ...
I0207 11:35:29.920621    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 885389110d12"
I0207 11:35:29.996093    3511 logs.go:123] Gathering logs for storage-provisioner [ec52b9ed7e21] ...
I0207 11:35:29.996109    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 ec52b9ed7e21"
I0207 11:35:30.038132    3511 logs.go:123] Gathering logs for controller_ingress [8d50139a5311] ...
I0207 11:35:30.038148    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 8d50139a5311"
I0207 11:35:30.103412    3511 logs.go:123] Gathering logs for dmesg ...
I0207 11:35:30.103428    3511 ssh_runner.go:195] Run: /bin/bash -c "sudo dmesg -PH -L=never --level warn,err,crit,alert,emerg | tail -n 400"
I0207 11:35:30.187663    3511 logs.go:123] Gathering logs for container status ...
I0207 11:35:30.187679    3511 ssh_runner.go:195] Run: /bin/bash -c "sudo `which crictl || echo crictl` ps -a || sudo docker ps -a"
I0207 11:35:30.342844    3511 logs.go:123] Gathering logs for kube-apiserver [39d1988a2fab] ...
I0207 11:35:30.342860    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 39d1988a2fab"
I0207 11:35:30.388927    3511 logs.go:123] Gathering logs for kube-scheduler [ee7c2215045a] ...
I0207 11:35:30.388942    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 ee7c2215045a"
I0207 11:35:30.460548    3511 logs.go:123] Gathering logs for kube-proxy [1db52edcdad4] ...
I0207 11:35:30.460566    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 1db52edcdad4"
I0207 11:35:30.516078    3511 logs.go:123] Gathering logs for kubelet ...
I0207 11:35:30.516099    3511 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u kubelet -n 400"
I0207 11:35:30.628460    3511 logs.go:123] Gathering logs for storage-provisioner [0b85673627c7] ...
I0207 11:35:30.628476    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 0b85673627c7"
I0207 11:35:30.695195    3511 logs.go:123] Gathering logs for kubernetes-dashboard [5631cbc21ffd] ...
I0207 11:35:30.695210    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 5631cbc21ffd"
I0207 11:35:30.742296    3511 logs.go:123] Gathering logs for Docker ...
I0207 11:35:30.742310    3511 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u docker -u cri-docker -n 400"
I0207 11:35:30.808246    3511 logs.go:123] Gathering logs for kube-scheduler [1d1e9abd9836] ...
I0207 11:35:30.808265    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 1d1e9abd9836"
I0207 11:35:33.381440    3511 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0207 11:35:33.408081    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-apiserver --format={{.ID}}
I0207 11:35:33.452481    3511 logs.go:284] 2 containers: [1b36eac9fea0 39d1988a2fab]
I0207 11:35:33.452579    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_etcd --format={{.ID}}
I0207 11:35:33.494249    3511 logs.go:284] 2 containers: [1eed8f4e884e 5cf1eff8b362]
I0207 11:35:33.494359    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_coredns --format={{.ID}}
I0207 11:35:33.532826    3511 logs.go:284] 2 containers: [06fb15ac4a32 b94028402ad3]
I0207 11:35:33.532954    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-scheduler --format={{.ID}}
I0207 11:35:33.577447    3511 logs.go:284] 2 containers: [1d1e9abd9836 ee7c2215045a]
I0207 11:35:33.577609    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-proxy --format={{.ID}}
I0207 11:35:33.634401    3511 logs.go:284] 2 containers: [1db52edcdad4 407d81ee6871]
I0207 11:35:33.634518    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-controller-manager --format={{.ID}}
I0207 11:35:33.665860    3511 logs.go:284] 2 containers: [e1bb8361945e 885389110d12]
I0207 11:35:33.665960    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kindnet --format={{.ID}}
I0207 11:35:33.696654    3511 logs.go:284] 0 containers: []
W0207 11:35:33.696671    3511 logs.go:286] No container was found matching "kindnet"
I0207 11:35:33.696783    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kubernetes-dashboard --format={{.ID}}
I0207 11:35:33.730726    3511 logs.go:284] 2 containers: [5631cbc21ffd f4b96dcbc9f2]
I0207 11:35:33.730828    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_controller_ingress --format={{.ID}}
I0207 11:35:33.760449    3511 logs.go:284] 1 containers: [8d50139a5311]
I0207 11:35:33.760544    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_storage-provisioner --format={{.ID}}
I0207 11:35:33.789856    3511 logs.go:284] 2 containers: [ec52b9ed7e21 0b85673627c7]
I0207 11:35:33.789884    3511 logs.go:123] Gathering logs for kubelet ...
I0207 11:35:33.789897    3511 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u kubelet -n 400"
I0207 11:35:33.864482    3511 logs.go:123] Gathering logs for dmesg ...
I0207 11:35:33.864500    3511 ssh_runner.go:195] Run: /bin/bash -c "sudo dmesg -PH -L=never --level warn,err,crit,alert,emerg | tail -n 400"
I0207 11:35:33.953882    3511 logs.go:123] Gathering logs for etcd [5cf1eff8b362] ...
I0207 11:35:33.953897    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 5cf1eff8b362"
I0207 11:35:34.023668    3511 logs.go:123] Gathering logs for kube-scheduler [ee7c2215045a] ...
I0207 11:35:34.023683    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 ee7c2215045a"
I0207 11:35:34.100229    3511 logs.go:123] Gathering logs for kube-proxy [407d81ee6871] ...
I0207 11:35:34.100244    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 407d81ee6871"
I0207 11:35:34.132198    3511 logs.go:123] Gathering logs for kube-apiserver [1b36eac9fea0] ...
I0207 11:35:34.132216    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 1b36eac9fea0"
I0207 11:35:34.190864    3511 logs.go:123] Gathering logs for kube-apiserver [39d1988a2fab] ...
I0207 11:35:34.190882    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 39d1988a2fab"
I0207 11:35:34.237957    3511 logs.go:123] Gathering logs for kubernetes-dashboard [5631cbc21ffd] ...
I0207 11:35:34.237987    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 5631cbc21ffd"
I0207 11:35:34.275652    3511 logs.go:123] Gathering logs for storage-provisioner [ec52b9ed7e21] ...
I0207 11:35:34.275666    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 ec52b9ed7e21"
I0207 11:35:34.308281    3511 logs.go:123] Gathering logs for container status ...
I0207 11:35:34.308295    3511 ssh_runner.go:195] Run: /bin/bash -c "sudo `which crictl || echo crictl` ps -a || sudo docker ps -a"
I0207 11:35:34.506651    3511 logs.go:123] Gathering logs for describe nodes ...
I0207 11:35:34.506668    3511 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig"
W0207 11:35:34.646638    3511 logs.go:130] failed describe nodes: command: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig" /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig": Process exited with status 1
stdout:

stderr:
E0207 03:35:34.632074   13296 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:35:34.633194   13296 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:35:34.634216   13296 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:35:34.636050   13296 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:35:34.637140   13296 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
The connection to the server localhost:8443 was refused - did you specify the right host or port?
 output: 
** stderr ** 
E0207 03:35:34.632074   13296 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:35:34.633194   13296 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:35:34.634216   13296 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:35:34.636050   13296 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:35:34.637140   13296 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
The connection to the server localhost:8443 was refused - did you specify the right host or port?

** /stderr **
I0207 11:35:34.646649    3511 logs.go:123] Gathering logs for etcd [1eed8f4e884e] ...
I0207 11:35:34.646662    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 1eed8f4e884e"
I0207 11:35:34.789895    3511 logs.go:123] Gathering logs for coredns [06fb15ac4a32] ...
I0207 11:35:34.789911    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 06fb15ac4a32"
I0207 11:35:34.846704    3511 logs.go:123] Gathering logs for kube-scheduler [1d1e9abd9836] ...
I0207 11:35:34.846718    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 1d1e9abd9836"
I0207 11:35:34.938342    3511 logs.go:123] Gathering logs for kube-proxy [1db52edcdad4] ...
I0207 11:35:34.938356    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 1db52edcdad4"
I0207 11:35:34.981034    3511 logs.go:123] Gathering logs for kube-controller-manager [e1bb8361945e] ...
I0207 11:35:34.981058    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 e1bb8361945e"
I0207 11:35:35.076458    3511 logs.go:123] Gathering logs for kubernetes-dashboard [f4b96dcbc9f2] ...
I0207 11:35:35.076473    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 f4b96dcbc9f2"
I0207 11:35:35.153412    3511 logs.go:123] Gathering logs for controller_ingress [8d50139a5311] ...
I0207 11:35:35.153432    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 8d50139a5311"
I0207 11:35:35.228821    3511 logs.go:123] Gathering logs for coredns [b94028402ad3] ...
I0207 11:35:35.228835    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 b94028402ad3"
I0207 11:35:35.286718    3511 logs.go:123] Gathering logs for kube-controller-manager [885389110d12] ...
I0207 11:35:35.286733    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 885389110d12"
I0207 11:35:35.352280    3511 logs.go:123] Gathering logs for storage-provisioner [0b85673627c7] ...
I0207 11:35:35.352297    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 0b85673627c7"
I0207 11:35:35.435215    3511 logs.go:123] Gathering logs for Docker ...
I0207 11:35:35.435234    3511 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u docker -u cri-docker -n 400"
I0207 11:35:38.021401    3511 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0207 11:35:38.042643    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-apiserver --format={{.ID}}
I0207 11:35:38.071759    3511 logs.go:284] 2 containers: [1b36eac9fea0 39d1988a2fab]
I0207 11:35:38.071870    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_etcd --format={{.ID}}
I0207 11:35:38.103109    3511 logs.go:284] 2 containers: [1eed8f4e884e 5cf1eff8b362]
I0207 11:35:38.103208    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_coredns --format={{.ID}}
I0207 11:35:38.139426    3511 logs.go:284] 2 containers: [06fb15ac4a32 b94028402ad3]
I0207 11:35:38.139527    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-scheduler --format={{.ID}}
I0207 11:35:38.171689    3511 logs.go:284] 2 containers: [1d1e9abd9836 ee7c2215045a]
I0207 11:35:38.171906    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-proxy --format={{.ID}}
I0207 11:35:38.204446    3511 logs.go:284] 2 containers: [1db52edcdad4 407d81ee6871]
I0207 11:35:38.204573    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-controller-manager --format={{.ID}}
I0207 11:35:38.230715    3511 logs.go:284] 2 containers: [e1bb8361945e 885389110d12]
I0207 11:35:38.230808    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kindnet --format={{.ID}}
I0207 11:35:38.276318    3511 logs.go:284] 0 containers: []
W0207 11:35:38.276336    3511 logs.go:286] No container was found matching "kindnet"
I0207 11:35:38.276466    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kubernetes-dashboard --format={{.ID}}
I0207 11:35:38.330855    3511 logs.go:284] 2 containers: [5631cbc21ffd f4b96dcbc9f2]
I0207 11:35:38.330949    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_controller_ingress --format={{.ID}}
I0207 11:35:38.359786    3511 logs.go:284] 1 containers: [8d50139a5311]
I0207 11:35:38.359888    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_storage-provisioner --format={{.ID}}
I0207 11:35:38.393720    3511 logs.go:284] 2 containers: [ec52b9ed7e21 0b85673627c7]
I0207 11:35:38.393749    3511 logs.go:123] Gathering logs for etcd [5cf1eff8b362] ...
I0207 11:35:38.393762    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 5cf1eff8b362"
I0207 11:35:38.478503    3511 logs.go:123] Gathering logs for coredns [06fb15ac4a32] ...
I0207 11:35:38.478522    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 06fb15ac4a32"
I0207 11:35:38.546065    3511 logs.go:123] Gathering logs for kube-proxy [1db52edcdad4] ...
I0207 11:35:38.546083    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 1db52edcdad4"
I0207 11:35:38.611435    3511 logs.go:123] Gathering logs for kube-controller-manager [885389110d12] ...
I0207 11:35:38.611452    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 885389110d12"
I0207 11:35:38.696767    3511 logs.go:123] Gathering logs for Docker ...
I0207 11:35:38.696782    3511 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u docker -u cri-docker -n 400"
I0207 11:35:38.760018    3511 logs.go:123] Gathering logs for describe nodes ...
I0207 11:35:38.760033    3511 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig"
W0207 11:35:38.924735    3511 logs.go:130] failed describe nodes: command: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig" /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig": Process exited with status 1
stdout:

stderr:
E0207 03:35:38.910079   13543 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:35:38.911290   13543 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:35:38.913242   13543 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:35:38.914105   13543 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:35:38.916298   13543 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
The connection to the server localhost:8443 was refused - did you specify the right host or port?
 output: 
** stderr ** 
E0207 03:35:38.910079   13543 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:35:38.911290   13543 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:35:38.913242   13543 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:35:38.914105   13543 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:35:38.916298   13543 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
The connection to the server localhost:8443 was refused - did you specify the right host or port?

** /stderr **
I0207 11:35:38.924924    3511 logs.go:123] Gathering logs for kube-apiserver [1b36eac9fea0] ...
I0207 11:35:38.924946    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 1b36eac9fea0"
I0207 11:35:38.993799    3511 logs.go:123] Gathering logs for kube-apiserver [39d1988a2fab] ...
I0207 11:35:38.993822    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 39d1988a2fab"
I0207 11:35:39.062501    3511 logs.go:123] Gathering logs for kube-scheduler [ee7c2215045a] ...
I0207 11:35:39.062521    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 ee7c2215045a"
I0207 11:35:39.132481    3511 logs.go:123] Gathering logs for storage-provisioner [ec52b9ed7e21] ...
I0207 11:35:39.132508    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 ec52b9ed7e21"
I0207 11:35:39.177164    3511 logs.go:123] Gathering logs for dmesg ...
I0207 11:35:39.177189    3511 ssh_runner.go:195] Run: /bin/bash -c "sudo dmesg -PH -L=never --level warn,err,crit,alert,emerg | tail -n 400"
I0207 11:35:39.261157    3511 logs.go:123] Gathering logs for coredns [b94028402ad3] ...
I0207 11:35:39.261186    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 b94028402ad3"
I0207 11:35:39.309120    3511 logs.go:123] Gathering logs for kube-proxy [407d81ee6871] ...
I0207 11:35:39.309142    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 407d81ee6871"
I0207 11:35:39.351143    3511 logs.go:123] Gathering logs for kube-controller-manager [e1bb8361945e] ...
I0207 11:35:39.351165    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 e1bb8361945e"
I0207 11:35:39.440097    3511 logs.go:123] Gathering logs for storage-provisioner [0b85673627c7] ...
I0207 11:35:39.440128    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 0b85673627c7"
I0207 11:35:39.556404    3511 logs.go:123] Gathering logs for kubelet ...
I0207 11:35:39.556427    3511 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u kubelet -n 400"
I0207 11:35:39.667299    3511 logs.go:123] Gathering logs for etcd [1eed8f4e884e] ...
I0207 11:35:39.667319    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 1eed8f4e884e"
I0207 11:35:39.856129    3511 logs.go:123] Gathering logs for kube-scheduler [1d1e9abd9836] ...
I0207 11:35:39.856145    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 1d1e9abd9836"
I0207 11:35:39.935528    3511 logs.go:123] Gathering logs for kubernetes-dashboard [5631cbc21ffd] ...
I0207 11:35:39.935547    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 5631cbc21ffd"
I0207 11:35:39.986383    3511 logs.go:123] Gathering logs for kubernetes-dashboard [f4b96dcbc9f2] ...
I0207 11:35:39.986401    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 f4b96dcbc9f2"
I0207 11:35:40.080050    3511 logs.go:123] Gathering logs for controller_ingress [8d50139a5311] ...
I0207 11:35:40.080067    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 8d50139a5311"
I0207 11:35:40.155192    3511 logs.go:123] Gathering logs for container status ...
I0207 11:35:40.155208    3511 ssh_runner.go:195] Run: /bin/bash -c "sudo `which crictl || echo crictl` ps -a || sudo docker ps -a"
I0207 11:35:42.860578    3511 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0207 11:35:42.891211    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-apiserver --format={{.ID}}
I0207 11:35:42.948829    3511 logs.go:284] 2 containers: [1b36eac9fea0 39d1988a2fab]
I0207 11:35:42.948954    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_etcd --format={{.ID}}
I0207 11:35:43.021458    3511 logs.go:284] 2 containers: [1eed8f4e884e 5cf1eff8b362]
I0207 11:35:43.021609    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_coredns --format={{.ID}}
I0207 11:35:43.076875    3511 logs.go:284] 2 containers: [06fb15ac4a32 b94028402ad3]
I0207 11:35:43.076976    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-scheduler --format={{.ID}}
I0207 11:35:43.135931    3511 logs.go:284] 2 containers: [1d1e9abd9836 ee7c2215045a]
I0207 11:35:43.136028    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-proxy --format={{.ID}}
I0207 11:35:43.233283    3511 logs.go:284] 2 containers: [1db52edcdad4 407d81ee6871]
I0207 11:35:43.233395    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-controller-manager --format={{.ID}}
I0207 11:35:43.286430    3511 logs.go:284] 2 containers: [e1bb8361945e 885389110d12]
I0207 11:35:43.286531    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kindnet --format={{.ID}}
I0207 11:35:43.334733    3511 logs.go:284] 0 containers: []
W0207 11:35:43.334771    3511 logs.go:286] No container was found matching "kindnet"
I0207 11:35:43.334846    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kubernetes-dashboard --format={{.ID}}
I0207 11:35:43.393778    3511 logs.go:284] 2 containers: [5631cbc21ffd f4b96dcbc9f2]
I0207 11:35:43.393881    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_controller_ingress --format={{.ID}}
I0207 11:35:43.489699    3511 logs.go:284] 1 containers: [8d50139a5311]
I0207 11:35:43.489820    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_storage-provisioner --format={{.ID}}
I0207 11:35:43.534049    3511 logs.go:284] 2 containers: [ec52b9ed7e21 0b85673627c7]
I0207 11:35:43.534077    3511 logs.go:123] Gathering logs for dmesg ...
I0207 11:35:43.534091    3511 ssh_runner.go:195] Run: /bin/bash -c "sudo dmesg -PH -L=never --level warn,err,crit,alert,emerg | tail -n 400"
I0207 11:35:43.622353    3511 logs.go:123] Gathering logs for describe nodes ...
I0207 11:35:43.622386    3511 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig"
W0207 11:35:43.795042    3511 logs.go:130] failed describe nodes: command: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig" /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig": Process exited with status 1
stdout:

stderr:
E0207 03:35:43.780564   13830 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:35:43.781243   13830 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:35:43.784220   13830 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:35:43.785075   13830 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:35:43.787068   13830 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
The connection to the server localhost:8443 was refused - did you specify the right host or port?
 output: 
** stderr ** 
E0207 03:35:43.780564   13830 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:35:43.781243   13830 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:35:43.784220   13830 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:35:43.785075   13830 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:35:43.787068   13830 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
The connection to the server localhost:8443 was refused - did you specify the right host or port?

** /stderr **
I0207 11:35:43.795055    3511 logs.go:123] Gathering logs for kube-apiserver [1b36eac9fea0] ...
I0207 11:35:43.795069    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 1b36eac9fea0"
I0207 11:35:43.946894    3511 logs.go:123] Gathering logs for etcd [1eed8f4e884e] ...
I0207 11:35:43.946917    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 1eed8f4e884e"
I0207 11:35:44.265352    3511 logs.go:123] Gathering logs for coredns [b94028402ad3] ...
I0207 11:35:44.265369    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 b94028402ad3"
I0207 11:35:44.324980    3511 logs.go:123] Gathering logs for kube-proxy [1db52edcdad4] ...
I0207 11:35:44.325003    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 1db52edcdad4"
I0207 11:35:44.381822    3511 logs.go:123] Gathering logs for kube-controller-manager [e1bb8361945e] ...
I0207 11:35:44.381838    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 e1bb8361945e"
I0207 11:35:44.534854    3511 logs.go:123] Gathering logs for Docker ...
I0207 11:35:44.534877    3511 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u docker -u cri-docker -n 400"
I0207 11:35:44.689203    3511 logs.go:123] Gathering logs for kubelet ...
I0207 11:35:44.689256    3511 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u kubelet -n 400"
I0207 11:35:44.864011    3511 logs.go:123] Gathering logs for etcd [5cf1eff8b362] ...
I0207 11:35:44.864034    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 5cf1eff8b362"
I0207 11:35:45.013328    3511 logs.go:123] Gathering logs for kubernetes-dashboard [f4b96dcbc9f2] ...
I0207 11:35:45.013526    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 f4b96dcbc9f2"
I0207 11:35:45.179146    3511 logs.go:123] Gathering logs for storage-provisioner [ec52b9ed7e21] ...
I0207 11:35:45.180188    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 ec52b9ed7e21"
I0207 11:35:45.266393    3511 logs.go:123] Gathering logs for kube-apiserver [39d1988a2fab] ...
I0207 11:35:45.266434    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 39d1988a2fab"
I0207 11:35:45.390240    3511 logs.go:123] Gathering logs for coredns [06fb15ac4a32] ...
I0207 11:35:45.390263    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 06fb15ac4a32"
I0207 11:35:45.518529    3511 logs.go:123] Gathering logs for kube-proxy [407d81ee6871] ...
I0207 11:35:45.518553    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 407d81ee6871"
I0207 11:35:45.612763    3511 logs.go:123] Gathering logs for controller_ingress [8d50139a5311] ...
I0207 11:35:45.612848    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 8d50139a5311"
I0207 11:35:45.733180    3511 logs.go:123] Gathering logs for kube-scheduler [1d1e9abd9836] ...
I0207 11:35:45.733202    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 1d1e9abd9836"
I0207 11:35:45.854130    3511 logs.go:123] Gathering logs for kube-scheduler [ee7c2215045a] ...
I0207 11:35:45.854168    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 ee7c2215045a"
I0207 11:35:45.951558    3511 logs.go:123] Gathering logs for kube-controller-manager [885389110d12] ...
I0207 11:35:45.951617    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 885389110d12"
I0207 11:35:46.066770    3511 logs.go:123] Gathering logs for kubernetes-dashboard [5631cbc21ffd] ...
I0207 11:35:46.066786    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 5631cbc21ffd"
I0207 11:35:46.142659    3511 logs.go:123] Gathering logs for storage-provisioner [0b85673627c7] ...
I0207 11:35:46.142780    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 0b85673627c7"
I0207 11:35:46.289594    3511 logs.go:123] Gathering logs for container status ...
I0207 11:35:46.289609    3511 ssh_runner.go:195] Run: /bin/bash -c "sudo `which crictl || echo crictl` ps -a || sudo docker ps -a"
I0207 11:35:49.201954    3511 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0207 11:35:49.227445    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-apiserver --format={{.ID}}
I0207 11:35:49.276923    3511 logs.go:284] 2 containers: [1b36eac9fea0 39d1988a2fab]
I0207 11:35:49.277112    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_etcd --format={{.ID}}
I0207 11:35:49.317999    3511 logs.go:284] 2 containers: [1eed8f4e884e 5cf1eff8b362]
I0207 11:35:49.318280    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_coredns --format={{.ID}}
I0207 11:35:49.358582    3511 logs.go:284] 2 containers: [06fb15ac4a32 b94028402ad3]
I0207 11:35:49.358742    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-scheduler --format={{.ID}}
I0207 11:35:49.402143    3511 logs.go:284] 2 containers: [1d1e9abd9836 ee7c2215045a]
I0207 11:35:49.402264    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-proxy --format={{.ID}}
I0207 11:35:49.466330    3511 logs.go:284] 2 containers: [1db52edcdad4 407d81ee6871]
I0207 11:35:49.466486    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-controller-manager --format={{.ID}}
I0207 11:35:49.520221    3511 logs.go:284] 2 containers: [e1bb8361945e 885389110d12]
I0207 11:35:49.520379    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kindnet --format={{.ID}}
I0207 11:35:49.582066    3511 logs.go:284] 0 containers: []
W0207 11:35:49.582084    3511 logs.go:286] No container was found matching "kindnet"
I0207 11:35:49.582198    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_controller_ingress --format={{.ID}}
I0207 11:35:49.649960    3511 logs.go:284] 1 containers: [8d50139a5311]
I0207 11:35:49.650072    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_storage-provisioner --format={{.ID}}
I0207 11:35:49.729458    3511 logs.go:284] 2 containers: [ec52b9ed7e21 0b85673627c7]
I0207 11:35:49.729585    3511 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kubernetes-dashboard --format={{.ID}}
I0207 11:35:49.773786    3511 logs.go:284] 2 containers: [5631cbc21ffd f4b96dcbc9f2]
I0207 11:35:49.773812    3511 logs.go:123] Gathering logs for coredns [06fb15ac4a32] ...
I0207 11:35:49.773867    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 06fb15ac4a32"
I0207 11:35:49.842465    3511 logs.go:123] Gathering logs for coredns [b94028402ad3] ...
I0207 11:35:49.842482    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 b94028402ad3"
I0207 11:35:49.913967    3511 logs.go:123] Gathering logs for storage-provisioner [ec52b9ed7e21] ...
I0207 11:35:49.913984    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 ec52b9ed7e21"
I0207 11:35:49.981997    3511 logs.go:123] Gathering logs for container status ...
I0207 11:35:49.982015    3511 ssh_runner.go:195] Run: /bin/bash -c "sudo `which crictl || echo crictl` ps -a || sudo docker ps -a"
I0207 11:35:50.245606    3511 logs.go:123] Gathering logs for dmesg ...
I0207 11:35:50.245622    3511 ssh_runner.go:195] Run: /bin/bash -c "sudo dmesg -PH -L=never --level warn,err,crit,alert,emerg | tail -n 400"
I0207 11:35:50.346868    3511 logs.go:123] Gathering logs for kube-scheduler [1d1e9abd9836] ...
I0207 11:35:50.346884    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 1d1e9abd9836"
I0207 11:35:50.454346    3511 logs.go:123] Gathering logs for kubernetes-dashboard [5631cbc21ffd] ...
I0207 11:35:50.454372    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 5631cbc21ffd"
I0207 11:35:50.543915    3511 logs.go:123] Gathering logs for kubernetes-dashboard [f4b96dcbc9f2] ...
I0207 11:35:50.543974    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 f4b96dcbc9f2"
I0207 11:35:50.700183    3511 logs.go:123] Gathering logs for kube-controller-manager [885389110d12] ...
I0207 11:35:50.700205    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 885389110d12"
I0207 11:35:50.841647    3511 logs.go:123] Gathering logs for storage-provisioner [0b85673627c7] ...
I0207 11:35:50.841675    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 0b85673627c7"
I0207 11:35:50.929673    3511 logs.go:123] Gathering logs for describe nodes ...
I0207 11:35:50.929696    3511 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig"
W0207 11:35:51.090950    3511 logs.go:130] failed describe nodes: command: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig" /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig": Process exited with status 1
stdout:

stderr:
E0207 03:35:51.074395   14250 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:35:51.075109   14250 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:35:51.076693   14250 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:35:51.078066   14250 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:35:51.080861   14250 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
The connection to the server localhost:8443 was refused - did you specify the right host or port?
 output: 
** stderr ** 
E0207 03:35:51.074395   14250 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:35:51.075109   14250 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:35:51.076693   14250 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:35:51.078066   14250 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:35:51.080861   14250 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
The connection to the server localhost:8443 was refused - did you specify the right host or port?

** /stderr **
I0207 11:35:51.090981    3511 logs.go:123] Gathering logs for kube-apiserver [1b36eac9fea0] ...
I0207 11:35:51.091006    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 1b36eac9fea0"
I0207 11:35:51.180750    3511 logs.go:123] Gathering logs for etcd [1eed8f4e884e] ...
I0207 11:35:51.180772    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 1eed8f4e884e"
I0207 11:35:51.404394    3511 logs.go:123] Gathering logs for etcd [5cf1eff8b362] ...
I0207 11:35:51.404428    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 5cf1eff8b362"
I0207 11:35:51.522633    3511 logs.go:123] Gathering logs for kube-proxy [407d81ee6871] ...
I0207 11:35:51.522776    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 407d81ee6871"
I0207 11:35:51.708993    3511 logs.go:123] Gathering logs for kube-controller-manager [e1bb8361945e] ...
I0207 11:35:51.709021    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 e1bb8361945e"
I0207 11:35:51.854636    3511 logs.go:123] Gathering logs for controller_ingress [8d50139a5311] ...
I0207 11:35:51.854657    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 8d50139a5311"
I0207 11:35:51.974022    3511 logs.go:123] Gathering logs for Docker ...
I0207 11:35:51.974038    3511 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u docker -u cri-docker -n 400"
I0207 11:35:52.052778    3511 logs.go:123] Gathering logs for kubelet ...
I0207 11:35:52.052799    3511 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u kubelet -n 400"
I0207 11:35:52.164112    3511 logs.go:123] Gathering logs for kube-apiserver [39d1988a2fab] ...
I0207 11:35:52.164154    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 39d1988a2fab"
I0207 11:35:52.236756    3511 logs.go:123] Gathering logs for kube-scheduler [ee7c2215045a] ...
I0207 11:35:52.236771    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 ee7c2215045a"
I0207 11:35:52.326644    3511 logs.go:123] Gathering logs for kube-proxy [1db52edcdad4] ...
I0207 11:35:52.326659    3511 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 1db52edcdad4"
I0207 11:35:54.884837    3511 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0207 11:35:54.947134    3511 kubeadm.go:640] restartCluster took 4m16.766456521s
W0207 11:35:54.947291    3511 out.go:239] ü§¶  Unable to restart cluster, will reset it: apiserver healthz: apiserver process never appeared
I0207 11:35:54.947374    3511 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.28.3:$PATH" kubeadm reset --cri-socket /var/run/cri-dockerd.sock --force"
I0207 11:36:11.244650    3511 ssh_runner.go:235] Completed: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.28.3:$PATH" kubeadm reset --cri-socket /var/run/cri-dockerd.sock --force": (16.29688855s)
I0207 11:36:11.245010    3511 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service kubelet
I0207 11:36:11.266823    3511 ssh_runner.go:195] Run: sudo cp /var/tmp/minikube/kubeadm.yaml.new /var/tmp/minikube/kubeadm.yaml
I0207 11:36:11.281725    3511 ssh_runner.go:195] Run: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf
I0207 11:36:11.296428    3511 kubeadm.go:152] config check failed, skipping stale config cleanup: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf: Process exited with status 2
stdout:

stderr:
ls: cannot access '/etc/kubernetes/admin.conf': No such file or directory
ls: cannot access '/etc/kubernetes/kubelet.conf': No such file or directory
ls: cannot access '/etc/kubernetes/controller-manager.conf': No such file or directory
ls: cannot access '/etc/kubernetes/scheduler.conf': No such file or directory
I0207 11:36:11.296518    3511 ssh_runner.go:286] Start: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.28.3:$PATH" kubeadm init --config /var/tmp/minikube/kubeadm.yaml  --ignore-preflight-errors=DirAvailable--etc-kubernetes-manifests,DirAvailable--var-lib-minikube,DirAvailable--var-lib-minikube-etcd,FileAvailable--etc-kubernetes-manifests-kube-scheduler.yaml,FileAvailable--etc-kubernetes-manifests-kube-apiserver.yaml,FileAvailable--etc-kubernetes-manifests-kube-controller-manager.yaml,FileAvailable--etc-kubernetes-manifests-etcd.yaml,Port-10250,Swap,NumCPU,Mem"
I0207 11:36:11.638296    3511 kubeadm.go:322] 	[WARNING Service-Kubelet]: kubelet service is not enabled, please run 'systemctl enable kubelet.service'
I0207 11:36:14.841124    3511 kubeadm.go:322] error execution phase preflight: [preflight] Some fatal errors occurred:
I0207 11:36:14.842243    3511 kubeadm.go:322] 	[ERROR ImagePull]: failed to pull image registry.k8s.io/kube-apiserver:v1.28.3: output: time="2024-02-07T03:36:12Z" level=fatal msg="validate service connection: validate CRI v1 image API for endpoint \"unix:///var/run/cri-dockerd.sock\": rpc error: code = Unknown desc = readdirent /var/lib/docker/image/overlay2/layerdb/mounts/a6c0da9b6145e18bcd661d5d6f8d52d3684a74feb456c6ed45a0cb764a879c43: bad message"
I0207 11:36:14.842392    3511 kubeadm.go:322] , error: exit status 1
I0207 11:36:14.843370    3511 kubeadm.go:322] 	[ERROR ImagePull]: failed to pull image registry.k8s.io/kube-controller-manager:v1.28.3: output: time="2024-02-07T03:36:12Z" level=fatal msg="validate service connection: validate CRI v1 image API for endpoint \"unix:///var/run/cri-dockerd.sock\": rpc error: code = Unknown desc = readdirent /var/lib/docker/image/overlay2/layerdb/mounts/a6c0da9b6145e18bcd661d5d6f8d52d3684a74feb456c6ed45a0cb764a879c43: bad message"
I0207 11:36:14.843533    3511 kubeadm.go:322] , error: exit status 1
I0207 11:36:14.844411    3511 kubeadm.go:322] 	[ERROR ImagePull]: failed to pull image registry.k8s.io/kube-scheduler:v1.28.3: output: time="2024-02-07T03:36:13Z" level=fatal msg="validate service connection: validate CRI v1 image API for endpoint \"unix:///var/run/cri-dockerd.sock\": rpc error: code = Unknown desc = readdirent /var/lib/docker/image/overlay2/layerdb/mounts/a6c0da9b6145e18bcd661d5d6f8d52d3684a74feb456c6ed45a0cb764a879c43: bad message"
I0207 11:36:14.844477    3511 kubeadm.go:322] , error: exit status 1
I0207 11:36:14.845350    3511 kubeadm.go:322] 	[ERROR ImagePull]: failed to pull image registry.k8s.io/kube-proxy:v1.28.3: output: time="2024-02-07T03:36:13Z" level=fatal msg="validate service connection: validate CRI v1 image API for endpoint \"unix:///var/run/cri-dockerd.sock\": rpc error: code = Unknown desc = readdirent /var/lib/docker/image/overlay2/layerdb/mounts/a6c0da9b6145e18bcd661d5d6f8d52d3684a74feb456c6ed45a0cb764a879c43: bad message"
I0207 11:36:14.845417    3511 kubeadm.go:322] , error: exit status 1
I0207 11:36:14.846251    3511 kubeadm.go:322] 	[ERROR ImagePull]: failed to pull image registry.k8s.io/pause:3.9: output: time="2024-02-07T03:36:13Z" level=fatal msg="validate service connection: validate CRI v1 image API for endpoint \"unix:///var/run/cri-dockerd.sock\": rpc error: code = Unknown desc = readdirent /var/lib/docker/image/overlay2/layerdb/mounts/a6c0da9b6145e18bcd661d5d6f8d52d3684a74feb456c6ed45a0cb764a879c43: bad message"
I0207 11:36:14.846323    3511 kubeadm.go:322] , error: exit status 1
I0207 11:36:14.847226    3511 kubeadm.go:322] 	[ERROR ImagePull]: failed to pull image registry.k8s.io/etcd:3.5.9-0: output: time="2024-02-07T03:36:14Z" level=fatal msg="validate service connection: validate CRI v1 image API for endpoint \"unix:///var/run/cri-dockerd.sock\": rpc error: code = Unknown desc = readdirent /var/lib/docker/image/overlay2/layerdb/mounts/a6c0da9b6145e18bcd661d5d6f8d52d3684a74feb456c6ed45a0cb764a879c43: bad message"
I0207 11:36:14.847289    3511 kubeadm.go:322] , error: exit status 1
I0207 11:36:14.848187    3511 kubeadm.go:322] 	[ERROR ImagePull]: failed to pull image registry.k8s.io/coredns/coredns:v1.10.1: output: time="2024-02-07T03:36:14Z" level=fatal msg="validate service connection: validate CRI v1 image API for endpoint \"unix:///var/run/cri-dockerd.sock\": rpc error: code = Unknown desc = readdirent /var/lib/docker/image/overlay2/layerdb/mounts/a6c0da9b6145e18bcd661d5d6f8d52d3684a74feb456c6ed45a0cb764a879c43: bad message"
I0207 11:36:14.848318    3511 kubeadm.go:322] , error: exit status 1
I0207 11:36:14.848572    3511 kubeadm.go:322] [preflight] If you know what you are doing, you can make a check non-fatal with `--ignore-preflight-errors=...`
I0207 11:36:14.848747    3511 kubeadm.go:322] To see the stack trace of this error execute with --v=5 or higher
I0207 11:36:14.854714    3511 kubeadm.go:322] [init] Using Kubernetes version: v1.28.3
I0207 11:36:14.854815    3511 kubeadm.go:322] [preflight] Running pre-flight checks
I0207 11:36:14.854974    3511 kubeadm.go:322] [preflight] Pulling images required for setting up a Kubernetes cluster
I0207 11:36:14.855197    3511 kubeadm.go:322] [preflight] This might take a minute or two, depending on the speed of your internet connection
I0207 11:36:14.855404    3511 kubeadm.go:322] [preflight] You can also perform this action in beforehand using 'kubeadm config images pull'
W0207 11:36:14.856060    3511 out.go:239] üí¢  initialization failed, will try again: wait: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.28.3:$PATH" kubeadm init --config /var/tmp/minikube/kubeadm.yaml  --ignore-preflight-errors=DirAvailable--etc-kubernetes-manifests,DirAvailable--var-lib-minikube,DirAvailable--var-lib-minikube-etcd,FileAvailable--etc-kubernetes-manifests-kube-scheduler.yaml,FileAvailable--etc-kubernetes-manifests-kube-apiserver.yaml,FileAvailable--etc-kubernetes-manifests-kube-controller-manager.yaml,FileAvailable--etc-kubernetes-manifests-etcd.yaml,Port-10250,Swap,NumCPU,Mem": Process exited with status 1
stdout:
[init] Using Kubernetes version: v1.28.3
[preflight] Running pre-flight checks
[preflight] Pulling images required for setting up a Kubernetes cluster
[preflight] This might take a minute or two, depending on the speed of your internet connection
[preflight] You can also perform this action in beforehand using 'kubeadm config images pull'

stderr:
	[WARNING Service-Kubelet]: kubelet service is not enabled, please run 'systemctl enable kubelet.service'
error execution phase preflight: [preflight] Some fatal errors occurred:
	[ERROR ImagePull]: failed to pull image registry.k8s.io/kube-apiserver:v1.28.3: output: time="2024-02-07T03:36:12Z" level=fatal msg="validate service connection: validate CRI v1 image API for endpoint \"unix:///var/run/cri-dockerd.sock\": rpc error: code = Unknown desc = readdirent /var/lib/docker/image/overlay2/layerdb/mounts/a6c0da9b6145e18bcd661d5d6f8d52d3684a74feb456c6ed45a0cb764a879c43: bad message"
, error: exit status 1
	[ERROR ImagePull]: failed to pull image registry.k8s.io/kube-controller-manager:v1.28.3: output: time="2024-02-07T03:36:12Z" level=fatal msg="validate service connection: validate CRI v1 image API for endpoint \"unix:///var/run/cri-dockerd.sock\": rpc error: code = Unknown desc = readdirent /var/lib/docker/image/overlay2/layerdb/mounts/a6c0da9b6145e18bcd661d5d6f8d52d3684a74feb456c6ed45a0cb764a879c43: bad message"
, error: exit status 1
	[ERROR ImagePull]: failed to pull image registry.k8s.io/kube-scheduler:v1.28.3: output: time="2024-02-07T03:36:13Z" level=fatal msg="validate service connection: validate CRI v1 image API for endpoint \"unix:///var/run/cri-dockerd.sock\": rpc error: code = Unknown desc = readdirent /var/lib/docker/image/overlay2/layerdb/mounts/a6c0da9b6145e18bcd661d5d6f8d52d3684a74feb456c6ed45a0cb764a879c43: bad message"
, error: exit status 1
	[ERROR ImagePull]: failed to pull image registry.k8s.io/kube-proxy:v1.28.3: output: time="2024-02-07T03:36:13Z" level=fatal msg="validate service connection: validate CRI v1 image API for endpoint \"unix:///var/run/cri-dockerd.sock\": rpc error: code = Unknown desc = readdirent /var/lib/docker/image/overlay2/layerdb/mounts/a6c0da9b6145e18bcd661d5d6f8d52d3684a74feb456c6ed45a0cb764a879c43: bad message"
, error: exit status 1
	[ERROR ImagePull]: failed to pull image registry.k8s.io/pause:3.9: output: time="2024-02-07T03:36:13Z" level=fatal msg="validate service connection: validate CRI v1 image API for endpoint \"unix:///var/run/cri-dockerd.sock\": rpc error: code = Unknown desc = readdirent /var/lib/docker/image/overlay2/layerdb/mounts/a6c0da9b6145e18bcd661d5d6f8d52d3684a74feb456c6ed45a0cb764a879c43: bad message"
, error: exit status 1
	[ERROR ImagePull]: failed to pull image registry.k8s.io/etcd:3.5.9-0: output: time="2024-02-07T03:36:14Z" level=fatal msg="validate service connection: validate CRI v1 image API for endpoint \"unix:///var/run/cri-dockerd.sock\": rpc error: code = Unknown desc = readdirent /var/lib/docker/image/overlay2/layerdb/mounts/a6c0da9b6145e18bcd661d5d6f8d52d3684a74feb456c6ed45a0cb764a879c43: bad message"
, error: exit status 1
	[ERROR ImagePull]: failed to pull image registry.k8s.io/coredns/coredns:v1.10.1: output: time="2024-02-07T03:36:14Z" level=fatal msg="validate service connection: validate CRI v1 image API for endpoint \"unix:///var/run/cri-dockerd.sock\": rpc error: code = Unknown desc = readdirent /var/lib/docker/image/overlay2/layerdb/mounts/a6c0da9b6145e18bcd661d5d6f8d52d3684a74feb456c6ed45a0cb764a879c43: bad message"
, error: exit status 1
[preflight] If you know what you are doing, you can make a check non-fatal with `--ignore-preflight-errors=...`
To see the stack trace of this error execute with --v=5 or higher

I0207 11:36:14.858096    3511 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.28.3:$PATH" kubeadm reset --cri-socket /var/run/cri-dockerd.sock --force"
I0207 11:36:26.957665    3511 ssh_runner.go:235] Completed: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.28.3:$PATH" kubeadm reset --cri-socket /var/run/cri-dockerd.sock --force": (12.099304515s)
I0207 11:36:26.959248    3511 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service kubelet
I0207 11:36:26.990328    3511 ssh_runner.go:195] Run: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf
I0207 11:36:27.006305    3511 kubeadm.go:152] config check failed, skipping stale config cleanup: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf: Process exited with status 2
stdout:

stderr:
ls: cannot access '/etc/kubernetes/admin.conf': No such file or directory
ls: cannot access '/etc/kubernetes/kubelet.conf': No such file or directory
ls: cannot access '/etc/kubernetes/controller-manager.conf': No such file or directory
ls: cannot access '/etc/kubernetes/scheduler.conf': No such file or directory
I0207 11:36:27.006418    3511 ssh_runner.go:286] Start: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.28.3:$PATH" kubeadm init --config /var/tmp/minikube/kubeadm.yaml  --ignore-preflight-errors=DirAvailable--etc-kubernetes-manifests,DirAvailable--var-lib-minikube,DirAvailable--var-lib-minikube-etcd,FileAvailable--etc-kubernetes-manifests-kube-scheduler.yaml,FileAvailable--etc-kubernetes-manifests-kube-apiserver.yaml,FileAvailable--etc-kubernetes-manifests-kube-controller-manager.yaml,FileAvailable--etc-kubernetes-manifests-etcd.yaml,Port-10250,Swap,NumCPU,Mem"
I0207 11:36:27.086388    3511 kubeadm.go:322] [init] Using Kubernetes version: v1.28.3
I0207 11:36:27.087500    3511 kubeadm.go:322] [preflight] Running pre-flight checks
I0207 11:36:27.344758    3511 kubeadm.go:322] [preflight] Pulling images required for setting up a Kubernetes cluster
I0207 11:36:27.344997    3511 kubeadm.go:322] [preflight] This might take a minute or two, depending on the speed of your internet connection
I0207 11:36:27.345203    3511 kubeadm.go:322] [preflight] You can also perform this action in beforehand using 'kubeadm config images pull'
I0207 11:36:30.174594    3511 kubeadm.go:322] 	[WARNING Service-Kubelet]: kubelet service is not enabled, please run 'systemctl enable kubelet.service'
I0207 11:36:30.174763    3511 kubeadm.go:322] error execution phase preflight: [preflight] Some fatal errors occurred:
I0207 11:36:30.175460    3511 kubeadm.go:322] 	[ERROR ImagePull]: failed to pull image registry.k8s.io/kube-apiserver:v1.28.3: output: time="2024-02-07T03:36:27Z" level=fatal msg="validate service connection: validate CRI v1 image API for endpoint \"unix:///var/run/cri-dockerd.sock\": rpc error: code = Unknown desc = readdirent /var/lib/docker/image/overlay2/layerdb/mounts/a6c0da9b6145e18bcd661d5d6f8d52d3684a74feb456c6ed45a0cb764a879c43: bad message"
I0207 11:36:30.175514    3511 kubeadm.go:322] , error: exit status 1
I0207 11:36:30.176454    3511 kubeadm.go:322] 	[ERROR ImagePull]: failed to pull image registry.k8s.io/kube-controller-manager:v1.28.3: output: time="2024-02-07T03:36:28Z" level=fatal msg="validate service connection: validate CRI v1 image API for endpoint \"unix:///var/run/cri-dockerd.sock\": rpc error: code = Unknown desc = readdirent /var/lib/docker/image/overlay2/layerdb/mounts/a6c0da9b6145e18bcd661d5d6f8d52d3684a74feb456c6ed45a0cb764a879c43: bad message"
I0207 11:36:30.176526    3511 kubeadm.go:322] , error: exit status 1
I0207 11:36:30.177154    3511 kubeadm.go:322] 	[ERROR ImagePull]: failed to pull image registry.k8s.io/kube-scheduler:v1.28.3: output: time="2024-02-07T03:36:28Z" level=fatal msg="validate service connection: validate CRI v1 image API for endpoint \"unix:///var/run/cri-dockerd.sock\": rpc error: code = Unknown desc = readdirent /var/lib/docker/image/overlay2/layerdb/mounts/a6c0da9b6145e18bcd661d5d6f8d52d3684a74feb456c6ed45a0cb764a879c43: bad message"
I0207 11:36:30.177208    3511 kubeadm.go:322] , error: exit status 1
I0207 11:36:30.177850    3511 kubeadm.go:322] 	[ERROR ImagePull]: failed to pull image registry.k8s.io/kube-proxy:v1.28.3: output: time="2024-02-07T03:36:28Z" level=fatal msg="validate service connection: validate CRI v1 image API for endpoint \"unix:///var/run/cri-dockerd.sock\": rpc error: code = Unknown desc = readdirent /var/lib/docker/image/overlay2/layerdb/mounts/a6c0da9b6145e18bcd661d5d6f8d52d3684a74feb456c6ed45a0cb764a879c43: bad message"
I0207 11:36:30.177911    3511 kubeadm.go:322] , error: exit status 1
I0207 11:36:30.178480    3511 kubeadm.go:322] 	[ERROR ImagePull]: failed to pull image registry.k8s.io/pause:3.9: output: time="2024-02-07T03:36:29Z" level=fatal msg="validate service connection: validate CRI v1 image API for endpoint \"unix:///var/run/cri-dockerd.sock\": rpc error: code = Unknown desc = readdirent /var/lib/docker/image/overlay2/layerdb/mounts/a6c0da9b6145e18bcd661d5d6f8d52d3684a74feb456c6ed45a0cb764a879c43: bad message"
I0207 11:36:30.178534    3511 kubeadm.go:322] , error: exit status 1
I0207 11:36:30.179321    3511 kubeadm.go:322] 	[ERROR ImagePull]: failed to pull image registry.k8s.io/etcd:3.5.9-0: output: time="2024-02-07T03:36:29Z" level=fatal msg="validate service connection: validate CRI v1 image API for endpoint \"unix:///var/run/cri-dockerd.sock\": rpc error: code = Unknown desc = readdirent /var/lib/docker/image/overlay2/layerdb/mounts/a6c0da9b6145e18bcd661d5d6f8d52d3684a74feb456c6ed45a0cb764a879c43: bad message"
I0207 11:36:30.179379    3511 kubeadm.go:322] , error: exit status 1
I0207 11:36:30.180144    3511 kubeadm.go:322] 	[ERROR ImagePull]: failed to pull image registry.k8s.io/coredns/coredns:v1.10.1: output: time="2024-02-07T03:36:30Z" level=fatal msg="validate service connection: validate CRI v1 image API for endpoint \"unix:///var/run/cri-dockerd.sock\": rpc error: code = Unknown desc = readdirent /var/lib/docker/image/overlay2/layerdb/mounts/a6c0da9b6145e18bcd661d5d6f8d52d3684a74feb456c6ed45a0cb764a879c43: bad message"
I0207 11:36:30.180200    3511 kubeadm.go:322] , error: exit status 1
I0207 11:36:30.180460    3511 kubeadm.go:322] [preflight] If you know what you are doing, you can make a check non-fatal with `--ignore-preflight-errors=...`
I0207 11:36:30.180711    3511 kubeadm.go:406] StartCluster complete in 4m52.056856395s
I0207 11:36:30.180750    3511 kubeadm.go:322] To see the stack trace of this error execute with --v=5 or higher
I0207 11:36:30.181175    3511 cri.go:54] listing CRI containers in root : {State:all Name:kube-apiserver Namespaces:[]}
I0207 11:36:30.181336    3511 ssh_runner.go:195] Run: sudo crictl ps -a --quiet --name=kube-apiserver
E0207 11:36:30.284631    3511 logs.go:281] Failed to list containers for "kube-apiserver": crictl list: sudo crictl ps -a --quiet --name=kube-apiserver: Process exited with status 1
stdout:

stderr:
time="2024-02-07T03:36:30Z" level=fatal msg="validate service connection: validate CRI v1 image API for endpoint \"unix:///var/run/cri-dockerd.sock\": rpc error: code = Unknown desc = readdirent /var/lib/docker/image/overlay2/layerdb/mounts/a6c0da9b6145e18bcd661d5d6f8d52d3684a74feb456c6ed45a0cb764a879c43: bad message"
I0207 11:36:30.284687    3511 cri.go:54] listing CRI containers in root : {State:all Name:etcd Namespaces:[]}
I0207 11:36:30.284806    3511 ssh_runner.go:195] Run: sudo crictl ps -a --quiet --name=etcd
E0207 11:36:30.386441    3511 logs.go:281] Failed to list containers for "etcd": crictl list: sudo crictl ps -a --quiet --name=etcd: Process exited with status 1
stdout:

stderr:
time="2024-02-07T03:36:30Z" level=fatal msg="validate service connection: validate CRI v1 image API for endpoint \"unix:///var/run/cri-dockerd.sock\": rpc error: code = Unknown desc = readdirent /var/lib/docker/image/overlay2/layerdb/mounts/a6c0da9b6145e18bcd661d5d6f8d52d3684a74feb456c6ed45a0cb764a879c43: bad message"
I0207 11:36:30.386485    3511 cri.go:54] listing CRI containers in root : {State:all Name:coredns Namespaces:[]}
I0207 11:36:30.386600    3511 ssh_runner.go:195] Run: sudo crictl ps -a --quiet --name=coredns
E0207 11:36:30.479692    3511 logs.go:281] Failed to list containers for "coredns": crictl list: sudo crictl ps -a --quiet --name=coredns: Process exited with status 1
stdout:

stderr:
time="2024-02-07T03:36:30Z" level=fatal msg="validate service connection: validate CRI v1 image API for endpoint \"unix:///var/run/cri-dockerd.sock\": rpc error: code = Unknown desc = readdirent /var/lib/docker/image/overlay2/layerdb/mounts/a6c0da9b6145e18bcd661d5d6f8d52d3684a74feb456c6ed45a0cb764a879c43: bad message"
I0207 11:36:30.479759    3511 cri.go:54] listing CRI containers in root : {State:all Name:kube-scheduler Namespaces:[]}
I0207 11:36:30.479858    3511 ssh_runner.go:195] Run: sudo crictl ps -a --quiet --name=kube-scheduler
E0207 11:36:30.566249    3511 logs.go:281] Failed to list containers for "kube-scheduler": crictl list: sudo crictl ps -a --quiet --name=kube-scheduler: Process exited with status 1
stdout:

stderr:
time="2024-02-07T03:36:30Z" level=fatal msg="validate service connection: validate CRI v1 image API for endpoint \"unix:///var/run/cri-dockerd.sock\": rpc error: code = Unknown desc = readdirent /var/lib/docker/image/overlay2/layerdb/mounts/a6c0da9b6145e18bcd661d5d6f8d52d3684a74feb456c6ed45a0cb764a879c43: bad message"
I0207 11:36:30.566306    3511 cri.go:54] listing CRI containers in root : {State:all Name:kube-proxy Namespaces:[]}
I0207 11:36:30.566423    3511 ssh_runner.go:195] Run: sudo crictl ps -a --quiet --name=kube-proxy
E0207 11:36:30.670588    3511 logs.go:281] Failed to list containers for "kube-proxy": crictl list: sudo crictl ps -a --quiet --name=kube-proxy: Process exited with status 1
stdout:

stderr:
time="2024-02-07T03:36:30Z" level=fatal msg="validate service connection: validate CRI v1 image API for endpoint \"unix:///var/run/cri-dockerd.sock\": rpc error: code = Unknown desc = readdirent /var/lib/docker/image/overlay2/layerdb/mounts/a6c0da9b6145e18bcd661d5d6f8d52d3684a74feb456c6ed45a0cb764a879c43: bad message"
I0207 11:36:30.670625    3511 cri.go:54] listing CRI containers in root : {State:all Name:kube-controller-manager Namespaces:[]}
I0207 11:36:30.670730    3511 ssh_runner.go:195] Run: sudo crictl ps -a --quiet --name=kube-controller-manager
E0207 11:36:30.778785    3511 logs.go:281] Failed to list containers for "kube-controller-manager": crictl list: sudo crictl ps -a --quiet --name=kube-controller-manager: Process exited with status 1
stdout:

stderr:
time="2024-02-07T03:36:30Z" level=fatal msg="validate service connection: validate CRI v1 image API for endpoint \"unix:///var/run/cri-dockerd.sock\": rpc error: code = Unknown desc = readdirent /var/lib/docker/image/overlay2/layerdb/mounts/a6c0da9b6145e18bcd661d5d6f8d52d3684a74feb456c6ed45a0cb764a879c43: bad message"
I0207 11:36:30.778834    3511 cri.go:54] listing CRI containers in root : {State:all Name:kindnet Namespaces:[]}
I0207 11:36:30.778949    3511 ssh_runner.go:195] Run: sudo crictl ps -a --quiet --name=kindnet
E0207 11:36:30.878599    3511 logs.go:281] Failed to list containers for "kindnet": crictl list: sudo crictl ps -a --quiet --name=kindnet: Process exited with status 1
stdout:

stderr:
time="2024-02-07T03:36:30Z" level=fatal msg="validate service connection: validate CRI v1 image API for endpoint \"unix:///var/run/cri-dockerd.sock\": rpc error: code = Unknown desc = readdirent /var/lib/docker/image/overlay2/layerdb/mounts/a6c0da9b6145e18bcd661d5d6f8d52d3684a74feb456c6ed45a0cb764a879c43: bad message"
I0207 11:36:30.878638    3511 cri.go:54] listing CRI containers in root : {State:all Name:storage-provisioner Namespaces:[]}
I0207 11:36:30.878747    3511 ssh_runner.go:195] Run: sudo crictl ps -a --quiet --name=storage-provisioner
E0207 11:36:30.987357    3511 logs.go:281] Failed to list containers for "storage-provisioner": crictl list: sudo crictl ps -a --quiet --name=storage-provisioner: Process exited with status 1
stdout:

stderr:
time="2024-02-07T03:36:30Z" level=fatal msg="validate service connection: validate CRI v1 image API for endpoint \"unix:///var/run/cri-dockerd.sock\": rpc error: code = Unknown desc = readdirent /var/lib/docker/image/overlay2/layerdb/mounts/a6c0da9b6145e18bcd661d5d6f8d52d3684a74feb456c6ed45a0cb764a879c43: bad message"
I0207 11:36:30.987395    3511 cri.go:54] listing CRI containers in root : {State:all Name:kubernetes-dashboard Namespaces:[]}
I0207 11:36:30.987530    3511 ssh_runner.go:195] Run: sudo crictl ps -a --quiet --name=kubernetes-dashboard
E0207 11:36:31.085029    3511 logs.go:281] Failed to list containers for "kubernetes-dashboard": crictl list: sudo crictl ps -a --quiet --name=kubernetes-dashboard: Process exited with status 1
stdout:

stderr:
time="2024-02-07T03:36:31Z" level=fatal msg="validate service connection: validate CRI v1 image API for endpoint \"unix:///var/run/cri-dockerd.sock\": rpc error: code = Unknown desc = readdirent /var/lib/docker/image/overlay2/layerdb/mounts/a6c0da9b6145e18bcd661d5d6f8d52d3684a74feb456c6ed45a0cb764a879c43: bad message"
I0207 11:36:31.085111    3511 cri.go:54] listing CRI containers in root : {State:all Name:controller_ingress Namespaces:[]}
I0207 11:36:31.085308    3511 ssh_runner.go:195] Run: sudo crictl ps -a --quiet --name=controller_ingress
E0207 11:36:31.174028    3511 logs.go:281] Failed to list containers for "controller_ingress": crictl list: sudo crictl ps -a --quiet --name=controller_ingress: Process exited with status 1
stdout:

stderr:
time="2024-02-07T03:36:31Z" level=fatal msg="validate service connection: validate CRI v1 image API for endpoint \"unix:///var/run/cri-dockerd.sock\": rpc error: code = Unknown desc = readdirent /var/lib/docker/image/overlay2/layerdb/mounts/a6c0da9b6145e18bcd661d5d6f8d52d3684a74feb456c6ed45a0cb764a879c43: bad message"
I0207 11:36:31.174093    3511 logs.go:123] Gathering logs for describe nodes ...
I0207 11:36:31.174108    3511 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig"
W0207 11:36:31.294417    3511 logs.go:130] failed describe nodes: command: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig" /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig": Process exited with status 1
stdout:

stderr:
E0207 03:36:31.278224   19622 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:36:31.279617   19622 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:36:31.281399   19622 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:36:31.283606   19622 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:36:31.284118   19622 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
The connection to the server localhost:8443 was refused - did you specify the right host or port?
 output: 
** stderr ** 
E0207 03:36:31.278224   19622 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:36:31.279617   19622 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:36:31.281399   19622 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:36:31.283606   19622 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
E0207 03:36:31.284118   19622 memcache.go:265] couldn't get current server API group list: Get "https://localhost:8443/api?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused
The connection to the server localhost:8443 was refused - did you specify the right host or port?

** /stderr **
I0207 11:36:31.294455    3511 logs.go:123] Gathering logs for Docker ...
I0207 11:36:31.294468    3511 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u docker -u cri-docker -n 400"
I0207 11:36:31.427251    3511 logs.go:123] Gathering logs for container status ...
I0207 11:36:31.427276    3511 ssh_runner.go:195] Run: /bin/bash -c "sudo `which crictl || echo crictl` ps -a || sudo docker ps -a"
I0207 11:36:31.617042    3511 logs.go:123] Gathering logs for kubelet ...
I0207 11:36:31.617071    3511 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u kubelet -n 400"
I0207 11:36:31.713934    3511 logs.go:123] Gathering logs for dmesg ...
I0207 11:36:31.713949    3511 ssh_runner.go:195] Run: /bin/bash -c "sudo dmesg -PH -L=never --level warn,err,crit,alert,emerg | tail -n 400"
W0207 11:36:31.806705    3511 out.go:369] Error starting cluster: wait: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.28.3:$PATH" kubeadm init --config /var/tmp/minikube/kubeadm.yaml  --ignore-preflight-errors=DirAvailable--etc-kubernetes-manifests,DirAvailable--var-lib-minikube,DirAvailable--var-lib-minikube-etcd,FileAvailable--etc-kubernetes-manifests-kube-scheduler.yaml,FileAvailable--etc-kubernetes-manifests-kube-apiserver.yaml,FileAvailable--etc-kubernetes-manifests-kube-controller-manager.yaml,FileAvailable--etc-kubernetes-manifests-etcd.yaml,Port-10250,Swap,NumCPU,Mem": Process exited with status 1
stdout:
[init] Using Kubernetes version: v1.28.3
[preflight] Running pre-flight checks
[preflight] Pulling images required for setting up a Kubernetes cluster
[preflight] This might take a minute or two, depending on the speed of your internet connection
[preflight] You can also perform this action in beforehand using 'kubeadm config images pull'

stderr:
	[WARNING Service-Kubelet]: kubelet service is not enabled, please run 'systemctl enable kubelet.service'
error execution phase preflight: [preflight] Some fatal errors occurred:
	[ERROR ImagePull]: failed to pull image registry.k8s.io/kube-apiserver:v1.28.3: output: time="2024-02-07T03:36:27Z" level=fatal msg="validate service connection: validate CRI v1 image API for endpoint \"unix:///var/run/cri-dockerd.sock\": rpc error: code = Unknown desc = readdirent /var/lib/docker/image/overlay2/layerdb/mounts/a6c0da9b6145e18bcd661d5d6f8d52d3684a74feb456c6ed45a0cb764a879c43: bad message"
, error: exit status 1
	[ERROR ImagePull]: failed to pull image registry.k8s.io/kube-controller-manager:v1.28.3: output: time="2024-02-07T03:36:28Z" level=fatal msg="validate service connection: validate CRI v1 image API for endpoint \"unix:///var/run/cri-dockerd.sock\": rpc error: code = Unknown desc = readdirent /var/lib/docker/image/overlay2/layerdb/mounts/a6c0da9b6145e18bcd661d5d6f8d52d3684a74feb456c6ed45a0cb764a879c43: bad message"
, error: exit status 1
	[ERROR ImagePull]: failed to pull image registry.k8s.io/kube-scheduler:v1.28.3: output: time="2024-02-07T03:36:28Z" level=fatal msg="validate service connection: validate CRI v1 image API for endpoint \"unix:///var/run/cri-dockerd.sock\": rpc error: code = Unknown desc = readdirent /var/lib/docker/image/overlay2/layerdb/mounts/a6c0da9b6145e18bcd661d5d6f8d52d3684a74feb456c6ed45a0cb764a879c43: bad message"
, error: exit status 1
	[ERROR ImagePull]: failed to pull image registry.k8s.io/kube-proxy:v1.28.3: output: time="2024-02-07T03:36:28Z" level=fatal msg="validate service connection: validate CRI v1 image API for endpoint \"unix:///var/run/cri-dockerd.sock\": rpc error: code = Unknown desc = readdirent /var/lib/docker/image/overlay2/layerdb/mounts/a6c0da9b6145e18bcd661d5d6f8d52d3684a74feb456c6ed45a0cb764a879c43: bad message"
, error: exit status 1
	[ERROR ImagePull]: failed to pull image registry.k8s.io/pause:3.9: output: time="2024-02-07T03:36:29Z" level=fatal msg="validate service connection: validate CRI v1 image API for endpoint \"unix:///var/run/cri-dockerd.sock\": rpc error: code = Unknown desc = readdirent /var/lib/docker/image/overlay2/layerdb/mounts/a6c0da9b6145e18bcd661d5d6f8d52d3684a74feb456c6ed45a0cb764a879c43: bad message"
, error: exit status 1
	[ERROR ImagePull]: failed to pull image registry.k8s.io/etcd:3.5.9-0: output: time="2024-02-07T03:36:29Z" level=fatal msg="validate service connection: validate CRI v1 image API for endpoint \"unix:///var/run/cri-dockerd.sock\": rpc error: code = Unknown desc = readdirent /var/lib/docker/image/overlay2/layerdb/mounts/a6c0da9b6145e18bcd661d5d6f8d52d3684a74feb456c6ed45a0cb764a879c43: bad message"
, error: exit status 1
	[ERROR ImagePull]: failed to pull image registry.k8s.io/coredns/coredns:v1.10.1: output: time="2024-02-07T03:36:30Z" level=fatal msg="validate service connection: validate CRI v1 image API for endpoint \"unix:///var/run/cri-dockerd.sock\": rpc error: code = Unknown desc = readdirent /var/lib/docker/image/overlay2/layerdb/mounts/a6c0da9b6145e18bcd661d5d6f8d52d3684a74feb456c6ed45a0cb764a879c43: bad message"
, error: exit status 1
[preflight] If you know what you are doing, you can make a check non-fatal with `--ignore-preflight-errors=...`
To see the stack trace of this error execute with --v=5 or higher
W0207 11:36:31.806822    3511 out.go:239] 
W0207 11:36:31.807165    3511 out.go:239] üí£  Error starting cluster: wait: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.28.3:$PATH" kubeadm init --config /var/tmp/minikube/kubeadm.yaml  --ignore-preflight-errors=DirAvailable--etc-kubernetes-manifests,DirAvailable--var-lib-minikube,DirAvailable--var-lib-minikube-etcd,FileAvailable--etc-kubernetes-manifests-kube-scheduler.yaml,FileAvailable--etc-kubernetes-manifests-kube-apiserver.yaml,FileAvailable--etc-kubernetes-manifests-kube-controller-manager.yaml,FileAvailable--etc-kubernetes-manifests-etcd.yaml,Port-10250,Swap,NumCPU,Mem": Process exited with status 1
stdout:
[init] Using Kubernetes version: v1.28.3
[preflight] Running pre-flight checks
[preflight] Pulling images required for setting up a Kubernetes cluster
[preflight] This might take a minute or two, depending on the speed of your internet connection
[preflight] You can also perform this action in beforehand using 'kubeadm config images pull'

stderr:
	[WARNING Service-Kubelet]: kubelet service is not enabled, please run 'systemctl enable kubelet.service'
error execution phase preflight: [preflight] Some fatal errors occurred:
	[ERROR ImagePull]: failed to pull image registry.k8s.io/kube-apiserver:v1.28.3: output: time="2024-02-07T03:36:27Z" level=fatal msg="validate service connection: validate CRI v1 image API for endpoint \"unix:///var/run/cri-dockerd.sock\": rpc error: code = Unknown desc = readdirent /var/lib/docker/image/overlay2/layerdb/mounts/a6c0da9b6145e18bcd661d5d6f8d52d3684a74feb456c6ed45a0cb764a879c43: bad message"
, error: exit status 1
	[ERROR ImagePull]: failed to pull image registry.k8s.io/kube-controller-manager:v1.28.3: output: time="2024-02-07T03:36:28Z" level=fatal msg="validate service connection: validate CRI v1 image API for endpoint \"unix:///var/run/cri-dockerd.sock\": rpc error: code = Unknown desc = readdirent /var/lib/docker/image/overlay2/layerdb/mounts/a6c0da9b6145e18bcd661d5d6f8d52d3684a74feb456c6ed45a0cb764a879c43: bad message"
, error: exit status 1
	[ERROR ImagePull]: failed to pull image registry.k8s.io/kube-scheduler:v1.28.3: output: time="2024-02-07T03:36:28Z" level=fatal msg="validate service connection: validate CRI v1 image API for endpoint \"unix:///var/run/cri-dockerd.sock\": rpc error: code = Unknown desc = readdirent /var/lib/docker/image/overlay2/layerdb/mounts/a6c0da9b6145e18bcd661d5d6f8d52d3684a74feb456c6ed45a0cb764a879c43: bad message"
, error: exit status 1
	[ERROR ImagePull]: failed to pull image registry.k8s.io/kube-proxy:v1.28.3: output: time="2024-02-07T03:36:28Z" level=fatal msg="validate service connection: validate CRI v1 image API for endpoint \"unix:///var/run/cri-dockerd.sock\": rpc error: code = Unknown desc = readdirent /var/lib/docker/image/overlay2/layerdb/mounts/a6c0da9b6145e18bcd661d5d6f8d52d3684a74feb456c6ed45a0cb764a879c43: bad message"
, error: exit status 1
	[ERROR ImagePull]: failed to pull image registry.k8s.io/pause:3.9: output: time="2024-02-07T03:36:29Z" level=fatal msg="validate service connection: validate CRI v1 image API for endpoint \"unix:///var/run/cri-dockerd.sock\": rpc error: code = Unknown desc = readdirent /var/lib/docker/image/overlay2/layerdb/mounts/a6c0da9b6145e18bcd661d5d6f8d52d3684a74feb456c6ed45a0cb764a879c43: bad message"
, error: exit status 1
	[ERROR ImagePull]: failed to pull image registry.k8s.io/etcd:3.5.9-0: output: time="2024-02-07T03:36:29Z" level=fatal msg="validate service connection: validate CRI v1 image API for endpoint \"unix:///var/run/cri-dockerd.sock\": rpc error: code = Unknown desc = readdirent /var/lib/docker/image/overlay2/layerdb/mounts/a6c0da9b6145e18bcd661d5d6f8d52d3684a74feb456c6ed45a0cb764a879c43: bad message"
, error: exit status 1
	[ERROR ImagePull]: failed to pull image registry.k8s.io/coredns/coredns:v1.10.1: output: time="2024-02-07T03:36:30Z" level=fatal msg="validate service connection: validate CRI v1 image API for endpoint \"unix:///var/run/cri-dockerd.sock\": rpc error: code = Unknown desc = readdirent /var/lib/docker/image/overlay2/layerdb/mounts/a6c0da9b6145e18bcd661d5d6f8d52d3684a74feb456c6ed45a0cb764a879c43: bad message"
, error: exit status 1
[preflight] If you know what you are doing, you can make a check non-fatal with `--ignore-preflight-errors=...`
To see the stack trace of this error execute with --v=5 or higher

W0207 11:36:31.807410    3511 out.go:239] 
W0207 11:36:31.813378    3511 out.go:239] [31m‚ï≠‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïÆ[0m
[31m‚îÇ[0m                                                                                           [31m‚îÇ[0m
[31m‚îÇ[0m    üòø  If the above advice does not help, please let us know:                             [31m‚îÇ[0m
[31m‚îÇ[0m    üëâ  https://github.com/kubernetes/minikube/issues/new/choose                           [31m‚îÇ[0m
[31m‚îÇ[0m                                                                                           [31m‚îÇ[0m
[31m‚îÇ[0m    Please run `minikube logs --file=logs.txt` and attach logs.txt to the GitHub issue.    [31m‚îÇ[0m
[31m‚îÇ[0m                                                                                           [31m‚îÇ[0m
[31m‚ï∞‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïØ[0m
I0207 11:36:31.862624    3511 out.go:177] 
W0207 11:36:31.887650    3511 out.go:239] ‚ùå  Exiting due to GUEST_START: failed to start node: wait: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.28.3:$PATH" kubeadm init --config /var/tmp/minikube/kubeadm.yaml  --ignore-preflight-errors=DirAvailable--etc-kubernetes-manifests,DirAvailable--var-lib-minikube,DirAvailable--var-lib-minikube-etcd,FileAvailable--etc-kubernetes-manifests-kube-scheduler.yaml,FileAvailable--etc-kubernetes-manifests-kube-apiserver.yaml,FileAvailable--etc-kubernetes-manifests-kube-controller-manager.yaml,FileAvailable--etc-kubernetes-manifests-etcd.yaml,Port-10250,Swap,NumCPU,Mem": Process exited with status 1
stdout:
[init] Using Kubernetes version: v1.28.3
[preflight] Running pre-flight checks
[preflight] Pulling images required for setting up a Kubernetes cluster
[preflight] This might take a minute or two, depending on the speed of your internet connection
[preflight] You can also perform this action in beforehand using 'kubeadm config images pull'

stderr:
	[WARNING Service-Kubelet]: kubelet service is not enabled, please run 'systemctl enable kubelet.service'
error execution phase preflight: [preflight] Some fatal errors occurred:
	[ERROR ImagePull]: failed to pull image registry.k8s.io/kube-apiserver:v1.28.3: output: time="2024-02-07T03:36:27Z" level=fatal msg="validate service connection: validate CRI v1 image API for endpoint \"unix:///var/run/cri-dockerd.sock\": rpc error: code = Unknown desc = readdirent /var/lib/docker/image/overlay2/layerdb/mounts/a6c0da9b6145e18bcd661d5d6f8d52d3684a74feb456c6ed45a0cb764a879c43: bad message"
, error: exit status 1
	[ERROR ImagePull]: failed to pull image registry.k8s.io/kube-controller-manager:v1.28.3: output: time="2024-02-07T03:36:28Z" level=fatal msg="validate service connection: validate CRI v1 image API for endpoint \"unix:///var/run/cri-dockerd.sock\": rpc error: code = Unknown desc = readdirent /var/lib/docker/image/overlay2/layerdb/mounts/a6c0da9b6145e18bcd661d5d6f8d52d3684a74feb456c6ed45a0cb764a879c43: bad message"
, error: exit status 1
	[ERROR ImagePull]: failed to pull image registry.k8s.io/kube-scheduler:v1.28.3: output: time="2024-02-07T03:36:28Z" level=fatal msg="validate service connection: validate CRI v1 image API for endpoint \"unix:///var/run/cri-dockerd.sock\": rpc error: code = Unknown desc = readdirent /var/lib/docker/image/overlay2/layerdb/mounts/a6c0da9b6145e18bcd661d5d6f8d52d3684a74feb456c6ed45a0cb764a879c43: bad message"
, error: exit status 1
	[ERROR ImagePull]: failed to pull image registry.k8s.io/kube-proxy:v1.28.3: output: time="2024-02-07T03:36:28Z" level=fatal msg="validate service connection: validate CRI v1 image API for endpoint \"unix:///var/run/cri-dockerd.sock\": rpc error: code = Unknown desc = readdirent /var/lib/docker/image/overlay2/layerdb/mounts/a6c0da9b6145e18bcd661d5d6f8d52d3684a74feb456c6ed45a0cb764a879c43: bad message"
, error: exit status 1
	[ERROR ImagePull]: failed to pull image registry.k8s.io/pause:3.9: output: time="2024-02-07T03:36:29Z" level=fatal msg="validate service connection: validate CRI v1 image API for endpoint \"unix:///var/run/cri-dockerd.sock\": rpc error: code = Unknown desc = readdirent /var/lib/docker/image/overlay2/layerdb/mounts/a6c0da9b6145e18bcd661d5d6f8d52d3684a74feb456c6ed45a0cb764a879c43: bad message"
, error: exit status 1
	[ERROR ImagePull]: failed to pull image registry.k8s.io/etcd:3.5.9-0: output: time="2024-02-07T03:36:29Z" level=fatal msg="validate service connection: validate CRI v1 image API for endpoint \"unix:///var/run/cri-dockerd.sock\": rpc error: code = Unknown desc = readdirent /var/lib/docker/image/overlay2/layerdb/mounts/a6c0da9b6145e18bcd661d5d6f8d52d3684a74feb456c6ed45a0cb764a879c43: bad message"
, error: exit status 1
	[ERROR ImagePull]: failed to pull image registry.k8s.io/coredns/coredns:v1.10.1: output: time="2024-02-07T03:36:30Z" level=fatal msg="validate service connection: validate CRI v1 image API for endpoint \"unix:///var/run/cri-dockerd.sock\": rpc error: code = Unknown desc = readdirent /var/lib/docker/image/overlay2/layerdb/mounts/a6c0da9b6145e18bcd661d5d6f8d52d3684a74feb456c6ed45a0cb764a879c43: bad message"
, error: exit status 1
[preflight] If you know what you are doing, you can make a check non-fatal with `--ignore-preflight-errors=...`
To see the stack trace of this error execute with --v=5 or higher

W0207 11:36:31.888297    3511 out.go:239] 
W0207 11:36:31.890523    3511 out.go:239] [31m‚ï≠‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïÆ[0m
[31m‚îÇ[0m                                                                                           [31m‚îÇ[0m
[31m‚îÇ[0m    üòø  If the above advice does not help, please let us know:                             [31m‚îÇ[0m
[31m‚îÇ[0m    üëâ  https://github.com/kubernetes/minikube/issues/new/choose                           [31m‚îÇ[0m
[31m‚îÇ[0m                                                                                           [31m‚îÇ[0m
[31m‚îÇ[0m    Please run `minikube logs --file=logs.txt` and attach logs.txt to the GitHub issue.    [31m‚îÇ[0m
[31m‚îÇ[0m                                                                                           [31m‚îÇ[0m
[31m‚ï∞‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïØ[0m
I0207 11:36:31.916091    3511 out.go:177] 

* 
* ==> Docker <==
* -- Journal begins at Wed 2024-02-07 03:31:20 UTC, ends at Wed 2024-02-07 03:40:43 UTC. --
Feb 07 03:36:18 minikube cri-dockerd[1302]: time="2024-02-07T03:36:18Z" level=info msg="Both sandbox container and checkpoint could not be found with id \"deprecated,\". Proceed without further sandbox information."
Feb 07 03:36:18 minikube cri-dockerd[1302]: time="2024-02-07T03:36:18Z" level=error msg="Failed to delete corrupt checkpoint for sandbox deprecated,: invalid key: \"deprecated,\""
Feb 07 03:36:18 minikube cri-dockerd[1302]: time="2024-02-07T03:36:18Z" level=info msg="Both sandbox container and checkpoint could not be found with id \"deprecated,\". Proceed without further sandbox information."
Feb 07 03:36:18 minikube cri-dockerd[1302]: time="2024-02-07T03:36:18Z" level=error msg="Failed to delete corrupt checkpoint for sandbox deprecated,: invalid key: \"deprecated,\""
Feb 07 03:36:18 minikube cri-dockerd[1302]: time="2024-02-07T03:36:18Z" level=info msg="Both sandbox container and checkpoint could not be found with id \"deprecated,\". Proceed without further sandbox information."
Feb 07 03:36:18 minikube cri-dockerd[1302]: time="2024-02-07T03:36:18Z" level=info msg="Both sandbox container and checkpoint could not be found with id \"please\". Proceed without further sandbox information."
Feb 07 03:36:18 minikube cri-dockerd[1302]: time="2024-02-07T03:36:18Z" level=info msg="Both sandbox container and checkpoint could not be found with id \"please\". Proceed without further sandbox information."
Feb 07 03:36:18 minikube cri-dockerd[1302]: time="2024-02-07T03:36:18Z" level=info msg="Both sandbox container and checkpoint could not be found with id \"please\". Proceed without further sandbox information."
Feb 07 03:36:18 minikube cri-dockerd[1302]: time="2024-02-07T03:36:18Z" level=info msg="Both sandbox container and checkpoint could not be found with id \"please\". Proceed without further sandbox information."
Feb 07 03:36:18 minikube cri-dockerd[1302]: time="2024-02-07T03:36:18Z" level=info msg="Both sandbox container and checkpoint could not be found with id \"please\". Proceed without further sandbox information."
Feb 07 03:36:18 minikube cri-dockerd[1302]: time="2024-02-07T03:36:18Z" level=info msg="Both sandbox container and checkpoint could not be found with id \"consider\". Proceed without further sandbox information."
Feb 07 03:36:18 minikube cri-dockerd[1302]: time="2024-02-07T03:36:18Z" level=info msg="Both sandbox container and checkpoint could not be found with id \"consider\". Proceed without further sandbox information."
Feb 07 03:36:18 minikube cri-dockerd[1302]: time="2024-02-07T03:36:18Z" level=info msg="Both sandbox container and checkpoint could not be found with id \"consider\". Proceed without further sandbox information."
Feb 07 03:36:19 minikube cri-dockerd[1302]: time="2024-02-07T03:36:19Z" level=info msg="Both sandbox container and checkpoint could not be found with id \"consider\". Proceed without further sandbox information."
Feb 07 03:36:19 minikube cri-dockerd[1302]: time="2024-02-07T03:36:19Z" level=info msg="Both sandbox container and checkpoint could not be found with id \"consider\". Proceed without further sandbox information."
Feb 07 03:36:19 minikube cri-dockerd[1302]: time="2024-02-07T03:36:19Z" level=info msg="Both sandbox container and checkpoint could not be found with id \"using\". Proceed without further sandbox information."
Feb 07 03:36:19 minikube cri-dockerd[1302]: time="2024-02-07T03:36:19Z" level=info msg="Both sandbox container and checkpoint could not be found with id \"using\". Proceed without further sandbox information."
Feb 07 03:36:19 minikube cri-dockerd[1302]: time="2024-02-07T03:36:19Z" level=info msg="Both sandbox container and checkpoint could not be found with id \"using\". Proceed without further sandbox information."
Feb 07 03:36:19 minikube cri-dockerd[1302]: time="2024-02-07T03:36:19Z" level=info msg="Both sandbox container and checkpoint could not be found with id \"using\". Proceed without further sandbox information."
Feb 07 03:36:19 minikube cri-dockerd[1302]: time="2024-02-07T03:36:19Z" level=info msg="Both sandbox container and checkpoint could not be found with id \"using\". Proceed without further sandbox information."
Feb 07 03:36:19 minikube cri-dockerd[1302]: time="2024-02-07T03:36:19Z" level=info msg="Both sandbox container and checkpoint could not be found with id \"full\". Proceed without further sandbox information."
Feb 07 03:36:19 minikube cri-dockerd[1302]: time="2024-02-07T03:36:19Z" level=info msg="Both sandbox container and checkpoint could not be found with id \"full\". Proceed without further sandbox information."
Feb 07 03:36:19 minikube cri-dockerd[1302]: time="2024-02-07T03:36:19Z" level=info msg="Both sandbox container and checkpoint could not be found with id \"full\". Proceed without further sandbox information."
Feb 07 03:36:19 minikube cri-dockerd[1302]: time="2024-02-07T03:36:19Z" level=info msg="Both sandbox container and checkpoint could not be found with id \"full\". Proceed without further sandbox information."
Feb 07 03:36:19 minikube cri-dockerd[1302]: time="2024-02-07T03:36:19Z" level=info msg="Both sandbox container and checkpoint could not be found with id \"full\". Proceed without further sandbox information."
Feb 07 03:36:20 minikube cri-dockerd[1302]: time="2024-02-07T03:36:20Z" level=error msg="Failed to delete corrupt checkpoint for sandbox format\": invalid key: \"format\\\"\""
Feb 07 03:36:20 minikube cri-dockerd[1302]: time="2024-02-07T03:36:20Z" level=info msg="Both sandbox container and checkpoint could not be found with id \"format\\\"\". Proceed without further sandbox information."
Feb 07 03:36:20 minikube cri-dockerd[1302]: time="2024-02-07T03:36:20Z" level=error msg="Failed to delete corrupt checkpoint for sandbox format\": invalid key: \"format\\\"\""
Feb 07 03:36:20 minikube cri-dockerd[1302]: time="2024-02-07T03:36:20Z" level=info msg="Both sandbox container and checkpoint could not be found with id \"format\\\"\". Proceed without further sandbox information."
Feb 07 03:36:20 minikube cri-dockerd[1302]: time="2024-02-07T03:36:20Z" level=error msg="Failed to delete corrupt checkpoint for sandbox format\": invalid key: \"format\\\"\""
Feb 07 03:36:20 minikube cri-dockerd[1302]: time="2024-02-07T03:36:20Z" level=info msg="Both sandbox container and checkpoint could not be found with id \"format\\\"\". Proceed without further sandbox information."
Feb 07 03:36:20 minikube cri-dockerd[1302]: time="2024-02-07T03:36:20Z" level=error msg="Failed to delete corrupt checkpoint for sandbox format\": invalid key: \"format\\\"\""
Feb 07 03:36:20 minikube cri-dockerd[1302]: time="2024-02-07T03:36:20Z" level=info msg="Both sandbox container and checkpoint could not be found with id \"format\\\"\". Proceed without further sandbox information."
Feb 07 03:36:20 minikube cri-dockerd[1302]: time="2024-02-07T03:36:20Z" level=error msg="Failed to delete corrupt checkpoint for sandbox format\": invalid key: \"format\\\"\""
Feb 07 03:36:20 minikube cri-dockerd[1302]: time="2024-02-07T03:36:20Z" level=info msg="Both sandbox container and checkpoint could not be found with id \"format\\\"\". Proceed without further sandbox information."
Feb 07 03:36:20 minikube cri-dockerd[1302]: time="2024-02-07T03:36:20Z" level=error msg="Failed to delete corrupt checkpoint for sandbox endpoint=\"/var/run/cri-dockerd.sock\": invalid key: \"endpoint=\\\"/var/run/cri-dockerd.sock\\\"\""
Feb 07 03:36:20 minikube cri-dockerd[1302]: time="2024-02-07T03:36:20Z" level=info msg="Both sandbox container and checkpoint could not be found with id \"endpoint=\\\"/var/run/cri-dockerd.sock\\\"\". Proceed without further sandbox information."
Feb 07 03:36:20 minikube cri-dockerd[1302]: time="2024-02-07T03:36:20Z" level=error msg="Failed to delete corrupt checkpoint for sandbox endpoint=\"/var/run/cri-dockerd.sock\": invalid key: \"endpoint=\\\"/var/run/cri-dockerd.sock\\\"\""
Feb 07 03:36:20 minikube cri-dockerd[1302]: time="2024-02-07T03:36:20Z" level=info msg="Both sandbox container and checkpoint could not be found with id \"endpoint=\\\"/var/run/cri-dockerd.sock\\\"\". Proceed without further sandbox information."
Feb 07 03:36:20 minikube cri-dockerd[1302]: time="2024-02-07T03:36:20Z" level=error msg="Failed to delete corrupt checkpoint for sandbox endpoint=\"/var/run/cri-dockerd.sock\": invalid key: \"endpoint=\\\"/var/run/cri-dockerd.sock\\\"\""
Feb 07 03:36:20 minikube cri-dockerd[1302]: time="2024-02-07T03:36:20Z" level=info msg="Both sandbox container and checkpoint could not be found with id \"endpoint=\\\"/var/run/cri-dockerd.sock\\\"\". Proceed without further sandbox information."
Feb 07 03:36:20 minikube cri-dockerd[1302]: time="2024-02-07T03:36:20Z" level=error msg="Failed to delete corrupt checkpoint for sandbox endpoint=\"/var/run/cri-dockerd.sock\": invalid key: \"endpoint=\\\"/var/run/cri-dockerd.sock\\\"\""
Feb 07 03:36:20 minikube cri-dockerd[1302]: time="2024-02-07T03:36:20Z" level=info msg="Both sandbox container and checkpoint could not be found with id \"endpoint=\\\"/var/run/cri-dockerd.sock\\\"\". Proceed without further sandbox information."
Feb 07 03:36:21 minikube cri-dockerd[1302]: time="2024-02-07T03:36:21Z" level=error msg="Failed to delete corrupt checkpoint for sandbox endpoint=\"/var/run/cri-dockerd.sock\": invalid key: \"endpoint=\\\"/var/run/cri-dockerd.sock\\\"\""
Feb 07 03:36:21 minikube cri-dockerd[1302]: time="2024-02-07T03:36:21Z" level=info msg="Both sandbox container and checkpoint could not be found with id \"endpoint=\\\"/var/run/cri-dockerd.sock\\\"\". Proceed without further sandbox information."
Feb 07 03:36:21 minikube cri-dockerd[1302]: time="2024-02-07T03:36:21Z" level=error msg="Failed to delete corrupt checkpoint for sandbox URL=\"unix:///var/run/cri-dockerd.sock\": invalid key: \"URL=\\\"unix:///var/run/cri-dockerd.sock\\\"\""
Feb 07 03:36:21 minikube cri-dockerd[1302]: time="2024-02-07T03:36:21Z" level=error msg="Failed to delete corrupt checkpoint for sandbox URL=\"unix:///var/run/cri-dockerd.sock\": invalid key: \"URL=\\\"unix:///var/run/cri-dockerd.sock\\\"\""
Feb 07 03:36:21 minikube cri-dockerd[1302]: time="2024-02-07T03:36:21Z" level=error msg="Failed to delete corrupt checkpoint for sandbox URL=\"unix:///var/run/cri-dockerd.sock\": invalid key: \"URL=\\\"unix:///var/run/cri-dockerd.sock\\\"\""
Feb 07 03:36:21 minikube cri-dockerd[1302]: time="2024-02-07T03:36:21Z" level=error msg="Failed to delete corrupt checkpoint for sandbox URL=\"unix:///var/run/cri-dockerd.sock\": invalid key: \"URL=\\\"unix:///var/run/cri-dockerd.sock\\\"\""
Feb 07 03:36:21 minikube cri-dockerd[1302]: time="2024-02-07T03:36:21Z" level=error msg="Failed to delete corrupt checkpoint for sandbox URL=\"unix:///var/run/cri-dockerd.sock\": invalid key: \"URL=\\\"unix:///var/run/cri-dockerd.sock\\\"\""
Feb 07 03:36:24 minikube dockerd[1045]: time="2024-02-07T03:36:24.897249140Z" level=error msg="Error removing init layer 0f2709b87b9b6f810aafd284e0d5f9d8b50e2eda374acd4473cc3ebe67c3dc32: readdirnames /var/lib/docker/overlay2/ebc7a649ba102b7215f6dbb045c6ac78a380de0b99828383e82e406539237056-init/diff/dev: readdirent dev: bad message"
Feb 07 03:36:24 minikube dockerd[1045]: time="2024-02-07T03:36:24.897924093Z" level=error msg="Handler for DELETE /v1.42/containers/0f2709b87b9b6f810aafd284e0d5f9d8b50e2eda374acd4473cc3ebe67c3dc32 returned error: container 0f2709b87b9b6f810aafd284e0d5f9d8b50e2eda374acd4473cc3ebe67c3dc32: driver \"overlay2\" failed to remove root filesystem: readdirnames /var/lib/docker/overlay2/ebc7a649ba102b7215f6dbb045c6ac78a380de0b99828383e82e406539237056-init/diff/dev: readdirent dev: bad message"
Feb 07 03:36:25 minikube dockerd[1045]: time="2024-02-07T03:36:25.070043947Z" level=error msg="Error removing init layer 0f2709b87b9b6f810aafd284e0d5f9d8b50e2eda374acd4473cc3ebe67c3dc32: readdirnames /var/lib/docker/overlay2/ebc7a649ba102b7215f6dbb045c6ac78a380de0b99828383e82e406539237056-init/diff/dev: readdirent dev: bad message"
Feb 07 03:36:25 minikube dockerd[1045]: time="2024-02-07T03:36:25.070339431Z" level=error msg="Handler for DELETE /v1.42/containers/0f2709b87b9b6f810aafd284e0d5f9d8b50e2eda374acd4473cc3ebe67c3dc32 returned error: container 0f2709b87b9b6f810aafd284e0d5f9d8b50e2eda374acd4473cc3ebe67c3dc32: driver \"overlay2\" failed to remove root filesystem: readdirnames /var/lib/docker/overlay2/ebc7a649ba102b7215f6dbb045c6ac78a380de0b99828383e82e406539237056-init/diff/dev: readdirent dev: bad message"
Feb 07 03:36:25 minikube dockerd[1045]: time="2024-02-07T03:36:25.207077196Z" level=error msg="Error removing init layer 0f2709b87b9b6f810aafd284e0d5f9d8b50e2eda374acd4473cc3ebe67c3dc32: readdirnames /var/lib/docker/overlay2/ebc7a649ba102b7215f6dbb045c6ac78a380de0b99828383e82e406539237056-init/diff/dev: readdirent dev: bad message"
Feb 07 03:36:25 minikube dockerd[1045]: time="2024-02-07T03:36:25.207498341Z" level=error msg="Handler for DELETE /v1.42/containers/0f2709b87b9b6f810aafd284e0d5f9d8b50e2eda374acd4473cc3ebe67c3dc32 returned error: container 0f2709b87b9b6f810aafd284e0d5f9d8b50e2eda374acd4473cc3ebe67c3dc32: driver \"overlay2\" failed to remove root filesystem: readdirnames /var/lib/docker/overlay2/ebc7a649ba102b7215f6dbb045c6ac78a380de0b99828383e82e406539237056-init/diff/dev: readdirent dev: bad message"
Feb 07 03:36:25 minikube dockerd[1045]: time="2024-02-07T03:36:25.384398876Z" level=error msg="Error removing init layer 0f2709b87b9b6f810aafd284e0d5f9d8b50e2eda374acd4473cc3ebe67c3dc32: readdirnames /var/lib/docker/overlay2/ebc7a649ba102b7215f6dbb045c6ac78a380de0b99828383e82e406539237056-init/diff/dev: readdirent dev: bad message"
Feb 07 03:36:25 minikube dockerd[1045]: time="2024-02-07T03:36:25.384861475Z" level=error msg="Handler for DELETE /v1.42/containers/0f2709b87b9b6f810aafd284e0d5f9d8b50e2eda374acd4473cc3ebe67c3dc32 returned error: container 0f2709b87b9b6f810aafd284e0d5f9d8b50e2eda374acd4473cc3ebe67c3dc32: driver \"overlay2\" failed to remove root filesystem: readdirnames /var/lib/docker/overlay2/ebc7a649ba102b7215f6dbb045c6ac78a380de0b99828383e82e406539237056-init/diff/dev: readdirent dev: bad message"
Feb 07 03:36:25 minikube dockerd[1045]: time="2024-02-07T03:36:25.540107468Z" level=error msg="Error removing init layer 0f2709b87b9b6f810aafd284e0d5f9d8b50e2eda374acd4473cc3ebe67c3dc32: readdirnames /var/lib/docker/overlay2/ebc7a649ba102b7215f6dbb045c6ac78a380de0b99828383e82e406539237056-init/diff/dev: readdirent dev: bad message"
Feb 07 03:36:25 minikube dockerd[1045]: time="2024-02-07T03:36:25.540269187Z" level=error msg="Handler for DELETE /v1.42/containers/0f2709b87b9b6f810aafd284e0d5f9d8b50e2eda374acd4473cc3ebe67c3dc32 returned error: container 0f2709b87b9b6f810aafd284e0d5f9d8b50e2eda374acd4473cc3ebe67c3dc32: driver \"overlay2\" failed to remove root filesystem: readdirnames /var/lib/docker/overlay2/ebc7a649ba102b7215f6dbb045c6ac78a380de0b99828383e82e406539237056-init/diff/dev: readdirent dev: bad message"

* 
* ==> container status <==
* CONTAINER ID   IMAGE                                                COMMAND                  CREATED        STATUS                      PORTS     NAMES
7bd4daa54598   b6188b3dc37c                                         "docker-entrypoint.s‚Ä¶"   17 hours ago   Exited (137) 15 hours ago             k8s_mysql_mysql-d6f56dd44-lzgxs_default_38e52deb-5035-4d5a-99ac-1a0d74b41802_0
d3f7aad57f44   registry.k8s.io/pause:3.9                            "/pause"                 17 hours ago   Exited (0) 15 hours ago               k8s_POD_mysql-d6f56dd44-lzgxs_default_38e52deb-5035-4d5a-99ac-1a0d74b41802_0
5631cbc21ffd   07655ddf2eeb                                         "/dashboard --insecu‚Ä¶"   20 hours ago   Exited (2) 15 hours ago               k8s_kubernetes-dashboard_kubernetes-dashboard-8694d4445c-98m6d_kubernetes-dashboard_891a3e24-cbc0-45a4-90b6-878128ecd052_1
a22f4fd8f498   kubernetesui/metrics-scraper                         "/metrics-sidecar"       25 hours ago   Exited (2) 15 hours ago               k8s_dashboard-metrics-scraper_dashboard-metrics-scraper-7fd5cb4ddc-nqbmj_kubernetes-dashboard_961f7e92-0cd3-4cf8-aa25-a56fcc5a2e55_0
f4b96dcbc9f2   kubernetesui/dashboard                               "/dashboard --insecu‚Ä¶"   25 hours ago   Exited (2) 20 hours ago               k8s_kubernetes-dashboard_kubernetes-dashboard-8694d4445c-98m6d_kubernetes-dashboard_891a3e24-cbc0-45a4-90b6-878128ecd052_0
7ecdfa8af41d   registry.k8s.io/pause:3.9                            "/pause"                 25 hours ago   Exited (0) 15 hours ago               k8s_POD_dashboard-metrics-scraper-7fd5cb4ddc-nqbmj_kubernetes-dashboard_961f7e92-0cd3-4cf8-aa25-a56fcc5a2e55_0
38892c590963   registry.k8s.io/pause:3.9                            "/pause"                 25 hours ago   Exited (0) 15 hours ago               k8s_POD_kubernetes-dashboard-8694d4445c-98m6d_kubernetes-dashboard_891a3e24-cbc0-45a4-90b6-878128ecd052_0
8d50139a5311   registry.k8s.io/ingress-nginx/controller             "/usr/bin/dumb-init ‚Ä¶"   25 hours ago   Exited (137) 15 hours ago             k8s_controller_ingress-nginx-controller-7c6974c4d8-zzzzg_ingress-nginx_01663013-f060-4c5d-9a52-057372855f63_0
4361038aae5c   registry.k8s.io/pause:3.9                            "/pause"                 26 hours ago   Exited (0) 15 hours ago               k8s_POD_ingress-nginx-controller-7c6974c4d8-zzzzg_ingress-nginx_01663013-f060-4c5d-9a52-057372855f63_0
d5ecc41880f1   registry.k8s.io/ingress-nginx/kube-webhook-certgen   "/kube-webhook-certg‚Ä¶"   26 hours ago   Exited (0) 26 hours ago               k8s_create_ingress-nginx-admission-create-n5rlv_ingress-nginx_2539773f-fdba-498f-8036-2aa51c67cd6f_0
d9af6b85df0a   registry.k8s.io/ingress-nginx/kube-webhook-certgen   "/kube-webhook-certg‚Ä¶"   26 hours ago   Exited (0) 26 hours ago               k8s_patch_ingress-nginx-admission-patch-zvcfw_ingress-nginx_00fe9e9a-2972-499c-8f6b-17fa1ac320ce_0
4b8a7cc812f7   registry.k8s.io/pause:3.9                            "/pause"                 26 hours ago   Exited (0) 26 hours ago               k8s_POD_ingress-nginx-admission-create-n5rlv_ingress-nginx_2539773f-fdba-498f-8036-2aa51c67cd6f_0
ee9e729230d3   registry.k8s.io/pause:3.9                            "/pause"                 26 hours ago   Exited (0) 26 hours ago               k8s_POD_ingress-nginx-admission-patch-zvcfw_ingress-nginx_00fe9e9a-2972-499c-8f6b-17fa1ac320ce_0
c4ab02b60bba   mongo                                                "docker-entrypoint.s‚Ä¶"   26 hours ago   Exited (0) 15 hours ago               k8s_mongodb_mongodb-deployment-699744c7d-wj6c6_default_682d674c-e428-4650-be2e-305272ad33b8_1
41620038e632   mongo-express                                        "/sbin/tini -- /dock‚Ä¶"   26 hours ago   Exited (143) 15 hours ago             k8s_mongo-express_mongo-express-859f75dd4f-8s2wq_default_90418dc0-eac0-4e4b-8974-42a47fbe2f5a_1
8e23971f30ef   nginx                                                "/docker-entrypoint.‚Ä¶"   26 hours ago   Exited (0) 15 hours ago               k8s_nginx_nginx-depl-6777bffb6f-4mg9q_default_226bb4c8-94b8-4032-8abb-684ea3523b5e_1
06fb15ac4a32   ead0a4a53df8                                         "/coredns -conf /etc‚Ä¶"   26 hours ago   Exited (0) 15 hours ago               k8s_coredns_coredns-5dd5756b68-dnzfc_kube-system_36daac5e-9558-4c87-aa12-a656d556cb2f_1
e2fb00ef9976   registry.k8s.io/pause:3.9                            "/pause"                 26 hours ago   Exited (0) 15 hours ago               k8s_POD_mongo-express-859f75dd4f-8s2wq_default_90418dc0-eac0-4e4b-8974-42a47fbe2f5a_1
656be3392aca   registry.k8s.io/pause:3.9                            "/pause"                 26 hours ago   Exited (0) 15 hours ago               k8s_POD_mongodb-deployment-699744c7d-wj6c6_default_682d674c-e428-4650-be2e-305272ad33b8_1
31087e631089   registry.k8s.io/pause:3.9                            "/pause"                 26 hours ago   Exited (0) 15 hours ago               k8s_POD_nginx-depl-6777bffb6f-4mg9q_default_226bb4c8-94b8-4032-8abb-684ea3523b5e_1
06f9e3cf4f3c   registry.k8s.io/pause:3.9                            "/pause"                 26 hours ago   Exited (0) 15 hours ago               k8s_POD_coredns-5dd5756b68-dnzfc_kube-system_36daac5e-9558-4c87-aa12-a656d556cb2f_1
0f2709b87b9b   registry.k8s.io/pause:3.9                            "/pause"                 26 hours ago   Removal In Progress                   k8s_POD_storage-provisioner_kube-system_352d8b76-e39d-4d08-a5f8-d4ec0c39dd3a_1
e1bb8361945e   10baa1ca1706                                         "kube-controller-man‚Ä¶"   26 hours ago   Exited (2) 15 hours ago               k8s_kube-controller-manager_kube-controller-manager-minikube_kube-system_11fc41667a2819cdb15b7270cb5cd200_2
1d1e9abd9836   6d1b4fd1b182                                         "kube-scheduler --au‚Ä¶"   26 hours ago   Exited (1) 15 hours ago               k8s_kube-scheduler_kube-scheduler-minikube_kube-system_75ac196d3709dde303d8a81c035c2c28_2
1eed8f4e884e   73deb9a3f702                                         "etcd --advertise-cl‚Ä¶"   26 hours ago   Removal In Progress                   k8s_etcd_etcd-minikube_kube-system_a416a405783312bf7eb663c9d20d55fd_1
f2d14835af70   nginx                                                "/docker-entrypoint.‚Ä¶"   37 hours ago   Exited (255) 26 hours ago             k8s_nginx_nginx-depl-6777bffb6f-4mg9q_default_226bb4c8-94b8-4032-8abb-684ea3523b5e_0
d3c3dabf73f8   registry.k8s.io/pause:3.9                            "/pause"                 37 hours ago   Exited (255) 26 hours ago             k8s_POD_nginx-depl-6777bffb6f-4mg9q_default_226bb4c8-94b8-4032-8abb-684ea3523b5e_0
fb2c39c5cb36   mongo-express                                        "/sbin/tini -- /dock‚Ä¶"   41 hours ago   Exited (255) 26 hours ago             k8s_mongo-express_mongo-express-859f75dd4f-8s2wq_default_90418dc0-eac0-4e4b-8974-42a47fbe2f5a_0
7a8dc1a9e7f3   registry.k8s.io/pause:3.9                            "/pause"                 41 hours ago   Exited (255) 26 hours ago             k8s_POD_mongo-express-859f75dd4f-8s2wq_default_90418dc0-eac0-4e4b-8974-42a47fbe2f5a_0
051baa613650   mongo                                                "docker-entrypoint.s‚Ä¶"   41 hours ago   Exited (255) 26 hours ago             k8s_mongodb_mongodb-deployment-699744c7d-wj6c6_default_682d674c-e428-4650-be2e-305272ad33b8_0
678d3babc1ba   registry.k8s.io/pause:3.9                            "/pause"                 41 hours ago   Exited (255) 26 hours ago             k8s_POD_mongodb-deployment-699744c7d-wj6c6_default_682d674c-e428-4650-be2e-305272ad33b8_0
b94028402ad3   ead0a4a53df8                                         "/coredns -conf /etc‚Ä¶"   41 hours ago   Exited (255) 26 hours ago             k8s_coredns_coredns-5dd5756b68-dnzfc_kube-system_36daac5e-9558-4c87-aa12-a656d556cb2f_0
dacc030a6bab   registry.k8s.io/pause:3.9                            "/pause"                 41 hours ago   Exited (255) 26 hours ago             k8s_POD_coredns-5dd5756b68-dnzfc_kube-system_36daac5e-9558-4c87-aa12-a656d556cb2f_0
885389110d12   10baa1ca1706                                         "kube-controller-man‚Ä¶"   41 hours ago   Exited (255) 26 hours ago             k8s_kube-controller-manager_kube-controller-manager-minikube_kube-system_11fc41667a2819cdb15b7270cb5cd200_1
time="2024-02-07T03:40:43Z" level=fatal msg="validate service connection: validate CRI v1 image API for endpoint \"unix:///var/run/cri-dockerd.sock\": rpc error: code = Unknown desc = readdirent /var/lib/docker/image/overlay2/layerdb/mounts/a6c0da9b6145e18bcd661d5d6f8d52d3684a74feb456c6ed45a0cb764a879c43: bad message"

* 
* ==> controller_ingress [8d50139a5311] <==
* 192.168.59.1 - - [06/Feb/2024:02:35:27 +0000] "GET /api/v1/replicaset/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp HTTP/1.1" 200 738 "http://dashboard.com/" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36" 409 0.177 [kubernetes-dashboard-kubernetes-dashboard-80] [] 10.244.0.19:9090 738 0.177 200 79c36db2b15d4322137027b02787089b
192.168.59.1 - - [06/Feb/2024:02:35:27 +0000] "GET /api/v1/pod/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp HTTP/1.1" 200 665 "http://dashboard.com/" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36" 402 0.462 [kubernetes-dashboard-kubernetes-dashboard-80] [] 10.244.0.19:9090 665 0.462 200 db15e4ee465a5c01f5b0a831a19e4d57
192.168.59.1 - - [06/Feb/2024:02:35:32 +0000] "GET /api/v1/cronjob/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp HTTP/1.1" 200 230 "http://dashboard.com/" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36" 406 0.020 [kubernetes-dashboard-kubernetes-dashboard-80] [] 10.244.0.19:9090 230 0.020 200 262655bd89ee2beeecf35a62c6afd3cf
192.168.59.1 - - [06/Feb/2024:02:35:32 +0000] "GET /api/v1/namespace HTTP/1.1" 200 823 "http://dashboard.com/" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36" 350 0.025 [kubernetes-dashboard-kubernetes-dashboard-80] [] 10.244.0.19:9090 823 0.024 200 c9ddea84d19f3edbb7d6dd53b943c635
192.168.59.1 - - [06/Feb/2024:02:35:32 +0000] "GET /api/v1/daemonset/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp HTTP/1.1" 200 235 "http://dashboard.com/" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36" 408 0.072 [kubernetes-dashboard-kubernetes-dashboard-80] [] 10.244.0.19:9090 235 0.072 200 5674c968a07c42d9d3a17220af6f0d9c
192.168.59.1 - - [06/Feb/2024:02:35:32 +0000] "GET /api/v1/job/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp HTTP/1.1" 200 232 "http://dashboard.com/" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36" 402 0.092 [kubernetes-dashboard-kubernetes-dashboard-80] [] 10.244.0.19:9090 232 0.093 200 c20f1ea2c76dc94dc3e4e3a6c2a9d497
192.168.59.1 - - [06/Feb/2024:02:35:32 +0000] "GET /api/v1/deployment/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp HTTP/1.1" 200 1149 "http://dashboard.com/" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36" 409 0.103 [kubernetes-dashboard-kubernetes-dashboard-80] [] 10.244.0.19:9090 1149 0.104 200 84047ca5f2fa4f8136527797fc62a6f2
192.168.59.1 - - [06/Feb/2024:02:35:32 +0000] "GET /api/v1/replicationcontroller/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp HTTP/1.1" 200 241 "http://dashboard.com/" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36" 420 0.041 [kubernetes-dashboard-kubernetes-dashboard-80] [] 10.244.0.19:9090 241 0.040 200 6076c144c9b35e50db05d33c29ac3064
192.168.59.1 - - [06/Feb/2024:02:35:32 +0000] "GET /api/v1/statefulset/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp HTTP/1.1" 200 229 "http://dashboard.com/" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36" 410 0.039 [kubernetes-dashboard-kubernetes-dashboard-80] [] 10.244.0.19:9090 229 0.039 200 cf06e64b025552b141b1c36d4e7dac77
192.168.59.1 - - [06/Feb/2024:02:35:32 +0000] "GET /api/v1/replicaset/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp HTTP/1.1" 200 738 "http://dashboard.com/" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36" 409 0.070 [kubernetes-dashboard-kubernetes-dashboard-80] [] 10.244.0.19:9090 738 0.069 200 d1ec99f8b8e191a0a1f8721af5af5c19
192.168.59.1 - - [06/Feb/2024:02:35:32 +0000] "GET /api/v1/pod/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp HTTP/1.1" 200 665 "http://dashboard.com/" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36" 402 0.128 [kubernetes-dashboard-kubernetes-dashboard-80] [] 10.244.0.19:9090 665 0.128 200 5a033c9dc8b9bbd9697653640fece91b
192.168.59.1 - - [06/Feb/2024:02:35:37 +0000] "GET /api/v1/cronjob/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp HTTP/1.1" 200 230 "http://dashboard.com/" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36" 406 0.015 [kubernetes-dashboard-kubernetes-dashboard-80] [] 10.244.0.19:9090 230 0.015 200 53abd119eacc678bc8ddaf032ebae3d8
192.168.59.1 - - [06/Feb/2024:02:35:37 +0000] "GET /api/v1/namespace HTTP/1.1" 200 823 "http://dashboard.com/" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36" 350 0.029 [kubernetes-dashboard-kubernetes-dashboard-80] [] 10.244.0.19:9090 823 0.029 200 30424545786d48a0756c246e76fed37b
192.168.59.1 - - [06/Feb/2024:02:35:37 +0000] "GET /api/v1/job/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp HTTP/1.1" 200 232 "http://dashboard.com/" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36" 402 0.070 [kubernetes-dashboard-kubernetes-dashboard-80] [] 10.244.0.19:9090 232 0.070 200 b5343af31131d44c009373ce4f1d024a
192.168.59.1 - - [06/Feb/2024:02:35:37 +0000] "GET /api/v1/daemonset/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp HTTP/1.1" 200 235 "http://dashboard.com/" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36" 408 0.072 [kubernetes-dashboard-kubernetes-dashboard-80] [] 10.244.0.19:9090 235 0.071 200 071912b496d158801b39d5c61cba1183
192.168.59.1 - - [06/Feb/2024:02:35:37 +0000] "GET /api/v1/deployment/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp HTTP/1.1" 200 1149 "http://dashboard.com/" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36" 409 0.094 [kubernetes-dashboard-kubernetes-dashboard-80] [] 10.244.0.19:9090 1149 0.095 200 0cf9e93650aca1f9e78560de94671861
192.168.59.1 - - [06/Feb/2024:02:35:37 +0000] "GET /api/v1/replicationcontroller/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp HTTP/1.1" 200 241 "http://dashboard.com/" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36" 420 0.032 [kubernetes-dashboard-kubernetes-dashboard-80] [] 10.244.0.19:9090 241 0.032 200 e72f179ba790e79b8f599f8c473f00f9
192.168.59.1 - - [06/Feb/2024:02:35:37 +0000] "GET /api/v1/statefulset/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp HTTP/1.1" 200 229 "http://dashboard.com/" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36" 410 0.039 [kubernetes-dashboard-kubernetes-dashboard-80] [] 10.244.0.19:9090 229 0.039 200 1f09d07704577b9e88f3d2de417de625
192.168.59.1 - - [06/Feb/2024:02:35:37 +0000] "GET /api/v1/replicaset/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp HTTP/1.1" 200 738 "http://dashboard.com/" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36" 409 0.075 [kubernetes-dashboard-kubernetes-dashboard-80] [] 10.244.0.19:9090 738 0.074 200 e07d771cfdec772392101b67bfeadb14
192.168.59.1 - - [06/Feb/2024:02:35:37 +0000] "GET /api/v1/pod/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp HTTP/1.1" 200 665 "http://dashboard.com/" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36" 402 0.111 [kubernetes-dashboard-kubernetes-dashboard-80] [] 10.244.0.19:9090 665 0.111 200 eefc8422389ac20d68c24c20b45fbf31
192.168.59.1 - - [06/Feb/2024:02:35:42 +0000] "GET /api/v1/namespace HTTP/1.1" 200 823 "http://dashboard.com/" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36" 350 0.007 [kubernetes-dashboard-kubernetes-dashboard-80] [] 10.244.0.19:9090 823 0.007 200 a521d50e4af2b4ac140aff2646bbd8bb
192.168.59.1 - - [06/Feb/2024:02:35:42 +0000] "GET /api/v1/cronjob/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp HTTP/1.1" 200 230 "http://dashboard.com/" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36" 406 0.012 [kubernetes-dashboard-kubernetes-dashboard-80] [] 10.244.0.19:9090 230 0.011 200 eca278858a7d3b88beaaf3f64beb4aed
192.168.59.1 - - [06/Feb/2024:02:35:42 +0000] "GET /api/v1/job/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp HTTP/1.1" 200 232 "http://dashboard.com/" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36" 402 0.032 [kubernetes-dashboard-kubernetes-dashboard-80] [] 10.244.0.19:9090 232 0.032 200 bc36091d38c64708587ebfb81b41c4dc
192.168.59.1 - - [06/Feb/2024:02:35:42 +0000] "GET /api/v1/daemonset/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp HTTP/1.1" 200 235 "http://dashboard.com/" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36" 408 0.050 [kubernetes-dashboard-kubernetes-dashboard-80] [] 10.244.0.19:9090 235 0.050 200 cb62e1bb97126fbfe5ca3ec3b552ce59
192.168.59.1 - - [06/Feb/2024:02:35:42 +0000] "GET /api/v1/deployment/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp HTTP/1.1" 200 1149 "http://dashboard.com/" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36" 409 0.086 [kubernetes-dashboard-kubernetes-dashboard-80] [] 10.244.0.19:9090 1149 0.086 200 b41e575efd575b05c06e6466750023a0
192.168.59.1 - - [06/Feb/2024:02:35:42 +0000] "GET /api/v1/replicationcontroller/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp HTTP/1.1" 200 241 "http://dashboard.com/" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36" 420 0.043 [kubernetes-dashboard-kubernetes-dashboard-80] [] 10.244.0.19:9090 241 0.042 200 7e3ee3f6c771fd4c76816ec84a922581
192.168.59.1 - - [06/Feb/2024:02:35:42 +0000] "GET /api/v1/statefulset/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp HTTP/1.1" 200 229 "http://dashboard.com/" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36" 410 0.074 [kubernetes-dashboard-kubernetes-dashboard-80] [] 10.244.0.19:9090 229 0.073 200 1e2825e9ec129023671396abdd01e6d1
192.168.59.1 - - [06/Feb/2024:02:35:42 +0000] "GET /api/v1/replicaset/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp HTTP/1.1" 200 738 "http://dashboard.com/" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36" 409 0.127 [kubernetes-dashboard-kubernetes-dashboard-80] [] 10.244.0.19:9090 738 0.127 200 e4c263c8183d2daeebb04c2d29aac180
192.168.59.1 - - [06/Feb/2024:02:35:42 +0000] "GET /api/v1/pod/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp HTTP/1.1" 200 665 "http://dashboard.com/" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36" 402 0.154 [kubernetes-dashboard-kubernetes-dashboard-80] [] 10.244.0.19:9090 665 0.154 200 9b5a82009d0b4a0024730104a2099806
192.168.59.1 - - [06/Feb/2024:02:35:44 +0000] "GET /api/v1/namespace HTTP/1.1" 200 823 "http://dashboard.com/" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36" 350 0.262 [kubernetes-dashboard-kubernetes-dashboard-80] [] 10.244.0.19:9090 823 0.262 200 441eab2458999968da425865b211e5f0
192.168.59.1 - - [06/Feb/2024:02:35:44 +0000] "GET /api/v1/cronjob/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp HTTP/1.1" 200 230 "http://dashboard.com/" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36" 406 0.253 [kubernetes-dashboard-kubernetes-dashboard-80] [] 10.244.0.19:9090 230 0.253 200 8a79913a04120f3de38d7ac7321a9548
192.168.59.1 - - [06/Feb/2024:02:35:44 +0000] "GET /api/v1/job/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp HTTP/1.1" 200 232 "http://dashboard.com/" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36" 402 0.308 [kubernetes-dashboard-kubernetes-dashboard-80] [] 10.244.0.19:9090 232 0.308 200 3673011d45ff9fc7674b353bda427c0c
192.168.59.1 - - [06/Feb/2024:02:35:44 +0000] "GET /api/v1/daemonset/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp HTTP/1.1" 200 235 "http://dashboard.com/" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36" 408 0.314 [kubernetes-dashboard-kubernetes-dashboard-80] [] 10.244.0.19:9090 235 0.314 200 b54ef48bfaf23692106e41bf0340aff2
192.168.59.1 - - [06/Feb/2024:02:35:44 +0000] "GET /api/v1/replicationcontroller/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp HTTP/1.1" 200 241 "http://dashboard.com/" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36" 420 0.593 [kubernetes-dashboard-kubernetes-dashboard-80] [] 10.244.0.19:9090 241 0.593 200 3b9ee3b66ce48d00fc5cbff0a6bb75b4
192.168.59.1 - - [06/Feb/2024:02:35:44 +0000] "GET /api/v1/statefulset/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp HTTP/1.1" 200 229 "http://dashboard.com/" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36" 410 0.541 [kubernetes-dashboard-kubernetes-dashboard-80] [] 10.244.0.19:9090 229 0.541 200 f6ea174c53720bb67778d6354b9e0f1f
192.168.59.1 - - [06/Feb/2024:02:35:44 +0000] "GET /api/v1/deployment/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp HTTP/1.1" 200 1149 "http://dashboard.com/" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36" 409 1.035 [kubernetes-dashboard-kubernetes-dashboard-80] [] 10.244.0.19:9090 1149 1.036 200 9299d46088280e11d6601ae78657730f
192.168.59.1 - - [06/Feb/2024:02:35:44 +0000] "GET /api/v1/replicaset/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp HTTP/1.1" 200 738 "http://dashboard.com/" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36" 409 0.794 [kubernetes-dashboard-kubernetes-dashboard-80] [] 10.244.0.19:9090 738 0.795 200 279a412c8d19bfce81d3d0feb63c3a95
192.168.59.1 - - [06/Feb/2024:02:35:44 +0000] "GET /api/v1/pod/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp HTTP/1.1" 200 665 "http://dashboard.com/" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36" 402 1.095 [kubernetes-dashboard-kubernetes-dashboard-80] [] 10.244.0.19:9090 665 1.095 200 4ef12c314cc3ff279e1c738fe5033bd0
E0206 07:49:12.013118       7 leaderelection.go:327] error retrieving resource lock ingress-nginx/ingress-nginx-leader: Get "https://10.96.0.1:443/apis/coordination.k8s.io/v1/namespaces/ingress-nginx/leases/ingress-nginx-leader": context deadline exceeded
I0206 07:49:12.013161       7 leaderelection.go:280] failed to renew lease ingress-nginx/ingress-nginx-leader: timed out waiting for the condition
E0206 07:49:12.014849       7 status.go:104] "error running poll" err="timed out waiting for the condition"
I0206 07:49:12.014962       7 leaderelection.go:245] attempting to acquire leader lease ingress-nginx/ingress-nginx-leader...
I0206 07:50:19.482989       7 leaderelection.go:255] successfully acquired lease ingress-nginx/ingress-nginx-leader
W0206 07:50:27.502320       7 controller.go:1214] Service "kubernetes-dashboard/kubernetes-dashboard" does not have any active Endpoint.
W0206 07:50:34.482392       7 controller.go:1214] Service "kubernetes-dashboard/kubernetes-dashboard" does not have any active Endpoint.
W0206 07:50:41.414783       7 controller.go:1214] Service "kubernetes-dashboard/kubernetes-dashboard" does not have any active Endpoint.
W0206 07:50:44.818584       7 controller.go:1214] Service "kubernetes-dashboard/kubernetes-dashboard" does not have any active Endpoint.
W0206 07:50:48.082259       7 controller.go:1214] Service "kubernetes-dashboard/kubernetes-dashboard" does not have any active Endpoint.
W0206 07:51:01.835344       7 controller.go:1214] Service "kubernetes-dashboard/kubernetes-dashboard" does not have any active Endpoint.
I0206 13:08:41.123396       7 sigterm.go:36] "Received SIGTERM, shutting down"
I0206 13:08:41.123473       7 nginx.go:379] "Shutting down controller queues"
I0206 13:08:41.190772       7 status.go:135] "removing value from ingress status" address=[{"ip":"192.168.59.100"}]
I0206 13:08:42.193581       7 nginx.go:387] "Stopping admission controller"
E0206 13:08:42.193653       7 nginx.go:326] "Error listening for TLS connections" err="http: Server closed"
I0206 13:08:42.193664       7 nginx.go:395] "Stopping NGINX process"
2024/02/06 13:08:42 [notice] 165#165: signal process started
I0206 13:08:43.337520       7 nginx.go:408] "NGINX process has stopped"
I0206 13:08:43.337545       7 sigterm.go:44] Handled quit, delaying controller exit for 10 seconds
E0206 13:08:43.690799       7 leaderelection.go:327] error retrieving resource lock ingress-nginx/ingress-nginx-leader: Get "https://10.96.0.1:443/apis/coordination.k8s.io/v1/namespaces/ingress-nginx/leases/ingress-nginx-leader": dial tcp 10.96.0.1:443: connect: connection refused
E0206 13:08:51.192101       7 leaderelection.go:327] error retrieving resource lock ingress-nginx/ingress-nginx-leader: Get "https://10.96.0.1:443/apis/coordination.k8s.io/v1/namespaces/ingress-nginx/leases/ingress-nginx-leader": dial tcp 10.96.0.1:443: connect: connection refused

* 
* ==> coredns [06fb15ac4a32] <==
* [WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 3.860298787s
[WARNING] plugin/health: Local health request to "http://:8080/health" failed: Get "http://:8080/health": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 1.616035996s
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 1.810160761s
[WARNING] plugin/health: Local health request to "http://:8080/health" failed: Get "http://:8080/health": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 1.913744686s
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 1.444334514s
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 1.58104108s
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 2.203591263s
[WARNING] plugin/health: Local health request to "http://:8080/health" failed: Get "http://:8080/health": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.26.1/tools/cache/reflector.go:169: watch of *v1.Namespace ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.26.1/tools/cache/reflector.go:169: watch of *v1.Service ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.26.1/tools/cache/reflector.go:169: Failed to watch *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?allowWatchBookmarks=true&resourceVersion=16246&timeout=9m36s&timeoutSeconds=576&watch=true": http2: client connection lost
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 1.373959944s
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 1.850160188s
[INFO] 10.244.0.14:39699 - 62547 "A IN mongodb-service.default.svc.cluster.local. udp 59 false 512" NOERROR qr,aa,rd 116 0.001216452s
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 1.268130073s
[INFO] 10.244.0.24:53259 - 36729 "A IN mysql.my-namespace.svc.cluster.local. udp 54 false 512" NOERROR qr,aa,rd 106 0.001106777s
[INFO] 10.244.0.24:53259 - 14197 "AAAA IN mysql.my-namespace.svc.cluster.local. udp 54 false 512" NOERROR qr,aa,rd 147 0.000877977s
[INFO] 10.244.0.25:48221 - 25286 "A IN mysql.my-namespace.svc.cluster.local. udp 54 false 512" NOERROR qr,aa,rd 106 0.000704395s
[INFO] 10.244.0.25:48221 - 7643 "AAAA IN mysql.my-namespace.svc.cluster.local. udp 54 false 512" NOERROR qr,aa,rd 147 0.000293469s
[INFO] 10.244.0.26:53864 - 9368 "A IN mysql.my-namespace.svc.cluster.local. udp 54 false 512" NOERROR qr,aa,rd 106 0.000295853s
[INFO] 10.244.0.26:53864 - 22687 "AAAA IN mysql.my-namespace.svc.cluster.local. udp 54 false 512" NOERROR qr,aa,rd 147 0.000204804s
[INFO] 10.244.0.27:54524 - 23595 "AAAA IN mysql.my-namespace.svc.cluster.local. udp 54 false 512" NOERROR qr,aa,rd 147 0.000337585s
[INFO] 10.244.0.27:54524 - 44846 "A IN mysql.my-namespace.svc.cluster.local. udp 54 false 512" NOERROR qr,aa,rd 106 0.000102993s
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 1.933517845s
[INFO] 10.244.0.28:43087 - 14772 "A IN mysql.my-namespace.svc.cluster.local. udp 54 false 512" NOERROR qr,aa,rd 106 0.000328489s
[INFO] 10.244.0.28:43087 - 29105 "AAAA IN mysql.my-namespace.svc.cluster.local. udp 54 false 512" NOERROR qr,aa,rd 147 0.000378866s
[INFO] 10.244.0.29:45869 - 26580 "A IN mysql.default.svc.cluster.local. udp 49 false 512" NXDOMAIN qr,aa,rd 142 0.000761213s
[INFO] 10.244.0.29:45869 - 52433 "AAAA IN mysql.default.svc.cluster.local. udp 49 false 512" NXDOMAIN qr,aa,rd 142 0.000213111s
[INFO] 10.244.0.29:47799 - 1816 "A IN mysql.svc.cluster.local. udp 41 false 512" NXDOMAIN qr,aa,rd 134 0.000348653s
[INFO] 10.244.0.29:47799 - 56350 "AAAA IN mysql.svc.cluster.local. udp 41 false 512" NXDOMAIN qr,aa,rd 134 0.000250121s
[INFO] 10.244.0.29:44430 - 7633 "A IN mysql.cluster.local. udp 37 false 512" NXDOMAIN qr,aa,rd 130 0.000562159s
[INFO] 10.244.0.29:44430 - 43999 "AAAA IN mysql.cluster.local. udp 37 false 512" NXDOMAIN qr,aa,rd 130 0.000115564s
[INFO] 10.244.0.29:59124 - 38459 "A IN mysql. udp 23 false 512" NXDOMAIN qr,rd,ra 23 0.157833637s
[INFO] 10.244.0.29:59124 - 7482 "AAAA IN mysql. udp 23 false 512" NXDOMAIN qr,rd,ra 23 0.156926706s
[INFO] 10.244.0.31:41090 - 13432 "A IN mysql.default.svc.cluster.local. udp 49 false 512" NOERROR qr,aa,rd 96 0.000723763s
[INFO] 10.244.0.31:41090 - 7551 "AAAA IN mysql.default.svc.cluster.local. udp 49 false 512" NOERROR qr,aa,rd 142 0.000857368s
[INFO] 10.244.0.32:56412 - 33088 "AAAA IN mysql.default.svc.cluster.local. udp 49 false 512" NOERROR qr,aa,rd 142 0.000265308s
[INFO] 10.244.0.32:56412 - 20805 "A IN mysql.default.svc.cluster.local. udp 49 false 512" NOERROR qr,aa,rd 96 0.000229116s
[INFO] 10.244.0.33:56276 - 60212 "AAAA IN mysql.default.svc.cluster.local. udp 49 false 512" NOERROR qr,aa,rd 142 0.000275824s
[INFO] 10.244.0.33:56276 - 13064 "A IN mysql.default.svc.cluster.local. udp 49 false 512" NOERROR qr,aa,rd 96 0.000243755s
[INFO] 10.244.0.34:44651 - 40519 "A IN mysql.default.svc.cluster.local. udp 49 false 512" NOERROR qr,aa,rd 96 0.000278781s
[INFO] 10.244.0.34:44651 - 56138 "AAAA IN mysql.default.svc.cluster.local. udp 49 false 512" NOERROR qr,aa,rd 142 0.00024426s
[INFO] 10.244.0.35:47042 - 10635 "AAAA IN mysql.default.svc.cluster.local. udp 49 false 512" NOERROR qr,aa,rd 142 0.000308142s
[INFO] 10.244.0.35:47042 - 48535 "A IN mysql.default.svc.cluster.local. udp 49 false 512" NOERROR qr,aa,rd 96 0.000189014s
[INFO] 10.244.0.36:47042 - 28924 "A IN mysql.default.svc.cluster.local. udp 49 false 512" NOERROR qr,aa,rd 96 0.000375142s
[INFO] 10.244.0.36:47042 - 2038 "AAAA IN mysql.default.svc.cluster.local. udp 49 false 512" NOERROR qr,aa,rd 142 0.000235881s
[INFO] 10.244.0.37:41562 - 30365 "AAAA IN mysql.default.svc.cluster.local. udp 49 false 512" NOERROR qr,aa,rd 142 0.000197062s
[INFO] 10.244.0.37:41562 - 35480 "A IN mysql.default.svc.cluster.local. udp 49 false 512" NOERROR qr,aa,rd 96 0.00005666s
[INFO] 10.244.0.38:34671 - 39049 "A IN mysql.default.svc.cluster.local. udp 49 false 512" NOERROR qr,aa,rd 96 0.000360117s
[INFO] 10.244.0.38:34671 - 39054 "AAAA IN mysql.default.svc.cluster.local. udp 49 false 512" NOERROR qr,aa,rd 142 0.000922707s
[INFO] 10.244.0.39:36086 - 15143 "AAAA IN mysql.default.svc.cluster.local. udp 49 false 512" NOERROR qr,aa,rd 142 0.000185737s
[INFO] 10.244.0.39:36086 - 44064 "A IN mysql.default.svc.cluster.local. udp 49 false 512" NOERROR qr,aa,rd 96 0.000111968s
[INFO] 10.244.0.40:35201 - 19669 "A IN mysql.default.svc.cluster.local. udp 49 false 512" NOERROR qr,aa,rd 96 0.000620375s
[INFO] 10.244.0.40:35201 - 17878 "AAAA IN mysql.default.svc.cluster.local. udp 49 false 512" NOERROR qr,aa,rd 142 0.001662626s
[INFO] 10.244.0.42:48498 - 26726 "A IN mysql.default.svc.cluster.local. udp 49 false 512" NOERROR qr,aa,rd 96 0.00039477s
[INFO] 10.244.0.42:48498 - 7019 "AAAA IN mysql.default.svc.cluster.local. udp 49 false 512" NOERROR qr,aa,rd 142 0.000170236s
[INFO] SIGTERM: Shutting down servers then terminating
[INFO] plugin/health: Going into lameduck mode for 5s

* 
* ==> coredns [b94028402ad3] <==
* [INFO] 10.244.0.10:56996 - 37687 "A IN mongo. udp 23 false 512" NXDOMAIN qr,rd,ra 23 0.004070736s
[INFO] 10.244.0.10:56984 - 26501 "AAAA IN mongo.default.svc.cluster.local. udp 49 false 512" NXDOMAIN qr,aa,rd 142 0.000570691s
[INFO] 10.244.0.10:56984 - 26210 "A IN mongo.default.svc.cluster.local. udp 49 false 512" NXDOMAIN qr,aa,rd 142 0.000814118s
[INFO] 10.244.0.10:39208 - 59784 "A IN mongo.svc.cluster.local. udp 41 false 512" NXDOMAIN qr,aa,rd 134 0.000154653s
[INFO] 10.244.0.10:39208 - 60128 "AAAA IN mongo.svc.cluster.local. udp 41 false 512" NXDOMAIN qr,aa,rd 134 0.000133506s
[INFO] 10.244.0.10:52374 - 38623 "AAAA IN mongo.cluster.local. udp 37 false 512" NXDOMAIN qr,aa,rd 130 0.000278601s
[INFO] 10.244.0.10:52374 - 38396 "A IN mongo.cluster.local. udp 37 false 512" NXDOMAIN qr,aa,rd 130 0.000339809s
[INFO] 10.244.0.10:35470 - 10818 "AAAA IN mongo. udp 23 false 512" NXDOMAIN qr,rd,ra 23 0.002739996s
[INFO] 10.244.0.10:35470 - 10628 "A IN mongo. udp 23 false 512" NXDOMAIN qr,rd,ra 23 0.003243668s
[INFO] 10.244.0.10:44995 - 32383 "A IN mongo.default.svc.cluster.local. udp 49 false 512" NXDOMAIN qr,aa,rd 142 0.000083557s
[INFO] 10.244.0.10:44995 - 32781 "AAAA IN mongo.default.svc.cluster.local. udp 49 false 512" NXDOMAIN qr,aa,rd 142 0.00014925s
[INFO] 10.244.0.10:43619 - 45390 "A IN mongo.svc.cluster.local. udp 41 false 512" NXDOMAIN qr,aa,rd 134 0.000185846s
[INFO] 10.244.0.10:43619 - 45784 "AAAA IN mongo.svc.cluster.local. udp 41 false 512" NXDOMAIN qr,aa,rd 134 0.000186815s
[INFO] 10.244.0.10:42516 - 58552 "A IN mongo.cluster.local. udp 37 false 512" NXDOMAIN qr,aa,rd 130 0.00021742s
[INFO] 10.244.0.10:42516 - 58779 "AAAA IN mongo.cluster.local. udp 37 false 512" NXDOMAIN qr,aa,rd 130 0.000176182s
[INFO] 10.244.0.10:50497 - 38523 "AAAA IN mongo. udp 23 false 512" NXDOMAIN qr,rd,ra 23 0.002548856s
[INFO] 10.244.0.10:50497 - 38254 "A IN mongo. udp 23 false 512" NXDOMAIN qr,rd,ra 23 0.003632652s
[INFO] 10.244.0.10:40576 - 33937 "AAAA IN mongo.default.svc.cluster.local. udp 49 false 512" NXDOMAIN qr,aa,rd 142 0.000351679s
[INFO] 10.244.0.10:40576 - 33622 "A IN mongo.default.svc.cluster.local. udp 49 false 512" NXDOMAIN qr,aa,rd 142 0.000358001s
[INFO] 10.244.0.10:39573 - 9352 "A IN mongo.svc.cluster.local. udp 41 false 512" NXDOMAIN qr,aa,rd 134 0.000094338s
[INFO] 10.244.0.10:39573 - 9559 "AAAA IN mongo.svc.cluster.local. udp 41 false 512" NXDOMAIN qr,aa,rd 134 0.000077296s
[INFO] 10.244.0.10:59215 - 44291 "A IN mongo.cluster.local. udp 37 false 512" NXDOMAIN qr,aa,rd 130 0.000065054s
[INFO] 10.244.0.10:59215 - 44760 "AAAA IN mongo.cluster.local. udp 37 false 512" NXDOMAIN qr,aa,rd 130 0.000300195s
[INFO] 10.244.0.10:57155 - 2826 "AAAA IN mongo. udp 23 false 512" NXDOMAIN qr,rd,ra 23 0.001018938s
[INFO] 10.244.0.10:57155 - 2664 "A IN mongo. udp 23 false 512" NXDOMAIN qr,rd,ra 23 0.003543191s
[INFO] 10.244.0.10:41017 - 13465 "A IN mongo.default.svc.cluster.local. udp 49 false 512" NXDOMAIN qr,aa,rd 142 0.000181614s
[INFO] 10.244.0.10:41017 - 13750 "AAAA IN mongo.default.svc.cluster.local. udp 49 false 512" NXDOMAIN qr,aa,rd 142 0.000441472s
[INFO] 10.244.0.10:34218 - 48400 "AAAA IN mongo.svc.cluster.local. udp 41 false 512" NXDOMAIN qr,aa,rd 134 0.000248067s
[INFO] 10.244.0.10:34218 - 48081 "A IN mongo.svc.cluster.local. udp 41 false 512" NXDOMAIN qr,aa,rd 134 0.000387939s
[INFO] 10.244.0.10:53718 - 34874 "A IN mongo.cluster.local. udp 37 false 512" NXDOMAIN qr,aa,rd 130 0.000312307s
[INFO] 10.244.0.10:53718 - 35192 "AAAA IN mongo.cluster.local. udp 37 false 512" NXDOMAIN qr,aa,rd 130 0.000070742s
[INFO] 10.244.0.10:53062 - 40199 "AAAA IN mongo. udp 23 false 512" NXDOMAIN qr,rd,ra 23 0.001287367s
[INFO] 10.244.0.10:53062 - 39940 "A IN mongo. udp 23 false 512" NXDOMAIN qr,rd,ra 23 0.00279147s
[INFO] 10.244.0.10:57794 - 49118 "A IN mongo.default.svc.cluster.local. udp 49 false 512" NXDOMAIN qr,aa,rd 142 0.000306145s
[INFO] 10.244.0.10:57794 - 49518 "AAAA IN mongo.default.svc.cluster.local. udp 49 false 512" NXDOMAIN qr,aa,rd 142 0.000275206s
[INFO] 10.244.0.10:37096 - 42920 "AAAA IN mongo.svc.cluster.local. udp 41 false 512" NXDOMAIN qr,aa,rd 134 0.000172622s
[INFO] 10.244.0.10:37096 - 42646 "A IN mongo.svc.cluster.local. udp 41 false 512" NXDOMAIN qr,aa,rd 134 0.000585944s
[INFO] 10.244.0.10:52546 - 12800 "AAAA IN mongo.cluster.local. udp 37 false 512" NXDOMAIN qr,aa,rd 130 0.000090976s
[INFO] 10.244.0.10:52546 - 12566 "A IN mongo.cluster.local. udp 37 false 512" NXDOMAIN qr,aa,rd 130 0.00004402s
[INFO] 10.244.0.10:34510 - 46839 "AAAA IN mongo. udp 23 false 512" NXDOMAIN qr,rd,ra 23 0.002389691s
[INFO] 10.244.0.10:34510 - 46367 "A IN mongo. udp 23 false 512" NXDOMAIN qr,rd,ra 23 0.007448453s
[INFO] 10.244.0.10:37455 - 41780 "A IN mongo.default.svc.cluster.local. udp 49 false 512" NXDOMAIN qr,aa,rd 142 0.000095579s
[INFO] 10.244.0.10:37455 - 42121 "AAAA IN mongo.default.svc.cluster.local. udp 49 false 512" NXDOMAIN qr,aa,rd 142 0.000072462s
[INFO] 10.244.0.10:42163 - 16889 "A IN mongo.svc.cluster.local. udp 41 false 512" NXDOMAIN qr,aa,rd 134 0.000093783s
[INFO] 10.244.0.10:42163 - 17161 "AAAA IN mongo.svc.cluster.local. udp 41 false 512" NXDOMAIN qr,aa,rd 134 0.000340885s
[INFO] 10.244.0.10:50640 - 1007 "A IN mongo.cluster.local. udp 37 false 512" NXDOMAIN qr,aa,rd 130 0.000134443s
[INFO] 10.244.0.10:50640 - 1260 "AAAA IN mongo.cluster.local. udp 37 false 512" NXDOMAIN qr,aa,rd 130 0.000060336s
[INFO] 10.244.0.10:59738 - 54183 "AAAA IN mongo. udp 23 false 512" NXDOMAIN qr,rd,ra 23 0.002899803s
[INFO] 10.244.0.10:59738 - 53987 "A IN mongo. udp 23 false 512" NXDOMAIN qr,rd,ra 23 0.003784947s
[INFO] 10.244.0.10:49424 - 28202 "AAAA IN mongo.default.svc.cluster.local. udp 49 false 512" NXDOMAIN qr,aa,rd 142 0.000107275s
[INFO] 10.244.0.10:49424 - 27841 "A IN mongo.default.svc.cluster.local. udp 49 false 512" NXDOMAIN qr,aa,rd 142 0.000237093s
[INFO] 10.244.0.10:54141 - 31685 "A IN mongo.svc.cluster.local. udp 41 false 512" NXDOMAIN qr,aa,rd 134 0.000268643s
[INFO] 10.244.0.10:54141 - 32088 "AAAA IN mongo.svc.cluster.local. udp 41 false 512" NXDOMAIN qr,aa,rd 134 0.000121595s
[INFO] 10.244.0.10:37168 - 35101 "A IN mongo.cluster.local. udp 37 false 512" NXDOMAIN qr,aa,rd 130 0.000076175s
[INFO] 10.244.0.10:37168 - 35568 "AAAA IN mongo.cluster.local. udp 37 false 512" NXDOMAIN qr,aa,rd 130 0.000170052s
[INFO] 10.244.0.10:60826 - 62030 "AAAA IN mongo. udp 23 false 512" NXDOMAIN qr,rd,ra 23 0.002756561s
[INFO] 10.244.0.10:60826 - 61787 "A IN mongo. udp 23 false 512" NXDOMAIN qr,rd,ra 23 0.003914385s
[INFO] 10.244.0.10:33760 - 33396 "A IN mongodb-service.default.svc.cluster.local. udp 59 false 512" NOERROR qr,aa,rd 116 0.000258002s
[INFO] 10.244.0.10:54449 - 16193 "A IN mongodb-service.default.svc.cluster.local. udp 59 false 512" NOERROR qr,aa,rd 116 0.000615638s
[INFO] 10.244.0.10:45564 - 37385 "A IN mongodb-service.default.svc.cluster.local. udp 59 false 512" NOERROR qr,aa,rd 116 0.00022325s

* 
* ==> describe nodes <==
* 
* ==> dmesg <==
* [  +0.049107] EXT4-fs warning (device sda1): ext4_dirblock_csum_verify:377: inode #4195503: comm cri-dockerd: No space for directory leaf checksum. Please run e2fsck -D.
[  +0.000004] EXT4-fs error (device sda1): htree_dirblock_to_tree:1003: inode #4195503: comm cri-dockerd: Directory block failed checksum
[  +0.078006] EXT4-fs warning (device sda1): ext4_dirblock_csum_verify:377: inode #4195503: comm cri-dockerd: No space for directory leaf checksum. Please run e2fsck -D.
[  +0.000005] EXT4-fs error (device sda1): htree_dirblock_to_tree:1003: inode #4195503: comm cri-dockerd: Directory block failed checksum
[  +0.277124] EXT4-fs warning (device sda1): ext4_dirblock_csum_verify:377: inode #4195503: comm cri-dockerd: No space for directory leaf checksum. Please run e2fsck -D.
[  +0.000005] EXT4-fs error (device sda1): htree_dirblock_to_tree:1003: inode #4195503: comm cri-dockerd: Directory block failed checksum
[  +0.140743] EXT4-fs warning (device sda1): ext4_dirblock_csum_verify:377: inode #4195503: comm cri-dockerd: No space for directory leaf checksum. Please run e2fsck -D.
[  +0.000003] EXT4-fs error (device sda1): htree_dirblock_to_tree:1003: inode #4195503: comm cri-dockerd: Directory block failed checksum
[  +0.062082] EXT4-fs warning (device sda1): ext4_dirblock_csum_verify:377: inode #4195503: comm cri-dockerd: No space for directory leaf checksum. Please run e2fsck -D.
[  +0.000003] EXT4-fs error (device sda1): htree_dirblock_to_tree:1003: inode #4195503: comm cri-dockerd: Directory block failed checksum
[  +0.081825] EXT4-fs warning (device sda1): ext4_dirblock_csum_verify:377: inode #4195503: comm cri-dockerd: No space for directory leaf checksum. Please run e2fsck -D.
[  +0.000005] EXT4-fs error (device sda1): htree_dirblock_to_tree:1003: inode #4195503: comm cri-dockerd: Directory block failed checksum
[ +11.678444] EXT4-fs warning: 22 callbacks suppressed
[  +0.000003] EXT4-fs warning (device sda1): ext4_dirblock_csum_verify:377: inode #4203065: comm dockerd: No space for directory leaf checksum. Please run e2fsck -D.
[  +0.000002] EXT4-fs error: 22 callbacks suppressed
[  +0.000002] EXT4-fs error (device sda1): htree_dirblock_to_tree:1003: inode #4203065: comm dockerd: Directory block failed checksum
[  +0.001361] EXT4-fs warning (device sda1): ext4_dirblock_csum_verify:377: inode #4203063: comm dockerd: No space for directory leaf checksum. Please run e2fsck -D.
[  +0.000003] EXT4-fs error (device sda1): htree_dirblock_to_tree:1003: inode #4203063: comm dockerd: Directory block failed checksum
[  +0.167065] EXT4-fs warning (device sda1): ext4_dirblock_csum_verify:377: inode #4203065: comm dockerd: No space for directory leaf checksum. Please run e2fsck -D.
[  +0.000005] EXT4-fs error (device sda1): htree_dirblock_to_tree:1003: inode #4203065: comm dockerd: Directory block failed checksum
[  +0.006428] EXT4-fs warning (device sda1): ext4_dirblock_csum_verify:377: inode #4203063: comm dockerd: No space for directory leaf checksum. Please run e2fsck -D.
[  +0.000003] EXT4-fs error (device sda1): htree_dirblock_to_tree:1003: inode #4203063: comm dockerd: Directory block failed checksum
[  +0.135337] EXT4-fs warning (device sda1): ext4_dirblock_csum_verify:377: inode #4203065: comm dockerd: No space for directory leaf checksum. Please run e2fsck -D.
[  +0.000004] EXT4-fs error (device sda1): htree_dirblock_to_tree:1003: inode #4203065: comm dockerd: Directory block failed checksum
[  +0.001365] EXT4-fs warning (device sda1): ext4_dirblock_csum_verify:377: inode #4203063: comm dockerd: No space for directory leaf checksum. Please run e2fsck -D.
[  +0.000005] EXT4-fs error (device sda1): htree_dirblock_to_tree:1003: inode #4203063: comm dockerd: Directory block failed checksum
[  +0.175500] EXT4-fs warning (device sda1): ext4_dirblock_csum_verify:377: inode #4203065: comm dockerd: No space for directory leaf checksum. Please run e2fsck -D.
[  +0.000003] EXT4-fs error (device sda1): htree_dirblock_to_tree:1003: inode #4203065: comm dockerd: Directory block failed checksum
[  +0.002248] EXT4-fs warning (device sda1): ext4_dirblock_csum_verify:377: inode #4203063: comm dockerd: No space for directory leaf checksum. Please run e2fsck -D.
[  +0.000003] EXT4-fs error (device sda1): htree_dirblock_to_tree:1003: inode #4203063: comm dockerd: Directory block failed checksum
[  +0.154653] EXT4-fs warning (device sda1): ext4_dirblock_csum_verify:377: inode #4203065: comm dockerd: No space for directory leaf checksum. Please run e2fsck -D.
[  +0.000003] EXT4-fs error (device sda1): htree_dirblock_to_tree:1003: inode #4203065: comm dockerd: Directory block failed checksum
[  +0.001323] EXT4-fs warning (device sda1): ext4_dirblock_csum_verify:377: inode #4203063: comm dockerd: No space for directory leaf checksum. Please run e2fsck -D.
[  +0.000003] EXT4-fs error (device sda1): htree_dirblock_to_tree:1003: inode #4203063: comm dockerd: Directory block failed checksum
[  +4.394738] EXT4-fs warning: 38 callbacks suppressed
[  +0.000002] EXT4-fs warning (device sda1): ext4_dirblock_csum_verify:377: inode #4195503: comm cri-dockerd: No space for directory leaf checksum. Please run e2fsck -D.
[  +0.000002] EXT4-fs error: 38 callbacks suppressed
[  +0.000002] EXT4-fs error (device sda1): htree_dirblock_to_tree:1003: inode #4195503: comm cri-dockerd: Directory block failed checksum
[  +0.067116] EXT4-fs warning (device sda1): ext4_dirblock_csum_verify:377: inode #4195503: comm cri-dockerd: No space for directory leaf checksum. Please run e2fsck -D.
[  +0.000007] EXT4-fs error (device sda1): htree_dirblock_to_tree:1003: inode #4195503: comm cri-dockerd: Directory block failed checksum
[  +0.053892] EXT4-fs warning (device sda1): ext4_dirblock_csum_verify:377: inode #4195503: comm cri-dockerd: No space for directory leaf checksum. Please run e2fsck -D.
[  +0.000003] EXT4-fs error (device sda1): htree_dirblock_to_tree:1003: inode #4195503: comm cri-dockerd: Directory block failed checksum
[  +0.054551] EXT4-fs warning (device sda1): ext4_dirblock_csum_verify:377: inode #4195503: comm cri-dockerd: No space for directory leaf checksum. Please run e2fsck -D.
[  +0.000003] EXT4-fs error (device sda1): htree_dirblock_to_tree:1003: inode #4195503: comm cri-dockerd: Directory block failed checksum
[  +0.053372] EXT4-fs warning (device sda1): ext4_dirblock_csum_verify:377: inode #4195503: comm cri-dockerd: No space for directory leaf checksum. Please run e2fsck -D.
[  +0.000003] EXT4-fs error (device sda1): htree_dirblock_to_tree:1003: inode #4195503: comm cri-dockerd: Directory block failed checksum
[  +0.111474] EXT4-fs warning (device sda1): ext4_dirblock_csum_verify:377: inode #4195503: comm cri-dockerd: No space for directory leaf checksum. Please run e2fsck -D.
[  +0.000003] EXT4-fs error (device sda1): htree_dirblock_to_tree:1003: inode #4195503: comm cri-dockerd: Directory block failed checksum
[  +0.101006] EXT4-fs warning (device sda1): ext4_dirblock_csum_verify:377: inode #4195503: comm cri-dockerd: No space for directory leaf checksum. Please run e2fsck -D.
[  +0.000003] EXT4-fs error (device sda1): htree_dirblock_to_tree:1003: inode #4195503: comm cri-dockerd: Directory block failed checksum
[  +0.094075] EXT4-fs warning (device sda1): ext4_dirblock_csum_verify:377: inode #4195503: comm cri-dockerd: No space for directory leaf checksum. Please run e2fsck -D.
[  +0.000003] EXT4-fs error (device sda1): htree_dirblock_to_tree:1003: inode #4195503: comm cri-dockerd: Directory block failed checksum
[  +0.084256] EXT4-fs warning (device sda1): ext4_dirblock_csum_verify:377: inode #4195503: comm cri-dockerd: No space for directory leaf checksum. Please run e2fsck -D.
[  +0.000004] EXT4-fs error (device sda1): htree_dirblock_to_tree:1003: inode #4195503: comm cri-dockerd: Directory block failed checksum
[  +0.103145] EXT4-fs warning (device sda1): ext4_dirblock_csum_verify:377: inode #4195503: comm cri-dockerd: No space for directory leaf checksum. Please run e2fsck -D.
[  +0.000005] EXT4-fs error (device sda1): htree_dirblock_to_tree:1003: inode #4195503: comm cri-dockerd: Directory block failed checksum
[Feb 7 03:40] EXT4-fs warning: 6 callbacks suppressed
[  +0.000002] EXT4-fs warning (device sda1): ext4_dirblock_csum_verify:377: inode #4195503: comm cri-dockerd: No space for directory leaf checksum. Please run e2fsck -D.
[  +0.000002] EXT4-fs error: 6 callbacks suppressed
[  +0.000001] EXT4-fs error (device sda1): htree_dirblock_to_tree:1003: inode #4195503: comm cri-dockerd: Directory block failed checksum

* 
* ==> etcd [1eed8f4e884e] <==
* 
* ==> kernel <==
*  03:40:43 up 9 min,  0 users,  load average: 0.06, 0.49, 0.39
Linux minikube 5.10.57 #1 SMP Tue Nov 7 06:51:54 UTC 2023 x86_64 GNU/Linux
PRETTY_NAME="Buildroot 2021.02.12"

* 
* ==> kube-controller-manager [885389110d12] <==
* I0205 10:50:27.667551       1 node_lifecycle_controller.go:877] "Missing timestamp for Node. Assuming now as a timestamp" node="minikube"
I0205 10:50:27.667698       1 node_lifecycle_controller.go:1071] "Controller detected that zone is now in new state" zone="" newState="Normal"
I0205 10:50:27.668065       1 shared_informer.go:318] Caches are synced for namespace
I0205 10:50:27.670846       1 shared_informer.go:318] Caches are synced for expand
I0205 10:50:27.672763       1 event.go:307] "Event occurred" object="minikube" fieldPath="" kind="Node" apiVersion="v1" type="Normal" reason="RegisteredNode" message="Node minikube event: Registered Node minikube in Controller"
I0205 10:50:27.673612       1 shared_informer.go:318] Caches are synced for cronjob
I0205 10:50:27.673741       1 shared_informer.go:318] Caches are synced for GC
I0205 10:50:27.674012       1 shared_informer.go:318] Caches are synced for stateful set
I0205 10:50:27.678966       1 shared_informer.go:318] Caches are synced for PVC protection
I0205 10:50:27.686057       1 shared_informer.go:318] Caches are synced for ReplicationController
I0205 10:50:27.691445       1 shared_informer.go:311] Waiting for caches to sync for garbage collector
I0205 10:50:27.691510       1 shared_informer.go:318] Caches are synced for attach detach
I0205 10:50:27.712303       1 shared_informer.go:318] Caches are synced for daemon sets
I0205 10:50:27.753527       1 shared_informer.go:318] Caches are synced for HPA
I0205 10:50:27.773289       1 shared_informer.go:318] Caches are synced for endpoint_slice
I0205 10:50:27.778468       1 shared_informer.go:318] Caches are synced for resource quota
I0205 10:50:27.812458       1 shared_informer.go:318] Caches are synced for endpoint
I0205 10:50:27.851562       1 shared_informer.go:318] Caches are synced for resource quota
I0205 10:50:27.858762       1 shared_informer.go:318] Caches are synced for endpoint_slice_mirroring
I0205 10:50:28.195661       1 shared_informer.go:318] Caches are synced for garbage collector
I0205 10:50:28.208257       1 shared_informer.go:318] Caches are synced for garbage collector
I0205 10:50:28.208726       1 garbagecollector.go:166] "All resource monitors have synced. Proceeding to collect garbage"
I0205 10:50:28.322043       1 event.go:307] "Event occurred" object="kube-system/coredns" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set coredns-5dd5756b68 to 1"
I0205 10:50:28.471688       1 event.go:307] "Event occurred" object="kube-system/kube-proxy" fieldPath="" kind="DaemonSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: kube-proxy-7sg25"
I0205 10:50:28.638230       1 event.go:307] "Event occurred" object="kube-system/coredns-5dd5756b68" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: coredns-5dd5756b68-dnzfc"
I0205 10:50:28.673483       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/coredns-5dd5756b68" duration="346.483679ms"
I0205 10:50:28.757238       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/coredns-5dd5756b68" duration="83.582755ms"
I0205 10:50:28.757542       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/coredns-5dd5756b68" duration="148.785¬µs"
I0205 10:50:28.844529       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/coredns-5dd5756b68" duration="637.966¬µs"
I0205 10:50:31.956860       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/coredns-5dd5756b68" duration="64.503¬µs"
I0205 10:50:32.047699       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/coredns-5dd5756b68" duration="18.915417ms"
I0205 10:50:32.048298       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/coredns-5dd5756b68" duration="80.058¬µs"
I0205 10:55:11.563860       1 event.go:307] "Event occurred" object="default/mongodb-deployment" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set mongodb-deployment-699744c7d to 1"
I0205 10:55:11.605896       1 event.go:307] "Event occurred" object="default/mongodb-deployment-699744c7d" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: mongodb-deployment-699744c7d-wj6c6"
I0205 10:55:11.693039       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/mongodb-deployment-699744c7d" duration="129.357094ms"
I0205 10:55:11.757728       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/mongodb-deployment-699744c7d" duration="64.169466ms"
I0205 10:55:11.761905       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/mongodb-deployment-699744c7d" duration="206.114¬µs"
I0205 10:55:11.767833       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/mongodb-deployment-699744c7d" duration="108.497¬µs"
I0205 10:55:18.484182       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/mongodb-deployment-699744c7d" duration="39.079014ms"
I0205 10:55:18.489196       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/mongodb-deployment-699744c7d" duration="2.988906ms"
I0205 10:56:37.372243       1 event.go:307] "Event occurred" object="default/mongo-express" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set mongo-express-859f75dd4f to 1"
I0205 10:56:37.420748       1 event.go:307] "Event occurred" object="default/mongo-express-859f75dd4f" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: mongo-express-859f75dd4f-8s2wq"
I0205 10:56:37.460764       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/mongo-express-859f75dd4f" duration="89.537616ms"
I0205 10:56:37.556921       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/mongo-express-859f75dd4f" duration="96.081952ms"
I0205 10:56:37.557823       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/mongo-express-859f75dd4f" duration="834.214¬µs"
I0205 10:56:42.915026       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/mongo-express-859f75dd4f" duration="21.38532ms"
I0205 10:56:42.915439       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/mongo-express-859f75dd4f" duration="182.78¬µs"
E0205 13:08:13.767741       1 resource_quota_controller.go:440] failed to discover resources: the server has asked for the client to provide credentials
I0205 13:08:14.223987       1 garbagecollector.go:818] "failed to discover preferred resources" error="the server has asked for the client to provide credentials"
I0205 13:11:28.582213       1 cleaner.go:175] "Cleaning CSR as it is more than approvedExpiration duration old and approved." csr="csr-29cj5" approvedExpiration="1h0m0s"
E0205 14:16:34.617384       1 resource_quota_controller.go:440] failed to discover resources: the server has asked for the client to provide credentials
I0205 14:16:35.056474       1 garbagecollector.go:818] "failed to discover preferred resources" error="the server has asked for the client to provide credentials"
I0205 14:47:05.007019       1 event.go:307] "Event occurred" object="default/nginx-depl" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set nginx-depl-6777bffb6f to 1"
I0205 14:47:05.076163       1 event.go:307] "Event occurred" object="default/nginx-depl-6777bffb6f" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: nginx-depl-6777bffb6f-4mg9q"
I0205 14:47:05.107828       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/nginx-depl-6777bffb6f" duration="102.406317ms"
I0205 14:47:05.145461       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/nginx-depl-6777bffb6f" duration="37.581778ms"
I0205 14:47:05.185805       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/nginx-depl-6777bffb6f" duration="40.274119ms"
I0205 14:47:05.186097       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/nginx-depl-6777bffb6f" duration="77.361¬µs"
I0205 14:47:40.875655       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/nginx-depl-6777bffb6f" duration="13.341906ms"
I0205 14:47:40.875827       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/nginx-depl-6777bffb6f" duration="74.966¬µs"

* 
* ==> kube-controller-manager [e1bb8361945e] <==
* I0206 07:50:38.402068       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/coredns-5dd5756b68" duration="152.124¬µs"
I0206 07:50:42.494703       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/coredns-5dd5756b68" duration="1.506692339s"
I0206 07:50:42.507607       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/coredns-5dd5756b68" duration="250.127¬µs"
I0206 07:50:42.924315       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="ingress-nginx/ingress-nginx-controller-7c6974c4d8" duration="366.697113ms"
I0206 07:50:42.924738       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="ingress-nginx/ingress-nginx-controller-7c6974c4d8" duration="104.307¬µs"
I0206 07:50:43.654624       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/nginx-depl-6777bffb6f" duration="273.82396ms"
I0206 07:50:43.654751       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/nginx-depl-6777bffb6f" duration="71.57¬µs"
I0206 07:50:43.830070       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/mongodb-deployment-699744c7d" duration="125.06288ms"
I0206 07:50:43.846104       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/mongodb-deployment-699744c7d" duration="11.504316ms"
I0206 07:50:43.936509       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/mongo-express-859f75dd4f" duration="89.291278ms"
I0206 07:50:43.936650       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/mongo-express-859f75dd4f" duration="78.265¬µs"
I0206 07:50:49.107727       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kubernetes-dashboard/dashboard-metrics-scraper-7fd5cb4ddc" duration="52.103549ms"
I0206 07:50:49.107896       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kubernetes-dashboard/dashboard-metrics-scraper-7fd5cb4ddc" duration="82.909¬µs"
I0206 07:50:49.440166       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-8694d4445c" duration="118.502043ms"
I0206 07:50:49.444720       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-8694d4445c" duration="4.285111ms"
I0206 07:51:01.803481       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-8694d4445c" duration="239.586387ms"
I0206 07:51:01.837931       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-8694d4445c" duration="202.652¬µs"
I0206 07:51:02.285466       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-8694d4445c" duration="201.704971ms"
I0206 07:51:02.291219       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-8694d4445c" duration="5.60303ms"
I0206 08:24:00.346533       1 event.go:307] "Event occurred" object="my-namespace/mysql-pv-claim" fieldPath="" kind="PersistentVolumeClaim" apiVersion="v1" type="Normal" reason="ExternalProvisioning" message="Waiting for a volume to be created either by the external provisioner 'k8s.io/minikube-hostpath' or manually by the system administrator. If volume creation is delayed, please verify that the provisioner is running and correctly registered."
I0206 08:28:47.276327       1 event.go:307] "Event occurred" object="my-namespace/mysql" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set mysql-669455cb47 to 1"
I0206 08:28:47.355457       1 event.go:307] "Event occurred" object="my-namespace/mysql-669455cb47" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: mysql-669455cb47-lnx4f"
I0206 08:28:47.398899       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="my-namespace/mysql-669455cb47" duration="121.211082ms"
I0206 08:28:47.426646       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="my-namespace/mysql-669455cb47" duration="27.586162ms"
I0206 08:28:47.427854       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="my-namespace/mysql-669455cb47" duration="62.776¬µs"
I0206 08:28:47.440408       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="my-namespace/mysql-669455cb47" duration="121.094¬µs"
I0206 08:30:06.288010       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="my-namespace/mysql-669455cb47" duration="209.258¬µs"
I0206 08:54:16.341577       1 event.go:307] "Event occurred" object="my-namespace/mysql-669455cb47" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: mysql-669455cb47-jgkdm"
I0206 08:54:16.366620       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="my-namespace/mysql-669455cb47" duration="136.139658ms"
I0206 08:54:16.366761       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="my-namespace/mysql-669455cb47" duration="83.271¬µs"
I0206 08:54:16.442906       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="my-namespace/mysql-669455cb47" duration="75.267228ms"
I0206 08:54:16.443958       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="my-namespace/mysql-669455cb47" duration="617.864¬µs"
I0206 08:54:16.467790       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="my-namespace/mysql-669455cb47" duration="69.321¬µs"
I0206 08:54:16.820814       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="my-namespace/mysql-669455cb47" duration="111.82¬µs"
I0206 08:54:17.046912       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="my-namespace/mysql-669455cb47" duration="131.509¬µs"
I0206 08:54:17.074156       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="my-namespace/mysql-669455cb47" duration="89.296¬µs"
I0206 08:54:17.089587       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="my-namespace/mysql-669455cb47" duration="47.011¬µs"
I0206 08:54:17.098547       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="my-namespace/mysql-669455cb47" duration="73.598¬µs"
I0206 08:54:21.261285       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="my-namespace/mysql-669455cb47" duration="44.582¬µs"
I0206 08:56:50.059203       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="my-namespace/mysql-669455cb47" duration="9.15¬µs"
I0206 09:10:07.128984       1 event.go:307] "Event occurred" object="my-namespace/mysql" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set mysql-d6f56dd44 to 1"
I0206 09:10:07.149469       1 event.go:307] "Event occurred" object="my-namespace/mysql-d6f56dd44" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: mysql-d6f56dd44-5rb7j"
I0206 09:10:07.170056       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="my-namespace/mysql-d6f56dd44" duration="41.314841ms"
I0206 09:10:07.214121       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="my-namespace/mysql-d6f56dd44" duration="43.764255ms"
I0206 09:10:07.214504       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="my-namespace/mysql-d6f56dd44" duration="319.27¬µs"
I0206 09:10:07.227851       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="my-namespace/mysql-d6f56dd44" duration="73.415¬µs"
I0206 09:11:23.390913       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="my-namespace/mysql-d6f56dd44" duration="30.822437ms"
I0206 09:11:23.392968       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="my-namespace/mysql-d6f56dd44" duration="162.136¬µs"
E0206 10:24:48.335003       1 resource_quota_controller.go:440] failed to discover resources: the server has asked for the client to provide credentials
I0206 10:24:50.450424       1 garbagecollector.go:818] "failed to discover preferred resources" error="the server has asked for the client to provide credentials"
I0206 10:40:38.284678       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="my-namespace/mysql-d6f56dd44" duration="7.246¬µs"
I0206 10:43:51.610736       1 event.go:307] "Event occurred" object="default/mysql" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set mysql-d6f56dd44 to 1"
I0206 10:43:51.653607       1 event.go:307] "Event occurred" object="default/mysql-d6f56dd44" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: mysql-d6f56dd44-lzgxs"
I0206 10:43:51.676561       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/mysql-d6f56dd44" duration="62.851966ms"
I0206 10:43:51.735882       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/mysql-d6f56dd44" duration="59.149162ms"
I0206 10:43:51.736446       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/mysql-d6f56dd44" duration="67.498¬µs"
I0206 10:43:54.471457       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/mysql-d6f56dd44" duration="30.821134ms"
I0206 10:43:54.471603       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/mysql-d6f56dd44" duration="73.067¬µs"
E0206 12:58:30.449393       1 resource_quota_controller.go:440] failed to discover resources: the server has asked for the client to provide credentials
I0206 12:58:35.124412       1 garbagecollector.go:818] "failed to discover preferred resources" error="the server has asked for the client to provide credentials"

* 
* ==> kube-scheduler [1d1e9abd9836] <==
* Trace[1615039529]: [13.720741554s] [13.720741554s] END
E0206 07:50:17.401696       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Pod: failed to list *v1.Pod: Get "https://192.168.59.100:8443/api/v1/pods?fieldSelector=status.phase%3DSucceeded%!C(MISSING)status.phase%3DFailed&resourceVersion=16221": net/http: TLS handshake timeout
W0206 07:50:17.401280       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.PodDisruptionBudget: Get "https://192.168.59.100:8443/apis/policy/v1/poddisruptionbudgets?resourceVersion=16230": net/http: TLS handshake timeout
I0206 07:50:17.401728       1 trace.go:236] Trace[231805961]: "Reflector ListAndWatch" name:vendor/k8s.io/client-go/informers/factory.go:150 (06-Feb-2024 07:50:03.667) (total time: 13733ms):
Trace[231805961]: ---"Objects listed" error:Get "https://192.168.59.100:8443/apis/policy/v1/poddisruptionbudgets?resourceVersion=16230": net/http: TLS handshake timeout 13733ms (07:50:17.401)
Trace[231805961]: [13.733805038s] [13.733805038s] END
E0206 07:50:17.401736       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.PodDisruptionBudget: failed to list *v1.PodDisruptionBudget: Get "https://192.168.59.100:8443/apis/policy/v1/poddisruptionbudgets?resourceVersion=16230": net/http: TLS handshake timeout
W0206 07:50:17.401356       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.ReplicaSet: Get "https://192.168.59.100:8443/apis/apps/v1/replicasets?resourceVersion=16228": net/http: TLS handshake timeout
I0206 07:50:17.423319       1 trace.go:236] Trace[1590901053]: "Reflector ListAndWatch" name:vendor/k8s.io/client-go/informers/factory.go:150 (06-Feb-2024 07:50:02.493) (total time: 14908ms):
Trace[1590901053]: ---"Objects listed" error:Get "https://192.168.59.100:8443/apis/apps/v1/replicasets?resourceVersion=16228": net/http: TLS handshake timeout 14908ms (07:50:17.401)
Trace[1590901053]: [14.908507648s] [14.908507648s] END
E0206 07:50:17.432592       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.ReplicaSet: failed to list *v1.ReplicaSet: Get "https://192.168.59.100:8443/apis/apps/v1/replicasets?resourceVersion=16228": net/http: TLS handshake timeout
W0206 07:50:17.478806       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.StorageClass: Get "https://192.168.59.100:8443/apis/storage.k8s.io/v1/storageclasses?resourceVersion=16227": net/http: TLS handshake timeout
I0206 07:50:18.172698       1 trace.go:236] Trace[470200099]: "Reflector ListAndWatch" name:vendor/k8s.io/client-go/informers/factory.go:150 (06-Feb-2024 07:50:02.488) (total time: 14990ms):
Trace[470200099]: ---"Objects listed" error:Get "https://192.168.59.100:8443/apis/storage.k8s.io/v1/storageclasses?resourceVersion=16227": net/http: TLS handshake timeout 14944ms (07:50:17.432)
Trace[470200099]: [14.990895238s] [14.990895238s] END
E0206 07:50:18.172758       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.StorageClass: failed to list *v1.StorageClass: Get "https://192.168.59.100:8443/apis/storage.k8s.io/v1/storageclasses?resourceVersion=16227": net/http: TLS handshake timeout
W0206 07:50:18.173181       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Namespace: Get "https://192.168.59.100:8443/api/v1/namespaces?resourceVersion=16210": net/http: TLS handshake timeout
I0206 07:50:18.173341       1 trace.go:236] Trace[1653116948]: "Reflector ListAndWatch" name:vendor/k8s.io/client-go/informers/factory.go:150 (06-Feb-2024 07:50:06.830) (total time: 11343ms):
Trace[1653116948]: ---"Objects listed" error:Get "https://192.168.59.100:8443/api/v1/namespaces?resourceVersion=16210": net/http: TLS handshake timeout 11343ms (07:50:18.173)
Trace[1653116948]: [11.343301589s] [11.343301589s] END
E0206 07:50:18.173367       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Namespace: failed to list *v1.Namespace: Get "https://192.168.59.100:8443/api/v1/namespaces?resourceVersion=16210": net/http: TLS handshake timeout
W0206 07:50:17.401442       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.CSIStorageCapacity: Get "https://192.168.59.100:8443/apis/storage.k8s.io/v1/csistoragecapacities?resourceVersion=16204": net/http: TLS handshake timeout
I0206 07:50:18.190007       1 trace.go:236] Trace[725064510]: "Reflector ListAndWatch" name:vendor/k8s.io/client-go/informers/factory.go:150 (06-Feb-2024 07:50:04.287) (total time: 13901ms):
Trace[725064510]: ---"Objects listed" error:Get "https://192.168.59.100:8443/apis/storage.k8s.io/v1/csistoragecapacities?resourceVersion=16204": net/http: TLS handshake timeout 13114ms (07:50:17.401)
Trace[725064510]: [13.901501848s] [13.901501848s] END
E0206 07:50:18.190053       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.CSIStorageCapacity: failed to list *v1.CSIStorageCapacity: Get "https://192.168.59.100:8443/apis/storage.k8s.io/v1/csistoragecapacities?resourceVersion=16204": net/http: TLS handshake timeout
W0206 07:50:17.401522       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.StatefulSet: Get "https://192.168.59.100:8443/apis/apps/v1/statefulsets?resourceVersion=16209": net/http: TLS handshake timeout
I0206 07:50:18.197527       1 trace.go:236] Trace[1987457064]: "Reflector ListAndWatch" name:vendor/k8s.io/client-go/informers/factory.go:150 (06-Feb-2024 07:50:05.776) (total time: 12413ms):
Trace[1987457064]: ---"Objects listed" error:Get "https://192.168.59.100:8443/apis/apps/v1/statefulsets?resourceVersion=16209": net/http: TLS handshake timeout 11625ms (07:50:17.401)
Trace[1987457064]: [12.41392104s] [12.41392104s] END
E0206 07:50:18.199287       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.StatefulSet: failed to list *v1.StatefulSet: Get "https://192.168.59.100:8443/apis/apps/v1/statefulsets?resourceVersion=16209": net/http: TLS handshake timeout
W0206 07:50:17.401546       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Node: Get "https://192.168.59.100:8443/api/v1/nodes?resourceVersion=16217": net/http: TLS handshake timeout
I0206 07:50:18.205292       1 trace.go:236] Trace[525115524]: "Reflector ListAndWatch" name:vendor/k8s.io/client-go/informers/factory.go:150 (06-Feb-2024 07:50:03.733) (total time: 14471ms):
Trace[525115524]: ---"Objects listed" error:Get "https://192.168.59.100:8443/api/v1/nodes?resourceVersion=16217": net/http: TLS handshake timeout 13668ms (07:50:17.401)
Trace[525115524]: [14.471791876s] [14.471791876s] END
E0206 07:50:18.205415       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Node: failed to list *v1.Node: Get "https://192.168.59.100:8443/api/v1/nodes?resourceVersion=16217": net/http: TLS handshake timeout
W0206 07:50:18.210913       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.ReplicationController: Get "https://192.168.59.100:8443/api/v1/replicationcontrollers?resourceVersion=16231": net/http: TLS handshake timeout
I0206 07:50:18.211158       1 trace.go:236] Trace[1784498640]: "Reflector ListAndWatch" name:vendor/k8s.io/client-go/informers/factory.go:150 (06-Feb-2024 07:49:58.065) (total time: 20145ms):
Trace[1784498640]: ---"Objects listed" error:Get "https://192.168.59.100:8443/api/v1/replicationcontrollers?resourceVersion=16231": net/http: TLS handshake timeout 20145ms (07:50:18.210)
Trace[1784498640]: [20.145400861s] [20.145400861s] END
E0206 07:50:18.211193       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.ReplicationController: failed to list *v1.ReplicationController: Get "https://192.168.59.100:8443/api/v1/replicationcontrollers?resourceVersion=16231": net/http: TLS handshake timeout
W0206 07:50:18.561253       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.CSIDriver: Get "https://192.168.59.100:8443/apis/storage.k8s.io/v1/csidrivers?resourceVersion=16225": net/http: TLS handshake timeout
I0206 07:50:18.564932       1 trace.go:236] Trace[1157582097]: "Reflector ListAndWatch" name:vendor/k8s.io/client-go/informers/factory.go:150 (06-Feb-2024 07:50:02.480) (total time: 16081ms):
Trace[1157582097]: ---"Objects listed" error:Get "https://192.168.59.100:8443/apis/storage.k8s.io/v1/csidrivers?resourceVersion=16225": net/http: TLS handshake timeout 16080ms (07:50:18.560)
Trace[1157582097]: [16.081055613s] [16.081055613s] END
E0206 07:50:18.565138       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: Get "https://192.168.59.100:8443/apis/storage.k8s.io/v1/csidrivers?resourceVersion=16225": net/http: TLS handshake timeout
W0206 07:50:20.210730       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Service: Get "https://192.168.59.100:8443/api/v1/services?resourceVersion=16209": net/http: TLS handshake timeout
I0206 07:50:20.210813       1 trace.go:236] Trace[1538751484]: "Reflector ListAndWatch" name:vendor/k8s.io/client-go/informers/factory.go:150 (06-Feb-2024 07:50:09.206) (total time: 11004ms):
Trace[1538751484]: ---"Objects listed" error:Get "https://192.168.59.100:8443/api/v1/services?resourceVersion=16209": net/http: TLS handshake timeout 11004ms (07:50:20.210)
Trace[1538751484]: [11.004585083s] [11.004585083s] END
W0206 07:50:20.210878       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.PersistentVolumeClaim: Get "https://192.168.59.100:8443/api/v1/persistentvolumeclaims?resourceVersion=16220": net/http: TLS handshake timeout
I0206 07:50:20.211020       1 trace.go:236] Trace[1265263008]: "Reflector ListAndWatch" name:vendor/k8s.io/client-go/informers/factory.go:150 (06-Feb-2024 07:50:06.810) (total time: 13400ms):
Trace[1265263008]: ---"Objects listed" error:Get "https://192.168.59.100:8443/api/v1/persistentvolumeclaims?resourceVersion=16220": net/http: TLS handshake timeout 13400ms (07:50:20.210)
Trace[1265263008]: [13.400215892s] [13.400215892s] END
E0206 07:50:20.211052       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.PersistentVolumeClaim: failed to list *v1.PersistentVolumeClaim: Get "https://192.168.59.100:8443/api/v1/persistentvolumeclaims?resourceVersion=16220": net/http: TLS handshake timeout
E0206 07:50:20.210885       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Service: failed to list *v1.Service: Get "https://192.168.59.100:8443/api/v1/services?resourceVersion=16209": net/http: TLS handshake timeout
I0206 13:08:41.261970       1 tlsconfig.go:255] "Shutting down DynamicServingCertificateController"
I0206 13:08:41.262835       1 secure_serving.go:258] Stopped listening on 127.0.0.1:10259
E0206 13:08:41.263285       1 run.go:74] "command failed" err="finished without leader elect"

* 
* ==> kubelet <==
* -- Journal begins at Wed 2024-02-07 03:31:20 UTC, ends at Wed 2024-02-07 03:40:44 UTC. --
Feb 07 03:35:50 minikube kubelet[14204]: I0207 03:35:50.658275   14204 server.go:895] "Client rotation is on, will bootstrap in background"
Feb 07 03:35:50 minikube kubelet[14204]: I0207 03:35:50.662500   14204 certificate_store.go:130] Loading cert/key pair from "/var/lib/kubelet/pki/kubelet-client-current.pem".
Feb 07 03:35:50 minikube kubelet[14204]: I0207 03:35:50.665573   14204 dynamic_cafile_content.go:157] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
Feb 07 03:35:50 minikube kubelet[14204]: E0207 03:35:50.727277   14204 run.go:74] "command failed" err="failed to run Kubelet: validate service connection: validate CRI v1 image API for endpoint \"unix:///var/run/cri-dockerd.sock\": rpc error: code = Unknown desc = readdirent /var/lib/docker/image/overlay2/layerdb/mounts/a6c0da9b6145e18bcd661d5d6f8d52d3684a74feb456c6ed45a0cb764a879c43: bad message"
Feb 07 03:35:50 minikube systemd[1]: kubelet.service: Main process exited, code=exited, status=1/FAILURE
Feb 07 03:35:50 minikube systemd[1]: kubelet.service: Failed with result 'exit-code'.
Feb 07 03:35:51 minikube systemd[1]: kubelet.service: Scheduled restart job, restart counter is at 246.
Feb 07 03:35:51 minikube systemd[1]: Stopped kubelet: The Kubernetes Node Agent.
Feb 07 03:35:51 minikube systemd[1]: Started kubelet: The Kubernetes Node Agent.
Feb 07 03:35:51 minikube kubelet[14267]: Flag --container-runtime-endpoint has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Feb 07 03:35:51 minikube kubelet[14267]: I0207 03:35:51.769393   14267 server.go:467] "Kubelet version" kubeletVersion="v1.28.3"
Feb 07 03:35:51 minikube kubelet[14267]: I0207 03:35:51.769661   14267 server.go:469] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
Feb 07 03:35:51 minikube kubelet[14267]: I0207 03:35:51.772576   14267 server.go:895] "Client rotation is on, will bootstrap in background"
Feb 07 03:35:51 minikube kubelet[14267]: I0207 03:35:51.785348   14267 certificate_store.go:130] Loading cert/key pair from "/var/lib/kubelet/pki/kubelet-client-current.pem".
Feb 07 03:35:51 minikube kubelet[14267]: I0207 03:35:51.796053   14267 dynamic_cafile_content.go:157] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
Feb 07 03:35:51 minikube kubelet[14267]: E0207 03:35:51.868074   14267 run.go:74] "command failed" err="failed to run Kubelet: validate service connection: validate CRI v1 image API for endpoint \"unix:///var/run/cri-dockerd.sock\": rpc error: code = Unknown desc = readdirent /var/lib/docker/image/overlay2/layerdb/mounts/a6c0da9b6145e18bcd661d5d6f8d52d3684a74feb456c6ed45a0cb764a879c43: bad message"
Feb 07 03:35:51 minikube systemd[1]: kubelet.service: Main process exited, code=exited, status=1/FAILURE
Feb 07 03:35:51 minikube systemd[1]: kubelet.service: Failed with result 'exit-code'.
Feb 07 03:35:52 minikube systemd[1]: kubelet.service: Scheduled restart job, restart counter is at 247.
Feb 07 03:35:52 minikube systemd[1]: Stopped kubelet: The Kubernetes Node Agent.
Feb 07 03:35:52 minikube systemd[1]: Started kubelet: The Kubernetes Node Agent.
Feb 07 03:35:52 minikube kubelet[14329]: Flag --container-runtime-endpoint has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Feb 07 03:35:52 minikube kubelet[14329]: I0207 03:35:52.867158   14329 server.go:467] "Kubelet version" kubeletVersion="v1.28.3"
Feb 07 03:35:52 minikube kubelet[14329]: I0207 03:35:52.867461   14329 server.go:469] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
Feb 07 03:35:52 minikube kubelet[14329]: I0207 03:35:52.868516   14329 server.go:895] "Client rotation is on, will bootstrap in background"
Feb 07 03:35:52 minikube kubelet[14329]: I0207 03:35:52.874200   14329 certificate_store.go:130] Loading cert/key pair from "/var/lib/kubelet/pki/kubelet-client-current.pem".
Feb 07 03:35:52 minikube kubelet[14329]: I0207 03:35:52.878032   14329 dynamic_cafile_content.go:157] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
Feb 07 03:35:52 minikube kubelet[14329]: E0207 03:35:52.937712   14329 run.go:74] "command failed" err="failed to run Kubelet: validate service connection: validate CRI v1 image API for endpoint \"unix:///var/run/cri-dockerd.sock\": rpc error: code = Unknown desc = readdirent /var/lib/docker/image/overlay2/layerdb/mounts/a6c0da9b6145e18bcd661d5d6f8d52d3684a74feb456c6ed45a0cb764a879c43: bad message"
Feb 07 03:35:52 minikube systemd[1]: kubelet.service: Main process exited, code=exited, status=1/FAILURE
Feb 07 03:35:52 minikube systemd[1]: kubelet.service: Failed with result 'exit-code'.
Feb 07 03:35:53 minikube systemd[1]: kubelet.service: Scheduled restart job, restart counter is at 248.
Feb 07 03:35:53 minikube systemd[1]: Stopped kubelet: The Kubernetes Node Agent.
Feb 07 03:35:53 minikube systemd[1]: Started kubelet: The Kubernetes Node Agent.
Feb 07 03:35:53 minikube kubelet[14352]: Flag --container-runtime-endpoint has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Feb 07 03:35:53 minikube kubelet[14352]: I0207 03:35:53.802601   14352 server.go:467] "Kubelet version" kubeletVersion="v1.28.3"
Feb 07 03:35:53 minikube kubelet[14352]: I0207 03:35:53.803049   14352 server.go:469] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
Feb 07 03:35:53 minikube kubelet[14352]: I0207 03:35:53.804062   14352 server.go:895] "Client rotation is on, will bootstrap in background"
Feb 07 03:35:53 minikube kubelet[14352]: I0207 03:35:53.809628   14352 certificate_store.go:130] Loading cert/key pair from "/var/lib/kubelet/pki/kubelet-client-current.pem".
Feb 07 03:35:53 minikube kubelet[14352]: I0207 03:35:53.814306   14352 dynamic_cafile_content.go:157] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
Feb 07 03:35:53 minikube kubelet[14352]: E0207 03:35:53.870679   14352 run.go:74] "command failed" err="failed to run Kubelet: validate service connection: validate CRI v1 image API for endpoint \"unix:///var/run/cri-dockerd.sock\": rpc error: code = Unknown desc = readdirent /var/lib/docker/image/overlay2/layerdb/mounts/a6c0da9b6145e18bcd661d5d6f8d52d3684a74feb456c6ed45a0cb764a879c43: bad message"
Feb 07 03:35:53 minikube systemd[1]: kubelet.service: Main process exited, code=exited, status=1/FAILURE
Feb 07 03:35:53 minikube systemd[1]: kubelet.service: Failed with result 'exit-code'.
Feb 07 03:35:54 minikube systemd[1]: kubelet.service: Scheduled restart job, restart counter is at 249.
Feb 07 03:35:54 minikube systemd[1]: Stopped kubelet: The Kubernetes Node Agent.
Feb 07 03:35:54 minikube systemd[1]: Started kubelet: The Kubernetes Node Agent.
Feb 07 03:35:54 minikube kubelet[14375]: Flag --container-runtime-endpoint has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Feb 07 03:35:54 minikube kubelet[14375]: I0207 03:35:54.841203   14375 server.go:467] "Kubelet version" kubeletVersion="v1.28.3"
Feb 07 03:35:54 minikube kubelet[14375]: I0207 03:35:54.841363   14375 server.go:469] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
Feb 07 03:35:54 minikube kubelet[14375]: I0207 03:35:54.842118   14375 server.go:895] "Client rotation is on, will bootstrap in background"
Feb 07 03:35:54 minikube kubelet[14375]: I0207 03:35:54.847534   14375 certificate_store.go:130] Loading cert/key pair from "/var/lib/kubelet/pki/kubelet-client-current.pem".
Feb 07 03:35:54 minikube kubelet[14375]: I0207 03:35:54.852060   14375 dynamic_cafile_content.go:157] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
Feb 07 03:35:54 minikube kubelet[14375]: E0207 03:35:54.923297   14375 run.go:74] "command failed" err="failed to run Kubelet: validate service connection: validate CRI v1 image API for endpoint \"unix:///var/run/cri-dockerd.sock\": rpc error: code = Unknown desc = readdirent /var/lib/docker/image/overlay2/layerdb/mounts/a6c0da9b6145e18bcd661d5d6f8d52d3684a74feb456c6ed45a0cb764a879c43: bad message"
Feb 07 03:35:54 minikube systemd[1]: kubelet.service: Main process exited, code=exited, status=1/FAILURE
Feb 07 03:35:54 minikube systemd[1]: kubelet.service: Failed with result 'exit-code'.
Feb 07 03:35:55 minikube systemd[1]: kubelet.service: Scheduled restart job, restart counter is at 250.
Feb 07 03:35:55 minikube systemd[1]: Stopped kubelet: The Kubernetes Node Agent.
Feb 07 03:35:55 minikube systemd[1]: Started kubelet: The Kubernetes Node Agent.
Feb 07 03:35:55 minikube systemd[1]: Stopping kubelet: The Kubernetes Node Agent...
Feb 07 03:35:55 minikube systemd[1]: kubelet.service: Succeeded.
Feb 07 03:35:55 minikube systemd[1]: Stopped kubelet: The Kubernetes Node Agent.

* 
* ==> kubernetes-dashboard [5631cbc21ffd] <==
* 2024/02/06 07:51:00 Using namespace: kubernetes-dashboard
2024/02/06 07:51:00 Using in-cluster config to connect to apiserver
2024/02/06 07:51:00 Using secret token for csrf signing
2024/02/06 07:51:00 Initializing csrf token from kubernetes-dashboard-csrf secret
2024/02/06 07:51:01 Successful initial request to the apiserver, version: v1.28.3
2024/02/06 07:51:01 Generating JWE encryption key
2024/02/06 07:51:01 New synchronizer has been registered: kubernetes-dashboard-key-holder-kubernetes-dashboard. Starting
2024/02/06 07:51:01 Starting secret synchronizer for kubernetes-dashboard-key-holder in namespace kubernetes-dashboard
2024/02/06 07:51:04 Initializing JWE encryption key from synchronized object
2024/02/06 07:51:04 Creating in-cluster Sidecar client
2024/02/06 07:51:04 Serving insecurely on HTTP port: 9090
2024/02/06 07:51:04 Successful request to sidecar
2024/02/06 07:51:00 Starting overwatch

* 
* ==> kubernetes-dashboard [f4b96dcbc9f2] <==
* 2024/02/06 07:50:40 Synchronizer kubernetes-dashboard-key-holder-kubernetes-dashboard exited with error: kubernetes-dashboard-key-holder-kubernetes-dashboard watch ended with timeout
2024/02/06 07:50:42 Restarting synchronizer: kubernetes-dashboard-key-holder-kubernetes-dashboard.
2024/02/06 07:50:42 Starting secret synchronizer for kubernetes-dashboard-key-holder in namespace kubernetes-dashboard
2024/02/06 07:50:42 Synchronizer kubernetes-dashboard-key-holder-kubernetes-dashboard exited with error: kubernetes-dashboard-key-holder-kubernetes-dashboard watch ended with timeout
2024/02/06 07:50:44 Restarting synchronizer: kubernetes-dashboard-key-holder-kubernetes-dashboard.
2024/02/06 07:50:44 Starting secret synchronizer for kubernetes-dashboard-key-holder in namespace kubernetes-dashboard
2024/02/06 07:50:44 Synchronizer kubernetes-dashboard-key-holder-kubernetes-dashboard exited with error: kubernetes-dashboard-key-holder-kubernetes-dashboard watch ended with timeout
2024/02/06 07:50:46 Restarting synchronizer: kubernetes-dashboard-key-holder-kubernetes-dashboard.
2024/02/06 07:50:46 Starting secret synchronizer for kubernetes-dashboard-key-holder in namespace kubernetes-dashboard
2024/02/06 07:50:46 Synchronizer kubernetes-dashboard-key-holder-kubernetes-dashboard exited with error: kubernetes-dashboard-key-holder-kubernetes-dashboard watch ended with timeout
2024/02/06 07:50:48 Restarting synchronizer: kubernetes-dashboard-key-holder-kubernetes-dashboard.
2024/02/06 07:50:48 Starting secret synchronizer for kubernetes-dashboard-key-holder in namespace kubernetes-dashboard
2024/02/06 07:50:48 Synchronizer kubernetes-dashboard-key-holder-kubernetes-dashboard exited with error: kubernetes-dashboard-key-holder-kubernetes-dashboard watch ended with timeout
2024/02/06 07:50:45 Restarting synchronizer: kubernetes-dashboard-key-holder-kubernetes-dashboard.
2024/02/06 07:50:45 Starting secret synchronizer for kubernetes-dashboard-key-holder in namespace kubernetes-dashboard
2024/02/06 07:50:45 Synchronizer kubernetes-dashboard-key-holder-kubernetes-dashboard exited with error: kubernetes-dashboard-key-holder-kubernetes-dashboard watch ended with timeout
E0206 07:50:47.398886       1 runtime.go:77] Observed a panic: synchronizer kubernetes-dashboard-key-holder-kubernetes-dashboard restart limit execeeded. Restarting pod.
goroutine 10 [running]:
k8s.io/apimachinery/pkg/util/runtime.logPanic({0x16c68e0?, 0xc0003f3860})
	/home/runner/go/bin/pkg/mod/k8s.io/apimachinery@v0.25.0/pkg/util/runtime/runtime.go:75 +0x99
k8s.io/apimachinery/pkg/util/runtime.HandleCrash({0x0, 0x0, 0x203000?})
	/home/runner/go/bin/pkg/mod/k8s.io/apimachinery@v0.25.0/pkg/util/runtime/runtime.go:49 +0x75
panic({0x16c68e0, 0xc0003f3860})
	/opt/hostedtoolcache/go/1.19.0/x64/src/runtime/panic.go:884 +0x212
github.com/kubernetes/dashboard/src/app/backend/sync.(*overwatch).monitorRestartEvents.func1()
	/home/runner/work/dashboard/dashboard/src/app/backend/sync/overwatch.go:92 +0x159
k8s.io/apimachinery/pkg/util/wait.BackoffUntil.func1(0xc000547f08?)
	/home/runner/go/bin/pkg/mod/k8s.io/apimachinery@v0.25.0/pkg/util/wait/wait.go:157 +0x3e
k8s.io/apimachinery/pkg/util/wait.BackoffUntil(0x0?, {0x1c23980, 0xc000030d20}, 0x1, 0xc0000d4a80)
	/home/runner/go/bin/pkg/mod/k8s.io/apimachinery@v0.25.0/pkg/util/wait/wait.go:158 +0xb6
k8s.io/apimachinery/pkg/util/wait.JitterUntil(0x0?, 0x0, 0x0, 0x0?, 0x0?)
	/home/runner/go/bin/pkg/mod/k8s.io/apimachinery@v0.25.0/pkg/util/wait/wait.go:135 +0x89
k8s.io/apimachinery/pkg/util/wait.Until(...)
	/home/runner/go/bin/pkg/mod/k8s.io/apimachinery@v0.25.0/pkg/util/wait/wait.go:92
k8s.io/apimachinery/pkg/util/wait.Forever(0x0?, 0x0?)
	/home/runner/go/bin/pkg/mod/k8s.io/apimachinery@v0.25.0/pkg/util/wait/wait.go:83 +0x28
created by github.com/kubernetes/dashboard/src/app/backend/sync.(*overwatch).monitorRestartEvents
	/home/runner/work/dashboard/dashboard/src/app/backend/sync/overwatch.go:88 +0x98
panic: synchronizer kubernetes-dashboard-key-holder-kubernetes-dashboard restart limit execeeded. Restarting pod. [recovered]
	panic: synchronizer kubernetes-dashboard-key-holder-kubernetes-dashboard restart limit execeeded. Restarting pod.

goroutine 10 [running]:
k8s.io/apimachinery/pkg/util/runtime.HandleCrash({0x0, 0x0, 0x203000?})
	/home/runner/go/bin/pkg/mod/k8s.io/apimachinery@v0.25.0/pkg/util/runtime/runtime.go:56 +0xd7
panic({0x16c68e0, 0xc0003f3860})
	/opt/hostedtoolcache/go/1.19.0/x64/src/runtime/panic.go:884 +0x212
github.com/kubernetes/dashboard/src/app/backend/sync.(*overwatch).monitorRestartEvents.func1()
	/home/runner/work/dashboard/dashboard/src/app/backend/sync/overwatch.go:92 +0x159
k8s.io/apimachinery/pkg/util/wait.BackoffUntil.func1(0xc000547f08?)
	/home/runner/go/bin/pkg/mod/k8s.io/apimachinery@v0.25.0/pkg/util/wait/wait.go:157 +0x3e
k8s.io/apimachinery/pkg/util/wait.BackoffUntil(0x0?, {0x1c23980, 0xc000030d20}, 0x1, 0xc0000d4a80)
	/home/runner/go/bin/pkg/mod/k8s.io/apimachinery@v0.25.0/pkg/util/wait/wait.go:158 +0xb6
k8s.io/apimachinery/pkg/util/wait.JitterUntil(0x0?, 0x0, 0x0, 0x0?, 0x0?)
	/home/runner/go/bin/pkg/mod/k8s.io/apimachinery@v0.25.0/pkg/util/wait/wait.go:135 +0x89
k8s.io/apimachinery/pkg/util/wait.Until(...)
	/home/runner/go/bin/pkg/mod/k8s.io/apimachinery@v0.25.0/pkg/util/wait/wait.go:92
k8s.io/apimachinery/pkg/util/wait.Forever(0x0?, 0x0?)
	/home/runner/go/bin/pkg/mod/k8s.io/apimachinery@v0.25.0/pkg/util/wait/wait.go:83 +0x28
created by github.com/kubernetes/dashboard/src/app/backend/sync.(*overwatch).monitorRestartEvents
	/home/runner/work/dashboard/dashboard/src/app/backend/sync/overwatch.go:88 +0x98

